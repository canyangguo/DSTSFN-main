/home/assassin/project/STG-NCDE-main
Traceback (most recent call last):
  File "/home/assassin/project/STG-NCDE-main/model/Run_cde.py", line 7, in <module>
    import torch
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/__init__.py", line 515, in <module>
    from ._tensor import Tensor
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/_tensor.py", line 16, in <module>
    import torch.utils.hooks as hooks
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 941, in get_code
  File "<frozen importlib._bootstrap_external>", line 1040, in get_data
KeyboardInterrupt
2022-06-24 11:43: Experiment log path in: ../runs/PEMSD7/06-24-11h43m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}
2022-06-24 11:43: Argument batch_size: 32
2022-06-24 11:43: Argument cheb_k: 2
2022-06-24 11:43: Argument column_wise: False
2022-06-24 11:43: Argument comment: ''
2022-06-24 11:43: Argument cuda: True
2022-06-24 11:43: Argument dataset: 'PEMSD7'
2022-06-24 11:43: Argument debug: False
2022-06-24 11:43: Argument default_graph: True
2022-06-24 11:43: Argument device: 1
2022-06-24 11:43: Argument early_stop: True
2022-06-24 11:43: Argument early_stop_patience: 15
2022-06-24 11:43: Argument embed_dim: 10
2022-06-24 11:43: Argument epochs: 200
2022-06-24 11:43: Argument g_type: 'agc'
2022-06-24 11:43: Argument grad_norm: False
2022-06-24 11:43: Argument hid_dim: 64
2022-06-24 11:43: Argument hid_hid_dim: 64
2022-06-24 11:43: Argument horizon: 12
2022-06-24 11:43: Argument input_dim: 2
2022-06-24 11:43: Argument lag: 12
2022-06-24 11:43: Argument log_dir: '../runs/PEMSD7/06-24-11h43m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}'
2022-06-24 11:43: Argument log_step: 20
2022-06-24 11:43: Argument loss_func: 'mae'
2022-06-24 11:43: Argument lr_decay: False
2022-06-24 11:43: Argument lr_decay_rate: 0.3
2022-06-24 11:43: Argument lr_decay_step: '5,20,40,70'
2022-06-24 11:43: Argument lr_init: 0.001
2022-06-24 11:43: Argument mae_thresh: None
2022-06-24 11:43: Argument mape_thresh: 0.0
2022-06-24 11:43: Argument max_grad_norm: 5
2022-06-24 11:43: Argument missing_rate: 0.1
2022-06-24 11:43: Argument missing_test: False
2022-06-24 11:43: Argument mode: 'train'
2022-06-24 11:43: Argument model: 'GCDE'
2022-06-24 11:43: Argument model_path: ''
2022-06-24 11:43: Argument model_type: 'type1'
2022-06-24 11:43: Argument normalizer: 'std'
2022-06-24 11:43: Argument num_layers: 2
2022-06-24 11:43: Argument num_nodes: 170
2022-06-24 11:43: Argument output_dim: 1
2022-06-24 11:43: Argument plot: False
2022-06-24 11:43: Argument real_value: True
2022-06-24 11:43: Argument seed: 0
2022-06-24 11:43: Argument solver: 'rk4'
2022-06-24 11:43: Argument teacher_forcing: False
2022-06-24 11:43: Argument tensorboard: True
2022-06-24 11:43: Argument test_ratio: 0.2
2022-06-24 11:43: Argument tod: False
2022-06-24 11:43: Argument val_ratio: 0.2
2022-06-24 11:43: Argument weight_decay: 0.001
2022-06-24 11:43: NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
2022-06-24 11:43: Total params: 374164
/home/assassin/project/STG-NCDE-main
Namespace(dataset='PEMSD7', mode='train', device=1, debug=False, model='GCDE', cuda=True, comment='', val_ratio=0.2, test_ratio=0.2, lag=12, horizon=12, num_nodes=170, tod=False, normalizer='std', column_wise=False, default_graph=True, model_type='type1', g_type='agc', input_dim=2, output_dim=1, embed_dim=10, hid_dim=64, hid_hid_dim=64, num_layers=2, cheb_k=2, solver='rk4', loss_func='mae', seed=0, batch_size=32, epochs=200, lr_init=0.001, weight_decay=0.001, lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', early_stop=True, early_stop_patience=15, grad_norm=False, max_grad_norm=5, teacher_forcing=False, real_value=True, missing_test=False, missing_rate=0.1, mae_thresh=None, mape_thresh=0.0, model_path='', log_dir='../runs', log_step=20, plot=False, tensorboard=True)
NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
*****************Model Parameter*****************
node_embeddings torch.Size([170, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([170, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 374164
*****************Finish Parameter****************
Load PEMSD7 Dataset shaped:  (28224, 883, 1) 1498.0 0.0 308.52346223738647 304.0
Normalize the dataset by Standard Normalization
Train:  (16912, 12, 883, 1) (16912, 12, 883, 1)
Val:  (5622, 12, 883, 1) (5622, 12, 883, 1)
Test:  (5621, 12, 883, 1) (5621, 12, 883, 1)
Creat Log File in:  ../runs/PEMSD7/06-24-11h43m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/run.log
*****************Model Parameter*****************
node_embeddings torch.Size([170, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([170, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 374164
*****************Finish Parameter****************
Traceback (most recent call last):
  File "/home/assassin/project/STG-NCDE-main/model/Run_cde.py", line 200, in <module>
    trainer.train()
  File "/home/assassin/project/STG-NCDE-main/model/BasicTrainer_cde.py", line 131, in train
    train_epoch_loss = self.train_epoch(epoch)
  File "/home/assassin/project/STG-NCDE-main/model/BasicTrainer_cde.py", line 91, in train_epoch
    output = self.model(self.times, train_coeffs)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/assassin/project/STG-NCDE-main/model/GCDE.py", line 53, in forward
    z_t = controldiffeq.cdeint_gde_dev(dX_dt=spline.derivative, #dh_dt
  File "/home/assassin/project/STG-NCDE-main/controldiffeq/cdeint_module.py", line 287, in cdeint_gde_dev
    out = odeint(func=vector_field, y0=init0, t=t, **kwargs)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torchdiffeq/_impl/adjoint.py", line 198, in odeint_adjoint
    ans = OdeintAdjointMethod.apply(shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol,
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torchdiffeq/_impl/adjoint.py", line 25, in forward
    ans = odeint(func, y0, t, rtol=rtol, atol=atol, method=method, options=options, event_fn=event_fn)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torchdiffeq/_impl/odeint.py", line 77, in odeint
    solution = solver.integrate(t)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torchdiffeq/_impl/solvers.py", line 105, in integrate
    dy, f0 = self._step_func(self.func, t0, dt, t1, y0)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torchdiffeq/_impl/fixed_grid.py", line 28, in _step_func
    f0 = func(t0, y0, perturb=Perturb.NEXT if self.perturb else Perturb.NONE)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py", line 189, in forward
    return self.base_func(t, y)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py", line 138, in forward
    f = self.base_func(t, _flat_to_shape(y, (), self.shapes))
  File "/home/assassin/project/STG-NCDE-main/controldiffeq/cdeint_module.py", line 93, in __call__
    vector_field_g = self.func_g(z) # vector_field_g: torch.Size([64, 207, 32, 2])
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/assassin/project/STG-NCDE-main/model/vector_fields.py", line 156, in forward
    z = self.agc(z)
  File "/home/assassin/project/STG-NCDE-main/model/vector_fields.py", line 192, in agc
    x_g = torch.einsum("knm,bmc->bknc", supports, z)      #B, cheb_k, N, dim_in
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/functional.py", line 299, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
RuntimeError: einsum(): operands do not broadcast with remapped shapes [original->remapped]: [2, 170, 170]->[1, 2, 170, 1, 170] [32, 883, 64]->[32, 1, 1, 64, 883]
2022-06-24 11:46: Experiment log path in: ../runs/PEMSD7/06-24-11h45m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}
2022-06-24 11:46: Argument batch_size: 32
2022-06-24 11:46: Argument cheb_k: 2
2022-06-24 11:46: Argument column_wise: False
2022-06-24 11:46: Argument comment: ''
2022-06-24 11:46: Argument cuda: True
2022-06-24 11:46: Argument dataset: 'PEMSD7'
2022-06-24 11:46: Argument debug: False
2022-06-24 11:46: Argument default_graph: True
2022-06-24 11:46: Argument device: 1
2022-06-24 11:46: Argument early_stop: True
2022-06-24 11:46: Argument early_stop_patience: 15
2022-06-24 11:46: Argument embed_dim: 10
2022-06-24 11:46: Argument epochs: 200
2022-06-24 11:46: Argument g_type: 'agc'
2022-06-24 11:46: Argument grad_norm: False
2022-06-24 11:46: Argument hid_dim: 64
2022-06-24 11:46: Argument hid_hid_dim: 64
2022-06-24 11:46: Argument horizon: 12
2022-06-24 11:46: Argument input_dim: 2
2022-06-24 11:46: Argument lag: 12
2022-06-24 11:46: Argument log_dir: '../runs/PEMSD7/06-24-11h45m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}'
2022-06-24 11:46: Argument log_step: 20
2022-06-24 11:46: Argument loss_func: 'mae'
2022-06-24 11:46: Argument lr_decay: False
2022-06-24 11:46: Argument lr_decay_rate: 0.3
2022-06-24 11:46: Argument lr_decay_step: '5,20,40,70'
2022-06-24 11:46: Argument lr_init: 0.001
2022-06-24 11:46: Argument mae_thresh: None
2022-06-24 11:46: Argument mape_thresh: 0.0
2022-06-24 11:46: Argument max_grad_norm: 5
2022-06-24 11:46: Argument missing_rate: 0.1
2022-06-24 11:46: Argument missing_test: False
2022-06-24 11:46: Argument mode: 'train'
2022-06-24 11:46: Argument model: 'GCDE'
2022-06-24 11:46: Argument model_path: ''
2022-06-24 11:46: Argument model_type: 'type1'
2022-06-24 11:46: Argument normalizer: 'std'
2022-06-24 11:46: Argument num_layers: 2
2022-06-24 11:46: Argument num_nodes: 883
2022-06-24 11:46: Argument output_dim: 1
2022-06-24 11:46: Argument plot: False
2022-06-24 11:46: Argument real_value: True
2022-06-24 11:46: Argument seed: 0
2022-06-24 11:46: Argument solver: 'rk4'
2022-06-24 11:46: Argument teacher_forcing: False
2022-06-24 11:46: Argument tensorboard: True
2022-06-24 11:46: Argument test_ratio: 0.2
2022-06-24 11:46: Argument tod: False
2022-06-24 11:46: Argument val_ratio: 0.2
2022-06-24 11:46: Argument weight_decay: 0.001
2022-06-24 11:46: NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
2022-06-24 11:46: Total params: 388424
2022-06-24 11:46: Train Epoch 1: 0/528 Loss: 287.177032
2022-06-24 11:46: Train Epoch 1: 20/528 Loss: 116.362724
2022-06-24 11:46: Train Epoch 1: 40/528 Loss: 45.919300
2022-06-24 11:46: Train Epoch 1: 60/528 Loss: 42.012360
2022-06-24 11:47: Train Epoch 1: 80/528 Loss: 48.692894
2022-06-24 11:47: Train Epoch 1: 100/528 Loss: 38.868488
2022-06-24 11:47: Train Epoch 1: 120/528 Loss: 38.068203
/home/assassin/project/STG-NCDE-main
Namespace(dataset='PEMSD7', mode='train', device=1, debug=False, model='GCDE', cuda=True, comment='', val_ratio=0.2, test_ratio=0.2, lag=12, horizon=12, num_nodes=883, tod=False, normalizer='std', column_wise=False, default_graph=True, model_type='type1', g_type='agc', input_dim=2, output_dim=1, embed_dim=10, hid_dim=64, hid_hid_dim=64, num_layers=2, cheb_k=2, solver='rk4', loss_func='mae', seed=0, batch_size=32, epochs=200, lr_init=0.001, weight_decay=0.001, lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', early_stop=True, early_stop_patience=15, grad_norm=False, max_grad_norm=5, teacher_forcing=False, real_value=True, missing_test=False, missing_rate=0.1, mae_thresh=None, mape_thresh=0.0, model_path='', log_dir='../runs', log_step=20, plot=False, tensorboard=True)
NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
*****************Model Parameter*****************
node_embeddings torch.Size([883, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([883, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 388424
*****************Finish Parameter****************
Load PEMSD7 Dataset shaped:  (28224, 883, 1) 1498.0 0.0 308.52346223738647 304.0
Normalize the dataset by Standard Normalization
Train:  (16912, 12, 883, 1) (16912, 12, 883, 1)
Val:  (5622, 12, 883, 1) (5622, 12, 883, 1)
Test:  (5621, 12, 883, 1) (5621, 12, 883, 1)
Creat Log File in:  ../runs/PEMSD7/06-24-11h45m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/run.log
*****************Model Parameter*****************
node_embeddings torch.Size([883, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([883, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 388424
*****************Finish Parameter****************
Traceback (most recent call last):
  File "/home/assassin/project/STG-NCDE-main/model/Run_cde.py", line 200, in <module>
    trainer.train()
  File "/home/assassin/project/STG-NCDE-main/model/BasicTrainer_cde.py", line 131, in train
    train_epoch_loss = self.train_epoch(epoch)
  File "/home/assassin/project/STG-NCDE-main/model/BasicTrainer_cde.py", line 99, in train_epoch
    loss.backward()
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/assassin/anaconda3/envs/stgncde/lib/python3.9/site-packages/torch/autograd/__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
2022-06-24 11:51: Experiment log path in: ../runs/PEMSD7/06-24-11h51m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}
2022-06-24 11:51: Argument batch_size: 32
2022-06-24 11:51: Argument cheb_k: 2
2022-06-24 11:51: Argument column_wise: False
2022-06-24 11:51: Argument comment: ''
2022-06-24 11:51: Argument cuda: True
2022-06-24 11:51: Argument dataset: 'PEMSD7'
2022-06-24 11:51: Argument debug: False
2022-06-24 11:51: Argument default_graph: True
2022-06-24 11:51: Argument device: 1
2022-06-24 11:51: Argument early_stop: True
2022-06-24 11:51: Argument early_stop_patience: 15
2022-06-24 11:51: Argument embed_dim: 10
2022-06-24 11:51: Argument epochs: 200
2022-06-24 11:51: Argument g_type: 'agc'
2022-06-24 11:51: Argument grad_norm: False
2022-06-24 11:51: Argument hid_dim: 64
2022-06-24 11:51: Argument hid_hid_dim: 64
2022-06-24 11:51: Argument horizon: 12
2022-06-24 11:51: Argument input_dim: 2
2022-06-24 11:51: Argument lag: 12
2022-06-24 11:51: Argument log_dir: '../runs/PEMSD7/06-24-11h51m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}'
2022-06-24 11:51: Argument log_step: 20
2022-06-24 11:51: Argument loss_func: 'mae'
2022-06-24 11:51: Argument lr_decay: False
2022-06-24 11:51: Argument lr_decay_rate: 0.3
2022-06-24 11:51: Argument lr_decay_step: '5,20,40,70'
2022-06-24 11:51: Argument lr_init: 0.001
2022-06-24 11:51: Argument mae_thresh: None
2022-06-24 11:51: Argument mape_thresh: 0.0
2022-06-24 11:51: Argument max_grad_norm: 5
2022-06-24 11:51: Argument missing_rate: 0.1
2022-06-24 11:51: Argument missing_test: False
2022-06-24 11:51: Argument mode: 'train'
2022-06-24 11:51: Argument model: 'GCDE'
2022-06-24 11:51: Argument model_path: ''
2022-06-24 11:51: Argument model_type: 'type1'
2022-06-24 11:51: Argument normalizer: 'std'
2022-06-24 11:51: Argument num_layers: 2
2022-06-24 11:51: Argument num_nodes: 883
2022-06-24 11:51: Argument output_dim: 1
2022-06-24 11:51: Argument plot: False
2022-06-24 11:51: Argument real_value: True
2022-06-24 11:51: Argument seed: 0
2022-06-24 11:51: Argument solver: 'rk4'
2022-06-24 11:51: Argument teacher_forcing: False
2022-06-24 11:51: Argument tensorboard: True
2022-06-24 11:51: Argument test_ratio: 0.2
2022-06-24 11:51: Argument tod: False
2022-06-24 11:51: Argument val_ratio: 0.2
2022-06-24 11:51: Argument weight_decay: 0.001
2022-06-24 11:51: NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
2022-06-24 11:51: Total params: 388424
2022-06-24 11:51: Train Epoch 1: 0/528 Loss: 287.177032
/home/assassin/project/STG-NCDE-main
Traceback (most recent call last):
  File "/home/assassin/project/STG-NCDE-main/model/Run_cde.py", line 16, in <module>
    from lib.dataloader import get_dataloader_cde
  File "/home/assassin/project/STG-NCDE-main/lib/dataloader.py", line 7, in <module>
    import controldiffeq
  File "/home/assassin/project/STG-NCDE-main/controldiffeq/__init__.py", line 1, in <module>
    from .cdeint_module import cdeint, VectorField, cdeint_gde, cdeint_gde_dev
  File "/home/assassin/project/STG-NCDE-main/controldiffeq/cdeint_module.py", line 2, in <module>
    import torchdiffeq
ModuleNotFoundError: No module named 'torchdiffeq'
2022-06-24 11:51: Train Epoch 1: 20/528 Loss: 116.362724
2022-06-24 11:52: Train Epoch 1: 40/528 Loss: 45.919300
2022-06-24 11:52: Train Epoch 1: 60/528 Loss: 42.012360
2022-06-24 11:53: Train Epoch 1: 80/528 Loss: 48.692894
2022-06-24 11:53: Train Epoch 1: 100/528 Loss: 38.868488
2022-06-24 11:54: Train Epoch 1: 120/528 Loss: 38.068203
2022-06-24 11:55: Train Epoch 1: 140/528 Loss: 38.149166
2022-06-24 11:56: Train Epoch 1: 160/528 Loss: 36.518806
2022-06-24 11:57: Train Epoch 1: 180/528 Loss: 41.873966
2022-06-24 11:58: Train Epoch 1: 200/528 Loss: 40.256321
2022-06-24 11:59: Train Epoch 1: 220/528 Loss: 32.196396
2022-06-24 11:59: Train Epoch 1: 240/528 Loss: 35.638359
2022-06-24 12:00: Train Epoch 1: 260/528 Loss: 35.058414
2022-06-24 12:01: Train Epoch 1: 280/528 Loss: 38.763855
2022-06-24 12:02: Train Epoch 1: 300/528 Loss: 30.246490
2022-06-24 12:03: Train Epoch 1: 320/528 Loss: 34.186848
2022-06-24 12:04: Train Epoch 1: 340/528 Loss: 29.561434
2022-06-24 12:05: Train Epoch 1: 360/528 Loss: 29.572783
2022-06-24 12:06: Train Epoch 1: 380/528 Loss: 33.085918
2022-06-24 12:07: Train Epoch 1: 400/528 Loss: 34.603363
2022-06-24 12:08: Train Epoch 1: 420/528 Loss: 29.824787
2022-06-24 12:09: Train Epoch 1: 440/528 Loss: 30.436525
2022-06-24 12:10: Train Epoch 1: 460/528 Loss: 27.941105
2022-06-24 12:11: Train Epoch 1: 480/528 Loss: 28.290083
2022-06-24 12:12: Train Epoch 1: 500/528 Loss: 30.090948
2022-06-24 12:13: Train Epoch 1: 520/528 Loss: 30.133308
2022-06-24 12:13: **********Train Epoch 1: averaged Loss: 43.432299
2022-06-24 12:16: **********Val Epoch 1: average Loss: 29.235360
2022-06-24 12:16: *********************************Current best model saved!
2022-06-24 12:16: Train Epoch 2: 0/528 Loss: 28.073502
2022-06-24 12:17: Train Epoch 2: 20/528 Loss: 30.791513
2022-06-24 12:17: Train Epoch 2: 40/528 Loss: 29.039362
2022-06-24 12:18: Train Epoch 2: 60/528 Loss: 29.344591
2022-06-24 12:19: Train Epoch 2: 80/528 Loss: 27.846453
2022-06-24 12:20: Train Epoch 2: 100/528 Loss: 28.994112
2022-06-24 12:21: Train Epoch 2: 120/528 Loss: 36.408550
2022-06-24 12:22: Train Epoch 2: 140/528 Loss: 29.300976
2022-06-24 12:23: Train Epoch 2: 160/528 Loss: 30.052107
2022-06-24 12:23: Train Epoch 2: 180/528 Loss: 33.184143
2022-06-24 12:24: Train Epoch 2: 200/528 Loss: 37.218445
2022-06-24 12:25: Train Epoch 2: 220/528 Loss: 31.317617
2022-06-24 12:26: Train Epoch 2: 240/528 Loss: 32.217331
2022-06-24 12:27: Train Epoch 2: 260/528 Loss: 32.542141
2022-06-24 12:28: Train Epoch 2: 280/528 Loss: 28.031374
2022-06-24 12:29: Train Epoch 2: 300/528 Loss: 28.430334
2022-06-24 12:30: Train Epoch 2: 320/528 Loss: 28.510441
2022-06-24 12:31: Train Epoch 2: 340/528 Loss: 36.495193
2022-06-24 12:32: Train Epoch 2: 360/528 Loss: 29.525633
2022-06-24 12:33: Train Epoch 2: 380/528 Loss: 27.690063
2022-06-24 12:34: Train Epoch 2: 400/528 Loss: 28.158510
2022-06-24 12:35: Train Epoch 2: 420/528 Loss: 29.897579
2022-06-24 12:36: Train Epoch 2: 440/528 Loss: 28.394213
2022-06-24 12:37: Train Epoch 2: 460/528 Loss: 28.693935
2022-06-24 12:38: Train Epoch 2: 480/528 Loss: 29.161528
2022-06-24 12:39: Train Epoch 2: 500/528 Loss: 27.563566
2022-06-24 12:39: Train Epoch 2: 520/528 Loss: 29.281168
2022-06-24 12:40: **********Train Epoch 2: averaged Loss: 29.798042
2022-06-24 12:42: **********Val Epoch 2: average Loss: 27.519919
2022-06-24 12:42: *********************************Current best model saved!
2022-06-24 12:42: Train Epoch 3: 0/528 Loss: 26.102192
2022-06-24 12:43: Train Epoch 3: 20/528 Loss: 29.673870
2022-06-24 12:44: Train Epoch 3: 40/528 Loss: 26.797310
2022-06-24 12:45: Train Epoch 3: 60/528 Loss: 31.729401
2022-06-24 12:46: Train Epoch 3: 80/528 Loss: 29.837250
2022-06-24 12:47: Train Epoch 3: 100/528 Loss: 34.303177
2022-06-24 12:48: Train Epoch 3: 120/528 Loss: 27.117693
2022-06-24 12:49: Train Epoch 3: 140/528 Loss: 30.572081
2022-06-24 12:50: Train Epoch 3: 160/528 Loss: 26.950111
2022-06-24 12:51: Train Epoch 3: 180/528 Loss: 28.122316
2022-06-24 12:52: Train Epoch 3: 200/528 Loss: 28.421835
2022-06-24 12:52: Train Epoch 3: 220/528 Loss: 30.681801
2022-06-24 12:53: Train Epoch 3: 240/528 Loss: 30.225664
2022-06-24 12:54: Train Epoch 3: 260/528 Loss: 28.974749
2022-06-24 12:55: Train Epoch 3: 280/528 Loss: 26.073065
2022-06-24 12:56: Train Epoch 3: 300/528 Loss: 32.993870
2022-06-24 12:57: Train Epoch 3: 320/528 Loss: 30.061396
2022-06-24 12:58: Train Epoch 3: 340/528 Loss: 26.588902
2022-06-24 12:59: Train Epoch 3: 360/528 Loss: 24.947298
2022-06-24 13:00: Train Epoch 3: 380/528 Loss: 25.857626
2022-06-24 13:01: Train Epoch 3: 400/528 Loss: 33.333801
2022-06-24 13:02: Train Epoch 3: 420/528 Loss: 23.919851
2022-06-24 13:03: Train Epoch 3: 440/528 Loss: 29.105515
2022-06-24 13:04: Train Epoch 3: 460/528 Loss: 26.720751
2022-06-24 13:05: Train Epoch 3: 480/528 Loss: 26.604897
2022-06-24 13:05: Train Epoch 3: 500/528 Loss: 24.859962
2022-06-24 13:06: Train Epoch 3: 520/528 Loss: 28.216702
2022-06-24 13:07: **********Train Epoch 3: averaged Loss: 28.245085
2022-06-24 13:09: **********Val Epoch 3: average Loss: 26.224884
2022-06-24 13:09: *********************************Current best model saved!
2022-06-24 13:09: Train Epoch 4: 0/528 Loss: 26.499874
2022-06-24 13:10: Train Epoch 4: 20/528 Loss: 27.660709
2022-06-24 13:11: Train Epoch 4: 40/528 Loss: 26.809608
2022-06-24 13:12: Train Epoch 4: 60/528 Loss: 26.187229
2022-06-24 13:13: Train Epoch 4: 80/528 Loss: 29.134527
2022-06-24 13:14: Train Epoch 4: 100/528 Loss: 25.256294
2022-06-24 13:15: Train Epoch 4: 120/528 Loss: 28.465235
2022-06-24 13:16: Train Epoch 4: 140/528 Loss: 25.032696
2022-06-24 13:17: Train Epoch 4: 160/528 Loss: 28.873886
2022-06-24 13:18: Train Epoch 4: 180/528 Loss: 28.661612
2022-06-24 13:18: Train Epoch 4: 200/528 Loss: 28.894934
2022-06-24 13:19: Train Epoch 4: 220/528 Loss: 24.723969
2022-06-24 13:21: Train Epoch 4: 240/528 Loss: 30.458092
2022-06-24 13:21: Train Epoch 4: 260/528 Loss: 25.645927
2022-06-24 13:23: Train Epoch 4: 280/528 Loss: 28.377110
2022-06-24 13:24: Train Epoch 4: 300/528 Loss: 25.382532
2022-06-24 13:24: Train Epoch 4: 320/528 Loss: 27.746706
2022-06-24 13:25: Train Epoch 4: 340/528 Loss: 27.159925
2022-06-24 13:26: Train Epoch 4: 360/528 Loss: 27.671114
2022-06-24 13:27: Train Epoch 4: 380/528 Loss: 26.113602
2022-06-24 13:28: Train Epoch 4: 400/528 Loss: 25.410532
2022-06-24 13:29: Train Epoch 4: 420/528 Loss: 27.830088
2022-06-24 13:29: Train Epoch 4: 440/528 Loss: 25.015095
2022-06-24 13:30: Train Epoch 4: 460/528 Loss: 26.715406
2022-06-24 13:31: Train Epoch 4: 480/528 Loss: 25.220530
2022-06-24 13:32: Train Epoch 4: 500/528 Loss: 24.396189
2022-06-24 13:33: Train Epoch 4: 520/528 Loss: 27.360155
2022-06-24 13:33: **********Train Epoch 4: averaged Loss: 27.515244
2022-06-24 13:36: **********Val Epoch 4: average Loss: 25.470553
2022-06-24 13:36: *********************************Current best model saved!
2022-06-24 13:36: Train Epoch 5: 0/528 Loss: 27.191069
2022-06-24 13:37: Train Epoch 5: 20/528 Loss: 25.620092
2022-06-24 13:38: Train Epoch 5: 40/528 Loss: 25.584234
2022-06-24 13:39: Train Epoch 5: 60/528 Loss: 26.499512
2022-06-24 13:40: Train Epoch 5: 80/528 Loss: 29.383518
2022-06-24 13:41: Train Epoch 5: 100/528 Loss: 27.333416
2022-06-24 13:42: Train Epoch 5: 120/528 Loss: 25.275597
2022-06-24 13:43: Train Epoch 5: 140/528 Loss: 25.409382
2022-06-24 13:44: Train Epoch 5: 160/528 Loss: 28.142189
2022-06-24 13:45: Train Epoch 5: 180/528 Loss: 26.794893
2022-06-24 13:46: Train Epoch 5: 200/528 Loss: 26.630629
2022-06-24 13:47: Train Epoch 5: 220/528 Loss: 25.320299
2022-06-24 13:48: Train Epoch 5: 240/528 Loss: 27.982874
2022-06-24 13:49: Train Epoch 5: 260/528 Loss: 26.165924
2022-06-24 13:50: Train Epoch 5: 280/528 Loss: 27.409382
2022-06-24 13:50: Train Epoch 5: 300/528 Loss: 25.800528
2022-06-24 13:51: Train Epoch 5: 320/528 Loss: 23.844603
2022-06-24 13:52: Train Epoch 5: 340/528 Loss: 28.880014
2022-06-24 13:53: Train Epoch 5: 360/528 Loss: 28.697546
2022-06-24 13:54: Train Epoch 5: 380/528 Loss: 23.930691
2022-06-24 13:55: Train Epoch 5: 400/528 Loss: 24.902653
2022-06-24 13:55: Train Epoch 5: 420/528 Loss: 29.975309
2022-06-24 13:56: Train Epoch 5: 440/528 Loss: 25.472626
2022-06-24 13:57: Train Epoch 5: 460/528 Loss: 25.656654
2022-06-24 13:58: Train Epoch 5: 480/528 Loss: 23.336853
2022-06-24 13:59: Train Epoch 5: 500/528 Loss: 24.605841
2022-06-24 14:00: Train Epoch 5: 520/528 Loss: 27.629065
2022-06-24 14:00: **********Train Epoch 5: averaged Loss: 26.016070
2022-06-24 14:03: **********Val Epoch 5: average Loss: 26.011190
2022-06-24 14:03: Train Epoch 6: 0/528 Loss: 27.196548
2022-06-24 14:04: Train Epoch 6: 20/528 Loss: 24.638491
2022-06-24 14:05: Train Epoch 6: 40/528 Loss: 23.608002
2022-06-24 14:06: Train Epoch 6: 60/528 Loss: 23.383270
2022-06-24 14:07: Train Epoch 6: 80/528 Loss: 23.007446
2022-06-24 14:08: Train Epoch 6: 100/528 Loss: 22.783621
2022-06-24 14:09: Train Epoch 6: 120/528 Loss: 22.403330
2022-06-24 14:10: Train Epoch 6: 140/528 Loss: 26.387226
2022-06-24 14:11: Train Epoch 6: 160/528 Loss: 27.472095
2022-06-24 14:12: Train Epoch 6: 180/528 Loss: 25.665421
2022-06-24 14:13: Train Epoch 6: 200/528 Loss: 26.883810
2022-06-24 14:14: Train Epoch 6: 220/528 Loss: 24.360027
2022-06-24 14:14: Train Epoch 6: 240/528 Loss: 23.478842
2022-06-24 14:15: Train Epoch 6: 260/528 Loss: 23.988214
2022-06-24 14:16: Train Epoch 6: 280/528 Loss: 26.501974
2022-06-24 14:17: Train Epoch 6: 300/528 Loss: 25.166344
2022-06-24 14:18: Train Epoch 6: 320/528 Loss: 25.086151
2022-06-24 14:19: Train Epoch 6: 340/528 Loss: 24.399664
2022-06-24 14:19: Train Epoch 6: 360/528 Loss: 24.588440
2022-06-24 14:20: Train Epoch 6: 380/528 Loss: 22.961775
2022-06-24 14:21: Train Epoch 6: 400/528 Loss: 24.878174
2022-06-24 14:22: Train Epoch 6: 420/528 Loss: 26.478743
2022-06-24 14:23: Train Epoch 6: 440/528 Loss: 22.651466
2022-06-24 14:24: Train Epoch 6: 460/528 Loss: 23.886642
2022-06-24 14:25: Train Epoch 6: 480/528 Loss: 25.397131
2022-06-24 14:26: Train Epoch 6: 500/528 Loss: 24.558313
2022-06-24 14:27: Train Epoch 6: 520/528 Loss: 24.591339
2022-06-24 14:27: **********Train Epoch 6: averaged Loss: 25.170925
2022-06-24 14:30: **********Val Epoch 6: average Loss: 23.841270
2022-06-24 14:30: *********************************Current best model saved!
2022-06-24 14:30: Train Epoch 7: 0/528 Loss: 24.233706
2022-06-24 14:31: Train Epoch 7: 20/528 Loss: 24.199387
2022-06-24 14:32: Train Epoch 7: 40/528 Loss: 23.885765
2022-06-24 14:33: Train Epoch 7: 60/528 Loss: 21.604832
2022-06-24 14:34: Train Epoch 7: 80/528 Loss: 25.081535
2022-06-24 14:35: Train Epoch 7: 100/528 Loss: 24.406574
2022-06-24 14:36: Train Epoch 7: 120/528 Loss: 26.017967
2022-06-24 14:37: Train Epoch 7: 140/528 Loss: 25.178310
2022-06-24 14:38: Train Epoch 7: 160/528 Loss: 25.552294
2022-06-24 14:39: Train Epoch 7: 180/528 Loss: 23.518673
2022-06-24 14:39: Train Epoch 7: 200/528 Loss: 24.227228
2022-06-24 14:40: Train Epoch 7: 220/528 Loss: 26.670546
2022-06-24 14:41: Train Epoch 7: 240/528 Loss: 25.087971
2022-06-24 14:42: Train Epoch 7: 260/528 Loss: 23.241566
2022-06-24 14:43: Train Epoch 7: 280/528 Loss: 24.908426
2022-06-24 14:44: Train Epoch 7: 300/528 Loss: 23.789219
2022-06-24 14:44: Train Epoch 7: 320/528 Loss: 23.495586
2022-06-24 14:45: Train Epoch 7: 340/528 Loss: 25.010765
2022-06-24 14:46: Train Epoch 7: 360/528 Loss: 23.143354
2022-06-24 14:47: Train Epoch 7: 380/528 Loss: 23.470390
2022-06-24 14:48: Train Epoch 7: 400/528 Loss: 25.060419
2022-06-24 14:49: Train Epoch 7: 420/528 Loss: 24.755699
2022-06-24 14:50: Train Epoch 7: 440/528 Loss: 26.657793
2022-06-24 14:51: Train Epoch 7: 460/528 Loss: 24.609385
2022-06-24 14:52: Train Epoch 7: 480/528 Loss: 23.979084
2022-06-24 14:53: Train Epoch 7: 500/528 Loss: 21.793472
2022-06-24 14:54: Train Epoch 7: 520/528 Loss: 22.084877
2022-06-24 14:54: **********Train Epoch 7: averaged Loss: 24.401375
2022-06-24 14:57: **********Val Epoch 7: average Loss: 24.758105
2022-06-24 14:57: Train Epoch 8: 0/528 Loss: 25.588015
2022-06-24 14:58: Train Epoch 8: 20/528 Loss: 25.481668
2022-06-24 14:59: Train Epoch 8: 40/528 Loss: 24.370359
2022-06-24 15:00: Train Epoch 8: 60/528 Loss: 24.609941
2022-06-24 15:01: Train Epoch 8: 80/528 Loss: 23.096163
2022-06-24 15:02: Train Epoch 8: 100/528 Loss: 21.901041
2022-06-24 15:03: Train Epoch 8: 120/528 Loss: 26.334295
2022-06-24 15:03: Train Epoch 8: 140/528 Loss: 24.399317
2022-06-24 15:04: Train Epoch 8: 160/528 Loss: 23.402699
2022-06-24 15:05: Train Epoch 8: 180/528 Loss: 22.911926
2022-06-24 15:06: Train Epoch 8: 200/528 Loss: 23.317154
2022-06-24 15:07: Train Epoch 8: 220/528 Loss: 23.256678
2022-06-24 15:08: Train Epoch 8: 240/528 Loss: 24.033236
2022-06-24 15:09: Train Epoch 8: 260/528 Loss: 22.847134
2022-06-24 15:09: Train Epoch 8: 280/528 Loss: 24.301933
2022-06-24 15:10: Train Epoch 8: 300/528 Loss: 25.972616
2022-06-24 15:11: Train Epoch 8: 320/528 Loss: 25.133394
2022-06-24 15:12: Train Epoch 8: 340/528 Loss: 22.321569
2022-06-24 15:13: Train Epoch 8: 360/528 Loss: 22.760576
2022-06-24 15:14: Train Epoch 8: 380/528 Loss: 24.811804
2022-06-24 15:15: Train Epoch 8: 400/528 Loss: 23.057560
2022-06-24 15:16: Train Epoch 8: 420/528 Loss: 24.794222
2022-06-24 15:17: Train Epoch 8: 440/528 Loss: 22.028463
2022-06-24 15:18: Train Epoch 8: 460/528 Loss: 23.542253
2022-06-24 15:18: Train Epoch 8: 480/528 Loss: 24.237722
2022-06-24 15:19: Train Epoch 8: 500/528 Loss: 22.578243
2022-06-24 15:20: Train Epoch 8: 520/528 Loss: 21.531860
2022-06-24 15:21: **********Train Epoch 8: averaged Loss: 23.880274
2022-06-24 15:24: **********Val Epoch 8: average Loss: 23.165448
2022-06-24 15:24: *********************************Current best model saved!
2022-06-24 15:24: Train Epoch 9: 0/528 Loss: 23.869768
2022-06-24 15:25: Train Epoch 9: 20/528 Loss: 23.732397
2022-06-24 15:26: Train Epoch 9: 40/528 Loss: 23.269278
2022-06-24 15:27: Train Epoch 9: 60/528 Loss: 22.614058
2022-06-24 15:28: Train Epoch 9: 80/528 Loss: 23.279188
2022-06-24 15:28: Train Epoch 9: 100/528 Loss: 22.272015
2022-06-24 15:29: Train Epoch 9: 120/528 Loss: 24.245567
2022-06-24 15:30: Train Epoch 9: 140/528 Loss: 22.334393
2022-06-24 15:31: Train Epoch 9: 160/528 Loss: 21.430264
2022-06-24 15:32: Train Epoch 9: 180/528 Loss: 23.150089
2022-06-24 15:33: Train Epoch 9: 200/528 Loss: 24.612694
2022-06-24 15:34: Train Epoch 9: 220/528 Loss: 25.396137
2022-06-24 15:34: Train Epoch 9: 240/528 Loss: 24.796751
2022-06-24 15:35: Train Epoch 9: 260/528 Loss: 22.778942
2022-06-24 15:36: Train Epoch 9: 280/528 Loss: 21.378147
2022-06-24 15:37: Train Epoch 9: 300/528 Loss: 24.404490
2022-06-24 15:38: Train Epoch 9: 320/528 Loss: 23.986799
2022-06-24 15:39: Train Epoch 9: 340/528 Loss: 25.261843
2022-06-24 15:40: Train Epoch 9: 360/528 Loss: 23.009506
2022-06-24 15:41: Train Epoch 9: 380/528 Loss: 22.289312
2022-06-24 15:42: Train Epoch 9: 400/528 Loss: 22.723839
2022-06-24 15:43: Train Epoch 9: 420/528 Loss: 22.535208
2022-06-24 15:44: Train Epoch 9: 440/528 Loss: 22.955250
2022-06-24 15:45: Train Epoch 9: 460/528 Loss: 23.667450
2022-06-24 15:46: Train Epoch 9: 480/528 Loss: 24.013853
2022-06-24 15:46: Train Epoch 9: 500/528 Loss: 23.253269
2022-06-24 15:47: Train Epoch 9: 520/528 Loss: 23.684891
2022-06-24 15:48: **********Train Epoch 9: averaged Loss: 23.307412
2022-06-24 15:51: **********Val Epoch 9: average Loss: 22.847368
2022-06-24 15:51: *********************************Current best model saved!
2022-06-24 15:51: Train Epoch 10: 0/528 Loss: 22.940762
2022-06-24 15:52: Train Epoch 10: 20/528 Loss: 22.931732
2022-06-24 15:53: Train Epoch 10: 40/528 Loss: 21.450628
2022-06-24 15:53: Train Epoch 10: 60/528 Loss: 22.994806
2022-06-24 15:54: Train Epoch 10: 80/528 Loss: 23.406446
2022-06-24 15:55: Train Epoch 10: 100/528 Loss: 22.765717
2022-06-24 15:56: Train Epoch 10: 120/528 Loss: 21.363649
2022-06-24 15:57: Train Epoch 10: 140/528 Loss: 21.390900
2022-06-24 15:58: Train Epoch 10: 160/528 Loss: 21.603300
2022-06-24 15:59: Train Epoch 10: 180/528 Loss: 21.365795
2022-06-24 15:59: Train Epoch 10: 200/528 Loss: 24.116154
2022-06-24 16:00: Train Epoch 10: 220/528 Loss: 22.204424
2022-06-24 16:01: Train Epoch 10: 240/528 Loss: 23.743481
2022-06-24 16:02: Train Epoch 10: 260/528 Loss: 22.135750
2022-06-24 16:03: Train Epoch 10: 280/528 Loss: 22.869600
2022-06-24 16:04: Train Epoch 10: 300/528 Loss: 23.391891
2022-06-24 16:05: Train Epoch 10: 320/528 Loss: 23.080177
2022-06-24 16:05: Train Epoch 10: 340/528 Loss: 23.037386
2022-06-24 16:06: Train Epoch 10: 360/528 Loss: 22.261192
2022-06-24 16:07: Train Epoch 10: 380/528 Loss: 21.032246
2022-06-24 16:08: Train Epoch 10: 400/528 Loss: 23.078806
2022-06-24 16:09: Train Epoch 10: 420/528 Loss: 22.040140
2022-06-24 16:10: Train Epoch 10: 440/528 Loss: 22.732475
2022-06-24 16:11: Train Epoch 10: 460/528 Loss: 21.659641
2022-06-24 16:12: Train Epoch 10: 480/528 Loss: 23.571352
2022-06-24 16:13: Train Epoch 10: 500/528 Loss: 24.304647
2022-06-24 16:14: Train Epoch 10: 520/528 Loss: 21.615055
2022-06-24 16:14: **********Train Epoch 10: averaged Loss: 22.823824
2022-06-24 16:17: **********Val Epoch 10: average Loss: 22.623419
2022-06-24 16:17: *********************************Current best model saved!
2022-06-24 16:17: Train Epoch 11: 0/528 Loss: 21.711401
2022-06-24 16:18: Train Epoch 11: 20/528 Loss: 23.210806
2022-06-24 16:19: Train Epoch 11: 40/528 Loss: 22.855171
2022-06-24 16:20: Train Epoch 11: 60/528 Loss: 22.811241
2022-06-24 16:21: Train Epoch 11: 80/528 Loss: 20.173002
2022-06-24 16:22: Train Epoch 11: 100/528 Loss: 20.572866
2022-06-24 16:23: Train Epoch 11: 120/528 Loss: 22.738037
2022-06-24 16:23: Train Epoch 11: 140/528 Loss: 22.428509
2022-06-24 16:24: Train Epoch 11: 160/528 Loss: 22.785624
2022-06-24 16:25: Train Epoch 11: 180/528 Loss: 21.227001
2022-06-24 16:26: Train Epoch 11: 200/528 Loss: 20.252216
2022-06-24 16:27: Train Epoch 11: 220/528 Loss: 23.425322
2022-06-24 16:28: Train Epoch 11: 240/528 Loss: 23.135559
2022-06-24 16:29: Train Epoch 11: 260/528 Loss: 23.132046
2022-06-24 16:30: Train Epoch 11: 280/528 Loss: 23.316690
2022-06-24 16:30: Train Epoch 11: 300/528 Loss: 21.976465
2022-06-24 16:31: Train Epoch 11: 320/528 Loss: 22.415682
2022-06-24 16:32: Train Epoch 11: 340/528 Loss: 21.924488
2022-06-24 16:33: Train Epoch 11: 360/528 Loss: 23.726236
2022-06-24 16:34: Train Epoch 11: 380/528 Loss: 23.067352
2022-06-24 16:35: Train Epoch 11: 400/528 Loss: 24.793116
2022-06-24 16:36: Train Epoch 11: 420/528 Loss: 21.646914
2022-06-24 16:37: Train Epoch 11: 440/528 Loss: 20.283718
2022-06-24 16:38: Train Epoch 11: 460/528 Loss: 23.747726
2022-06-24 16:39: Train Epoch 11: 480/528 Loss: 24.243628
2022-06-24 16:40: Train Epoch 11: 500/528 Loss: 23.177605
2022-06-24 16:41: Train Epoch 11: 520/528 Loss: 23.070642
2022-06-24 16:41: **********Train Epoch 11: averaged Loss: 22.655025
2022-06-24 16:44: **********Val Epoch 11: average Loss: 22.537578
2022-06-24 16:44: *********************************Current best model saved!
2022-06-24 16:44: Train Epoch 12: 0/528 Loss: 22.271812
2022-06-24 16:45: Train Epoch 12: 20/528 Loss: 22.129660
2022-06-24 16:46: Train Epoch 12: 40/528 Loss: 22.042065
2022-06-24 16:47: Train Epoch 12: 60/528 Loss: 22.536612
2022-06-24 16:48: Train Epoch 12: 80/528 Loss: 21.391937
2022-06-24 16:48: Train Epoch 12: 100/528 Loss: 19.426294
2022-06-24 16:49: Train Epoch 12: 120/528 Loss: 22.487617
2022-06-24 16:50: Train Epoch 12: 140/528 Loss: 22.916029
2022-06-24 16:51: Train Epoch 12: 160/528 Loss: 20.105358
2022-06-24 16:52: Train Epoch 12: 180/528 Loss: 21.124367
2022-06-24 16:53: Train Epoch 12: 200/528 Loss: 23.991274
2022-06-24 16:53: Train Epoch 12: 220/528 Loss: 22.239946
2022-06-24 16:54: Train Epoch 12: 240/528 Loss: 21.939323
2022-06-24 16:55: Train Epoch 12: 260/528 Loss: 21.172888
2022-06-24 16:56: Train Epoch 12: 280/528 Loss: 22.518318
2022-06-24 16:57: Train Epoch 12: 300/528 Loss: 24.258055
2022-06-24 16:58: Train Epoch 12: 320/528 Loss: 21.603609
2022-06-24 16:59: Train Epoch 12: 340/528 Loss: 21.625120
2022-06-24 17:00: Train Epoch 12: 360/528 Loss: 23.375231
2022-06-24 17:01: Train Epoch 12: 380/528 Loss: 22.678207
2022-06-24 17:02: Train Epoch 12: 400/528 Loss: 24.080603
2022-06-24 17:03: Train Epoch 12: 420/528 Loss: 21.690481
2022-06-24 17:04: Train Epoch 12: 440/528 Loss: 21.435242
2022-06-24 17:05: Train Epoch 12: 460/528 Loss: 22.857391
2022-06-24 17:06: Train Epoch 12: 480/528 Loss: 23.269014
2022-06-24 17:07: Train Epoch 12: 500/528 Loss: 21.502977
2022-06-24 17:08: Train Epoch 12: 520/528 Loss: 18.924475
2022-06-24 17:08: **********Train Epoch 12: averaged Loss: 22.321618
2022-06-24 17:11: **********Val Epoch 12: average Loss: 23.214590
2022-06-24 17:11: Train Epoch 13: 0/528 Loss: 22.626244
2022-06-24 17:12: Train Epoch 13: 20/528 Loss: 21.912111
2022-06-24 17:13: Train Epoch 13: 40/528 Loss: 21.673317
2022-06-24 17:13: Train Epoch 13: 60/528 Loss: 22.037064
2022-06-24 17:14: Train Epoch 13: 80/528 Loss: 25.318512
2022-06-24 17:15: Train Epoch 13: 100/528 Loss: 22.469608
2022-06-24 17:16: Train Epoch 13: 120/528 Loss: 21.196592
2022-06-24 17:17: Train Epoch 13: 140/528 Loss: 23.809999
2022-06-24 17:18: Train Epoch 13: 160/528 Loss: 22.300459
2022-06-24 17:18: Train Epoch 13: 180/528 Loss: 24.078623
2022-06-24 17:19: Train Epoch 13: 200/528 Loss: 20.689404
2022-06-24 17:20: Train Epoch 13: 220/528 Loss: 21.406652
2022-06-24 17:21: Train Epoch 13: 240/528 Loss: 23.921700
2022-06-24 17:22: Train Epoch 13: 260/528 Loss: 22.718010
2022-06-24 17:23: Train Epoch 13: 280/528 Loss: 22.753698
2022-06-24 17:24: Train Epoch 13: 300/528 Loss: 21.593618
2022-06-24 17:25: Train Epoch 13: 320/528 Loss: 25.373999
2022-06-24 17:26: Train Epoch 13: 340/528 Loss: 23.455412
2022-06-24 17:27: Train Epoch 13: 360/528 Loss: 22.691311
2022-06-24 17:28: Train Epoch 13: 380/528 Loss: 19.571886
2022-06-24 17:29: Train Epoch 13: 400/528 Loss: 22.251526
2022-06-24 17:30: Train Epoch 13: 420/528 Loss: 22.292166
2022-06-24 17:31: Train Epoch 13: 440/528 Loss: 22.166861
2022-06-24 17:32: Train Epoch 13: 460/528 Loss: 22.680794
2022-06-24 17:33: Train Epoch 13: 480/528 Loss: 22.545471
2022-06-24 17:34: Train Epoch 13: 500/528 Loss: 24.587387
2022-06-24 17:34: Train Epoch 13: 520/528 Loss: 20.763325
2022-06-24 17:35: **********Train Epoch 13: averaged Loss: 22.282134
2022-06-24 17:37: **********Val Epoch 13: average Loss: 22.102668
2022-06-24 17:37: *********************************Current best model saved!
2022-06-24 17:37: Train Epoch 14: 0/528 Loss: 20.931572
2022-06-24 17:38: Train Epoch 14: 20/528 Loss: 21.871916
2022-06-24 17:39: Train Epoch 14: 40/528 Loss: 21.936064
2022-06-24 17:40: Train Epoch 14: 60/528 Loss: 21.458487
2022-06-24 17:41: Train Epoch 14: 80/528 Loss: 22.833389
2022-06-24 17:42: Train Epoch 14: 100/528 Loss: 22.358181
2022-06-24 17:42: Train Epoch 14: 120/528 Loss: 20.690821
2022-06-24 17:43: Train Epoch 14: 140/528 Loss: 21.334845
2022-06-24 17:44: Train Epoch 14: 160/528 Loss: 20.837006
2022-06-24 17:45: Train Epoch 14: 180/528 Loss: 23.071764
2022-06-24 17:46: Train Epoch 14: 200/528 Loss: 22.943687
2022-06-24 17:47: Train Epoch 14: 220/528 Loss: 21.848646
2022-06-24 17:48: Train Epoch 14: 240/528 Loss: 24.058475
2022-06-24 17:48: Train Epoch 14: 260/528 Loss: 21.471758
2022-06-24 17:49: Train Epoch 14: 280/528 Loss: 24.056814
2022-06-24 17:50: Train Epoch 14: 300/528 Loss: 21.665030
2022-06-24 17:51: Train Epoch 14: 320/528 Loss: 20.378704
2022-06-24 17:52: Train Epoch 14: 340/528 Loss: 23.405045
2022-06-24 17:53: Train Epoch 14: 360/528 Loss: 21.216825
2022-06-24 17:54: Train Epoch 14: 380/528 Loss: 22.807110
2022-06-24 17:55: Train Epoch 14: 400/528 Loss: 22.030205
2022-06-24 17:56: Train Epoch 14: 420/528 Loss: 20.525017
2022-06-24 17:57: Train Epoch 14: 440/528 Loss: 21.145618
2022-06-24 17:58: Train Epoch 14: 460/528 Loss: 22.794611
2022-06-24 17:59: Train Epoch 14: 480/528 Loss: 22.415121
2022-06-24 18:00: Train Epoch 14: 500/528 Loss: 21.968267
2022-06-24 18:01: Train Epoch 14: 520/528 Loss: 22.454975
2022-06-24 18:01: **********Train Epoch 14: averaged Loss: 21.980956
2022-06-24 18:04: **********Val Epoch 14: average Loss: 23.100094
2022-06-24 18:04: Train Epoch 15: 0/528 Loss: 22.616648
2022-06-24 18:05: Train Epoch 15: 20/528 Loss: 21.117493
2022-06-24 18:05: Train Epoch 15: 40/528 Loss: 19.416895
2022-06-24 18:06: Train Epoch 15: 60/528 Loss: 20.514433
2022-06-24 18:07: Train Epoch 15: 80/528 Loss: 21.533415
2022-06-24 18:08: Train Epoch 15: 100/528 Loss: 21.035437
2022-06-24 18:09: Train Epoch 15: 120/528 Loss: 22.435608
2022-06-24 18:09: Train Epoch 15: 140/528 Loss: 22.994677
2022-06-24 18:10: Train Epoch 15: 160/528 Loss: 22.165791
2022-06-24 18:11: Train Epoch 15: 180/528 Loss: 20.966574
2022-06-24 18:12: Train Epoch 15: 200/528 Loss: 20.398151
2022-06-24 18:13: Train Epoch 15: 220/528 Loss: 22.463470
2022-06-24 18:14: Train Epoch 15: 240/528 Loss: 20.189615
2022-06-24 18:15: Train Epoch 15: 260/528 Loss: 20.397669
2022-06-24 18:16: Train Epoch 15: 280/528 Loss: 21.280807
2022-06-24 18:17: Train Epoch 15: 300/528 Loss: 20.980282
2022-06-24 18:17: Train Epoch 15: 320/528 Loss: 22.302479
2022-06-24 18:18: Train Epoch 15: 340/528 Loss: 21.933372
2022-06-24 18:19: Train Epoch 15: 360/528 Loss: 20.252073
2022-06-24 18:20: Train Epoch 15: 380/528 Loss: 19.994135
2022-06-24 18:21: Train Epoch 15: 400/528 Loss: 24.068779
2022-06-24 18:22: Train Epoch 15: 420/528 Loss: 24.180944
2022-06-24 18:23: Train Epoch 15: 440/528 Loss: 23.628996
2022-06-24 18:24: Train Epoch 15: 460/528 Loss: 21.186211
2022-06-24 18:25: Train Epoch 15: 480/528 Loss: 21.870930
2022-06-24 18:26: Train Epoch 15: 500/528 Loss: 25.049217
2022-06-24 18:27: Train Epoch 15: 520/528 Loss: 21.931900
2022-06-24 18:27: **********Train Epoch 15: averaged Loss: 22.045461
2022-06-24 18:30: **********Val Epoch 15: average Loss: 21.661497
2022-06-24 18:30: *********************************Current best model saved!
2022-06-24 18:30: Train Epoch 16: 0/528 Loss: 23.079512
2022-06-24 18:31: Train Epoch 16: 20/528 Loss: 22.876369
2022-06-24 18:32: Train Epoch 16: 40/528 Loss: 20.780205
2022-06-24 18:33: Train Epoch 16: 60/528 Loss: 20.603563
2022-06-24 18:33: Train Epoch 16: 80/528 Loss: 22.312201
2022-06-24 18:34: Train Epoch 16: 100/528 Loss: 22.618593
2022-06-24 18:35: Train Epoch 16: 120/528 Loss: 21.782366
2022-06-24 18:36: Train Epoch 16: 140/528 Loss: 22.542324
2022-06-24 18:37: Train Epoch 16: 160/528 Loss: 21.816563
2022-06-24 18:38: Train Epoch 16: 180/528 Loss: 20.797857
2022-06-24 18:38: Train Epoch 16: 200/528 Loss: 20.898726
2022-06-24 18:39: Train Epoch 16: 220/528 Loss: 21.632057
2022-06-24 18:40: Train Epoch 16: 240/528 Loss: 21.165968
2022-06-24 18:41: Train Epoch 16: 260/528 Loss: 21.293077
2022-06-24 18:42: Train Epoch 16: 280/528 Loss: 20.968243
2022-06-24 18:43: Train Epoch 16: 300/528 Loss: 20.076860
2022-06-24 18:44: Train Epoch 16: 320/528 Loss: 22.209839
2022-06-24 18:45: Train Epoch 16: 340/528 Loss: 20.700790
2022-06-24 18:46: Train Epoch 16: 360/528 Loss: 22.878370
2022-06-24 18:47: Train Epoch 16: 380/528 Loss: 20.579912
2022-06-24 18:48: Train Epoch 16: 400/528 Loss: 20.088923
2022-06-24 18:49: Train Epoch 16: 420/528 Loss: 22.750486
2022-06-24 18:49: Train Epoch 16: 440/528 Loss: 21.829319
2022-06-24 18:50: Train Epoch 16: 460/528 Loss: 22.206480
2022-06-24 18:51: Train Epoch 16: 480/528 Loss: 22.086592
2022-06-24 18:52: Train Epoch 16: 500/528 Loss: 20.654131
2022-06-24 18:53: Train Epoch 16: 520/528 Loss: 21.612904
2022-06-24 18:54: **********Train Epoch 16: averaged Loss: 21.602487
2022-06-24 18:56: **********Val Epoch 16: average Loss: 22.086933
2022-06-24 18:56: Train Epoch 17: 0/528 Loss: 20.889244
2022-06-24 18:57: Train Epoch 17: 20/528 Loss: 22.548220
2022-06-24 18:58: Train Epoch 17: 40/528 Loss: 21.865595
2022-06-24 18:59: Train Epoch 17: 60/528 Loss: 19.460363
2022-06-24 19:00: Train Epoch 17: 80/528 Loss: 22.633139
2022-06-24 19:00: Train Epoch 17: 100/528 Loss: 22.243994
2022-06-24 19:01: Train Epoch 17: 120/528 Loss: 20.321636
2022-06-24 19:02: Train Epoch 17: 140/528 Loss: 20.793869
2022-06-24 19:03: Train Epoch 17: 160/528 Loss: 21.313620
2022-06-24 19:04: Train Epoch 17: 180/528 Loss: 19.098936
2022-06-24 19:05: Train Epoch 17: 200/528 Loss: 20.910267
2022-06-24 19:06: Train Epoch 17: 220/528 Loss: 20.045675
2022-06-24 19:06: Train Epoch 17: 240/528 Loss: 22.171560
2022-06-24 19:07: Train Epoch 17: 260/528 Loss: 20.994858
2022-06-24 19:08: Train Epoch 17: 280/528 Loss: 19.766994
2022-06-24 19:09: Train Epoch 17: 300/528 Loss: 19.571547
2022-06-24 19:10: Train Epoch 17: 320/528 Loss: 20.308037
2022-06-24 19:11: Train Epoch 17: 340/528 Loss: 21.955664
2022-06-24 19:12: Train Epoch 17: 360/528 Loss: 22.044573
2022-06-24 19:13: Train Epoch 17: 380/528 Loss: 21.631037
2022-06-24 19:14: Train Epoch 17: 400/528 Loss: 21.427275
2022-06-24 19:15: Train Epoch 17: 420/528 Loss: 22.055506
2022-06-24 19:16: Train Epoch 17: 440/528 Loss: 19.951899
2022-06-24 19:17: Train Epoch 17: 460/528 Loss: 21.471270
2022-06-24 19:18: Train Epoch 17: 480/528 Loss: 20.950506
2022-06-24 19:19: Train Epoch 17: 500/528 Loss: 21.315556
2022-06-24 19:19: Train Epoch 17: 520/528 Loss: 21.454475
2022-06-24 19:20: **********Train Epoch 17: averaged Loss: 21.570757
2022-06-24 19:23: **********Val Epoch 17: average Loss: 21.911576
2022-06-24 19:23: Train Epoch 18: 0/528 Loss: 21.546967
2022-06-24 19:23: Train Epoch 18: 20/528 Loss: 21.924383
2022-06-24 19:24: Train Epoch 18: 40/528 Loss: 20.805571
2022-06-24 19:25: Train Epoch 18: 60/528 Loss: 22.139414
2022-06-24 19:26: Train Epoch 18: 80/528 Loss: 20.742746
2022-06-24 19:27: Train Epoch 18: 100/528 Loss: 23.317600
2022-06-24 19:28: Train Epoch 18: 120/528 Loss: 23.061123
2022-06-24 19:28: Train Epoch 18: 140/528 Loss: 21.409811
2022-06-24 19:29: Train Epoch 18: 160/528 Loss: 21.452679
2022-06-24 19:30: Train Epoch 18: 180/528 Loss: 21.223143
2022-06-24 19:31: Train Epoch 18: 200/528 Loss: 21.515547
2022-06-24 19:32: Train Epoch 18: 220/528 Loss: 19.915913
2022-06-24 19:33: Train Epoch 18: 240/528 Loss: 20.276707
2022-06-24 19:34: Train Epoch 18: 260/528 Loss: 22.054234
2022-06-24 19:34: Train Epoch 18: 280/528 Loss: 21.015497
2022-06-24 19:35: Train Epoch 18: 300/528 Loss: 22.531195
2022-06-24 19:36: Train Epoch 18: 320/528 Loss: 20.794281
2022-06-24 19:37: Train Epoch 18: 340/528 Loss: 19.618078
2022-06-24 19:38: Train Epoch 18: 360/528 Loss: 21.583256
2022-06-24 19:39: Train Epoch 18: 380/528 Loss: 20.716270
2022-06-24 19:40: Train Epoch 18: 400/528 Loss: 22.847536
2022-06-24 19:41: Train Epoch 18: 420/528 Loss: 23.031845
2022-06-24 19:42: Train Epoch 18: 440/528 Loss: 20.125076
2022-06-24 19:43: Train Epoch 18: 460/528 Loss: 20.548082
2022-06-24 19:44: Train Epoch 18: 480/528 Loss: 21.145010
2022-06-24 19:45: Train Epoch 18: 500/528 Loss: 22.385160
2022-06-24 19:46: Train Epoch 18: 520/528 Loss: 20.595455
2022-06-24 19:46: **********Train Epoch 18: averaged Loss: 21.368437
2022-06-24 19:49: **********Val Epoch 18: average Loss: 21.902802
2022-06-24 19:49: Train Epoch 19: 0/528 Loss: 18.887373
2022-06-24 19:50: Train Epoch 19: 20/528 Loss: 19.905682
2022-06-24 19:50: Train Epoch 19: 40/528 Loss: 22.825245
2022-06-24 19:51: Train Epoch 19: 60/528 Loss: 23.409494
2022-06-24 19:52: Train Epoch 19: 80/528 Loss: 21.471933
2022-06-24 19:53: Train Epoch 19: 100/528 Loss: 20.020817
2022-06-24 19:54: Train Epoch 19: 120/528 Loss: 22.525105
2022-06-24 19:55: Train Epoch 19: 140/528 Loss: 21.981131
2022-06-24 19:55: Train Epoch 19: 160/528 Loss: 20.119267
2022-06-24 19:56: Train Epoch 19: 180/528 Loss: 20.386467
2022-06-24 19:57: Train Epoch 19: 200/528 Loss: 20.007893
2022-06-24 19:58: Train Epoch 19: 220/528 Loss: 21.565847
2022-06-24 19:59: Train Epoch 19: 240/528 Loss: 20.505342
2022-06-24 20:00: Train Epoch 19: 260/528 Loss: 23.729670
2022-06-24 20:01: Train Epoch 19: 280/528 Loss: 22.266813
2022-06-24 20:01: Train Epoch 19: 300/528 Loss: 20.738199
2022-06-24 20:02: Train Epoch 19: 320/528 Loss: 20.724060
2022-06-24 20:03: Train Epoch 19: 340/528 Loss: 21.007132
2022-06-24 20:04: Train Epoch 19: 360/528 Loss: 20.455370
2022-06-24 20:05: Train Epoch 19: 380/528 Loss: 20.056852
2022-06-24 20:06: Train Epoch 19: 400/528 Loss: 22.064434
2022-06-24 20:07: Train Epoch 19: 420/528 Loss: 21.604570
2022-06-24 20:08: Train Epoch 19: 440/528 Loss: 21.278856
2022-06-24 20:09: Train Epoch 19: 460/528 Loss: 20.640158
2022-06-24 20:10: Train Epoch 19: 480/528 Loss: 21.614580
2022-06-24 20:11: Train Epoch 19: 500/528 Loss: 22.651361
2022-06-24 20:12: Train Epoch 19: 520/528 Loss: 22.208834
2022-06-24 20:12: **********Train Epoch 19: averaged Loss: 21.313778
2022-06-24 20:15: **********Val Epoch 19: average Loss: 21.912852
2022-06-24 20:15: Train Epoch 20: 0/528 Loss: 20.835794
2022-06-24 20:16: Train Epoch 20: 20/528 Loss: 21.637848
2022-06-24 20:17: Train Epoch 20: 40/528 Loss: 21.903770
2022-06-24 20:18: Train Epoch 20: 60/528 Loss: 20.757330
2022-06-24 20:18: Train Epoch 20: 80/528 Loss: 20.379513
2022-06-24 20:19: Train Epoch 20: 100/528 Loss: 20.572531
2022-06-24 20:20: Train Epoch 20: 120/528 Loss: 21.582428
2022-06-24 20:21: Train Epoch 20: 140/528 Loss: 21.206978
2022-06-24 20:22: Train Epoch 20: 160/528 Loss: 20.930103
2022-06-24 20:23: Train Epoch 20: 180/528 Loss: 21.584188
2022-06-24 20:23: Train Epoch 20: 200/528 Loss: 22.684364
2022-06-24 20:24: Train Epoch 20: 220/528 Loss: 21.153683
2022-06-24 20:25: Train Epoch 20: 240/528 Loss: 23.268051
2022-06-24 20:26: Train Epoch 20: 260/528 Loss: 21.396494
2022-06-24 20:27: Train Epoch 20: 280/528 Loss: 18.935097
2022-06-24 20:28: Train Epoch 20: 300/528 Loss: 21.959158
2022-06-24 20:29: Train Epoch 20: 320/528 Loss: 20.683174
2022-06-24 20:29: Train Epoch 20: 340/528 Loss: 21.764336
2022-06-24 20:30: Train Epoch 20: 360/528 Loss: 21.449057
2022-06-24 20:31: Train Epoch 20: 380/528 Loss: 22.163574
2022-06-24 20:32: Train Epoch 20: 400/528 Loss: 20.972118
2022-06-24 20:33: Train Epoch 20: 420/528 Loss: 19.893042
2022-06-24 20:34: Train Epoch 20: 440/528 Loss: 21.164494
2022-06-24 20:35: Train Epoch 20: 460/528 Loss: 20.517773
2022-06-24 20:36: Train Epoch 20: 480/528 Loss: 21.799934
2022-06-24 20:37: Train Epoch 20: 500/528 Loss: 23.593340
2022-06-24 20:38: Train Epoch 20: 520/528 Loss: 23.374067
2022-06-24 20:38: **********Train Epoch 20: averaged Loss: 21.204754
2022-06-24 20:41: **********Val Epoch 20: average Loss: 21.496161
2022-06-24 20:41: *********************************Current best model saved!
2022-06-24 20:41: Train Epoch 21: 0/528 Loss: 20.393839
2022-06-24 20:42: Train Epoch 21: 20/528 Loss: 21.786585
2022-06-24 20:43: Train Epoch 21: 40/528 Loss: 20.130316
2022-06-24 20:44: Train Epoch 21: 60/528 Loss: 19.355162
2022-06-24 20:45: Train Epoch 21: 80/528 Loss: 20.072868
2022-06-24 20:46: Train Epoch 21: 100/528 Loss: 21.048889
2022-06-24 20:47: Train Epoch 21: 120/528 Loss: 18.754374
2022-06-24 20:47: Train Epoch 21: 140/528 Loss: 21.605556
2022-06-24 20:48: Train Epoch 21: 160/528 Loss: 19.870117
2022-06-24 20:49: Train Epoch 21: 180/528 Loss: 19.049101
2022-06-24 20:50: Train Epoch 21: 200/528 Loss: 21.881269
2022-06-24 20:51: Train Epoch 21: 220/528 Loss: 21.349268
2022-06-24 20:52: Train Epoch 21: 240/528 Loss: 22.132633
2022-06-24 20:52: Train Epoch 21: 260/528 Loss: 19.622559
2022-06-24 20:53: Train Epoch 21: 280/528 Loss: 21.495255
2022-06-24 20:54: Train Epoch 21: 300/528 Loss: 22.085888
2022-06-24 20:55: Train Epoch 21: 320/528 Loss: 22.799822
2022-06-24 20:56: Train Epoch 21: 340/528 Loss: 20.369169
2022-06-24 20:57: Train Epoch 21: 360/528 Loss: 20.966867
2022-06-24 20:58: Train Epoch 21: 380/528 Loss: 20.796442
2022-06-24 20:59: Train Epoch 21: 400/528 Loss: 22.598171
2022-06-24 20:59: Train Epoch 21: 420/528 Loss: 20.116808
2022-06-24 21:00: Train Epoch 21: 440/528 Loss: 19.847923
2022-06-24 21:01: Train Epoch 21: 460/528 Loss: 21.225861
2022-06-24 21:02: Train Epoch 21: 480/528 Loss: 21.129076
2022-06-24 21:03: Train Epoch 21: 500/528 Loss: 20.236496
2022-06-24 21:04: Train Epoch 21: 520/528 Loss: 22.549877
2022-06-24 21:04: **********Train Epoch 21: averaged Loss: 21.171562
2022-06-24 21:07: **********Val Epoch 21: average Loss: 21.693017
2022-06-24 21:07: Train Epoch 22: 0/528 Loss: 21.480831
2022-06-24 21:08: Train Epoch 22: 20/528 Loss: 21.899757
2022-06-24 21:09: Train Epoch 22: 40/528 Loss: 21.630062
2022-06-24 21:10: Train Epoch 22: 60/528 Loss: 21.052319
2022-06-24 21:11: Train Epoch 22: 80/528 Loss: 22.026140
2022-06-24 21:12: Train Epoch 22: 100/528 Loss: 22.296690
2022-06-24 21:13: Train Epoch 22: 120/528 Loss: 21.646885
2022-06-24 21:14: Train Epoch 22: 140/528 Loss: 19.524891
2022-06-24 21:15: Train Epoch 22: 160/528 Loss: 22.557592
2022-06-24 21:16: Train Epoch 22: 180/528 Loss: 19.390186
2022-06-24 21:16: Train Epoch 22: 200/528 Loss: 21.877789
2022-06-24 21:17: Train Epoch 22: 220/528 Loss: 20.725819
2022-06-24 21:18: Train Epoch 22: 240/528 Loss: 21.797106
2022-06-24 21:19: Train Epoch 22: 260/528 Loss: 24.274132
2022-06-24 21:20: Train Epoch 22: 280/528 Loss: 20.255775
2022-06-24 21:20: Train Epoch 22: 300/528 Loss: 21.346365
2022-06-24 21:21: Train Epoch 22: 320/528 Loss: 21.040867
2022-06-24 21:22: Train Epoch 22: 340/528 Loss: 21.202467
2022-06-24 21:23: Train Epoch 22: 360/528 Loss: 22.498466
2022-06-24 21:24: Train Epoch 22: 380/528 Loss: 21.104116
2022-06-24 21:25: Train Epoch 22: 400/528 Loss: 20.901958
2022-06-24 21:26: Train Epoch 22: 420/528 Loss: 20.904676
2022-06-24 21:27: Train Epoch 22: 440/528 Loss: 19.444386
2022-06-24 21:27: Train Epoch 22: 460/528 Loss: 21.596804
2022-06-24 21:28: Train Epoch 22: 480/528 Loss: 20.647242
2022-06-24 21:29: Train Epoch 22: 500/528 Loss: 19.691372
2022-06-24 21:30: Train Epoch 22: 520/528 Loss: 21.960196
2022-06-24 21:30: **********Train Epoch 22: averaged Loss: 20.933618
2022-06-24 21:33: **********Val Epoch 22: average Loss: 21.650166
2022-06-24 21:33: Train Epoch 23: 0/528 Loss: 22.389322
2022-06-24 21:34: Train Epoch 23: 20/528 Loss: 22.805126
2022-06-24 21:35: Train Epoch 23: 40/528 Loss: 20.284792
2022-06-24 21:36: Train Epoch 23: 60/528 Loss: 22.676777
2022-06-24 21:37: Train Epoch 23: 80/528 Loss: 20.807253
2022-06-24 21:38: Train Epoch 23: 100/528 Loss: 22.075560
2022-06-24 21:39: Train Epoch 23: 120/528 Loss: 20.187170
2022-06-24 21:40: Train Epoch 23: 140/528 Loss: 20.607285
2022-06-24 21:41: Train Epoch 23: 160/528 Loss: 22.357079
2022-06-24 21:42: Train Epoch 23: 180/528 Loss: 19.677717
2022-06-24 21:43: Train Epoch 23: 200/528 Loss: 20.897951
2022-06-24 21:43: Train Epoch 23: 220/528 Loss: 20.799116
2022-06-24 21:44: Train Epoch 23: 240/528 Loss: 19.593517
2022-06-24 21:45: Train Epoch 23: 260/528 Loss: 22.100445
2022-06-24 21:46: Train Epoch 23: 280/528 Loss: 23.388535
2022-06-24 21:47: Train Epoch 23: 300/528 Loss: 20.456211
2022-06-24 21:48: Train Epoch 23: 320/528 Loss: 22.573647
2022-06-24 21:48: Train Epoch 23: 340/528 Loss: 22.175413
2022-06-24 21:49: Train Epoch 23: 360/528 Loss: 21.868610
2022-06-24 21:50: Train Epoch 23: 380/528 Loss: 21.342070
2022-06-24 21:51: Train Epoch 23: 400/528 Loss: 20.944115
2022-06-24 21:52: Train Epoch 23: 420/528 Loss: 22.538307
2022-06-24 21:53: Train Epoch 23: 440/528 Loss: 18.310678
2022-06-24 21:54: Train Epoch 23: 460/528 Loss: 21.643467
2022-06-24 21:55: Train Epoch 23: 480/528 Loss: 20.280176
2022-06-24 21:56: Train Epoch 23: 500/528 Loss: 19.836206
2022-06-24 21:57: Train Epoch 23: 520/528 Loss: 18.543032
2022-06-24 21:57: **********Train Epoch 23: averaged Loss: 20.986969
2022-06-24 22:00: **********Val Epoch 23: average Loss: 21.097399
2022-06-24 22:00: *********************************Current best model saved!
2022-06-24 22:00: Train Epoch 24: 0/528 Loss: 21.054853
2022-06-24 22:01: Train Epoch 24: 20/528 Loss: 18.887287
2022-06-24 22:02: Train Epoch 24: 40/528 Loss: 21.478580
2022-06-24 22:03: Train Epoch 24: 60/528 Loss: 19.093174
2022-06-24 22:04: Train Epoch 24: 80/528 Loss: 21.049646
2022-06-24 22:05: Train Epoch 24: 100/528 Loss: 21.419817
2022-06-24 22:06: Train Epoch 24: 120/528 Loss: 23.689346
2022-06-24 22:07: Train Epoch 24: 140/528 Loss: 21.059189
2022-06-24 22:08: Train Epoch 24: 160/528 Loss: 20.904865
2022-06-24 22:09: Train Epoch 24: 180/528 Loss: 19.874115
2022-06-24 22:09: Train Epoch 24: 200/528 Loss: 21.521748
2022-06-24 22:10: Train Epoch 24: 220/528 Loss: 20.569216
2022-06-24 22:11: Train Epoch 24: 240/528 Loss: 21.117334
2022-06-24 22:12: Train Epoch 24: 260/528 Loss: 20.025478
2022-06-24 22:13: Train Epoch 24: 280/528 Loss: 20.364712
2022-06-24 22:14: Train Epoch 24: 300/528 Loss: 19.749840
2022-06-24 22:15: Train Epoch 24: 320/528 Loss: 21.445984
2022-06-24 22:15: Train Epoch 24: 340/528 Loss: 19.964979
2022-06-24 22:16: Train Epoch 24: 360/528 Loss: 21.279411
2022-06-24 22:17: Train Epoch 24: 380/528 Loss: 19.137754
2022-06-24 22:18: Train Epoch 24: 400/528 Loss: 22.112440
2022-06-24 22:19: Train Epoch 24: 420/528 Loss: 20.075781
2022-06-24 22:20: Train Epoch 24: 440/528 Loss: 20.075500
2022-06-24 22:20: Train Epoch 24: 460/528 Loss: 21.126213
2022-06-24 22:21: Train Epoch 24: 480/528 Loss: 19.058929
2022-06-24 22:22: Train Epoch 24: 500/528 Loss: 19.731314
2022-06-24 22:23: Train Epoch 24: 520/528 Loss: 21.086964
2022-06-24 22:23: **********Train Epoch 24: averaged Loss: 20.793141
2022-06-24 22:26: **********Val Epoch 24: average Loss: 21.558841
2022-06-24 22:26: Train Epoch 25: 0/528 Loss: 22.350342
2022-06-24 22:27: Train Epoch 25: 20/528 Loss: 19.847466
2022-06-24 22:28: Train Epoch 25: 40/528 Loss: 21.021032
2022-06-24 22:29: Train Epoch 25: 60/528 Loss: 19.816139
2022-06-24 22:30: Train Epoch 25: 80/528 Loss: 21.076208
2022-06-24 22:31: Train Epoch 25: 100/528 Loss: 21.233408
2022-06-24 22:32: Train Epoch 25: 120/528 Loss: 20.018387
2022-06-24 22:33: Train Epoch 25: 140/528 Loss: 21.236408
2022-06-24 22:34: Train Epoch 25: 160/528 Loss: 21.472948
2022-06-24 22:35: Train Epoch 25: 180/528 Loss: 20.228153
2022-06-24 22:36: Train Epoch 25: 200/528 Loss: 20.127701
2022-06-24 22:37: Train Epoch 25: 220/528 Loss: 21.829048
2022-06-24 22:37: Train Epoch 25: 240/528 Loss: 22.083364
2022-06-24 22:38: Train Epoch 25: 260/528 Loss: 21.802061
2022-06-24 22:39: Train Epoch 25: 280/528 Loss: 19.668953
2022-06-24 22:40: Train Epoch 25: 300/528 Loss: 20.117554
2022-06-24 22:41: Train Epoch 25: 320/528 Loss: 18.806419
2022-06-24 22:42: Train Epoch 25: 340/528 Loss: 20.887726
2022-06-24 22:42: Train Epoch 25: 360/528 Loss: 18.806564
2022-06-24 22:43: Train Epoch 25: 380/528 Loss: 19.203321
2022-06-24 22:44: Train Epoch 25: 400/528 Loss: 21.177662
2022-06-24 22:45: Train Epoch 25: 420/528 Loss: 20.613506
2022-06-24 22:46: Train Epoch 25: 440/528 Loss: 21.177109
2022-06-24 22:47: Train Epoch 25: 460/528 Loss: 22.301687
2022-06-24 22:47: Train Epoch 25: 480/528 Loss: 19.395485
2022-06-24 22:48: Train Epoch 25: 500/528 Loss: 19.414225
2022-06-24 22:49: Train Epoch 25: 520/528 Loss: 20.113396
2022-06-24 22:49: **********Train Epoch 25: averaged Loss: 20.803356
2022-06-24 22:52: **********Val Epoch 25: average Loss: 21.248213
2022-06-24 22:52: Train Epoch 26: 0/528 Loss: 18.771574
2022-06-24 22:53: Train Epoch 26: 20/528 Loss: 20.812593
2022-06-24 22:54: Train Epoch 26: 40/528 Loss: 20.444033
2022-06-24 22:54: Train Epoch 26: 60/528 Loss: 18.844851
2022-06-24 22:55: Train Epoch 26: 80/528 Loss: 20.931538
2022-06-24 22:56: Train Epoch 26: 100/528 Loss: 21.304882
2022-06-24 22:57: Train Epoch 26: 120/528 Loss: 22.422426
2022-06-24 22:58: Train Epoch 26: 140/528 Loss: 22.589970
2022-06-24 22:59: Train Epoch 26: 160/528 Loss: 22.684158
2022-06-24 23:00: Train Epoch 26: 180/528 Loss: 21.637510
2022-06-24 23:01: Train Epoch 26: 200/528 Loss: 19.463659
2022-06-24 23:01: Train Epoch 26: 220/528 Loss: 20.847059
2022-06-24 23:02: Train Epoch 26: 240/528 Loss: 20.334755
2022-06-24 23:03: Train Epoch 26: 260/528 Loss: 21.635527
2022-06-24 23:04: Train Epoch 26: 280/528 Loss: 21.639368
2022-06-24 23:05: Train Epoch 26: 300/528 Loss: 21.463936
2022-06-24 23:06: Train Epoch 26: 320/528 Loss: 23.406225
2022-06-24 23:06: Train Epoch 26: 340/528 Loss: 20.424427
2022-06-24 23:07: Train Epoch 26: 360/528 Loss: 20.276768
2022-06-24 23:08: Train Epoch 26: 380/528 Loss: 18.517328
2022-06-24 23:09: Train Epoch 26: 400/528 Loss: 18.439070
2022-06-24 23:09: Train Epoch 26: 420/528 Loss: 19.324499
2022-06-24 23:10: Train Epoch 26: 440/528 Loss: 19.737129
2022-06-24 23:11: Train Epoch 26: 460/528 Loss: 21.118542
2022-06-24 23:12: Train Epoch 26: 480/528 Loss: 22.422705
2022-06-24 23:12: Train Epoch 26: 500/528 Loss: 21.414911
2022-06-24 23:13: Train Epoch 26: 520/528 Loss: 21.378651
2022-06-24 23:14: **********Train Epoch 26: averaged Loss: 20.720785
2022-06-24 23:16: **********Val Epoch 26: average Loss: 21.020107
2022-06-24 23:16: *********************************Current best model saved!
2022-06-24 23:16: Train Epoch 27: 0/528 Loss: 21.724787
2022-06-24 23:17: Train Epoch 27: 20/528 Loss: 21.645506
2022-06-24 23:18: Train Epoch 27: 40/528 Loss: 21.836773
2022-06-24 23:18: Train Epoch 27: 60/528 Loss: 21.360006
2022-06-24 23:19: Train Epoch 27: 80/528 Loss: 18.775679
2022-06-24 23:20: Train Epoch 27: 100/528 Loss: 19.373688
2022-06-24 23:21: Train Epoch 27: 120/528 Loss: 21.000591
2022-06-24 23:22: Train Epoch 27: 140/528 Loss: 19.933969
2022-06-24 23:23: Train Epoch 27: 160/528 Loss: 21.370560
2022-06-24 23:23: Train Epoch 27: 180/528 Loss: 19.571003
2022-06-24 23:24: Train Epoch 27: 200/528 Loss: 21.438240
2022-06-24 23:25: Train Epoch 27: 220/528 Loss: 21.536697
2022-06-24 23:26: Train Epoch 27: 240/528 Loss: 20.191246
2022-06-24 23:27: Train Epoch 27: 260/528 Loss: 21.594984
2022-06-24 23:28: Train Epoch 27: 280/528 Loss: 20.578194
2022-06-24 23:29: Train Epoch 27: 300/528 Loss: 20.155310
2022-06-24 23:29: Train Epoch 27: 320/528 Loss: 20.465267
2022-06-24 23:30: Train Epoch 27: 340/528 Loss: 21.095779
2022-06-24 23:31: Train Epoch 27: 360/528 Loss: 20.470829
2022-06-24 23:32: Train Epoch 27: 380/528 Loss: 19.906708
2022-06-24 23:33: Train Epoch 27: 400/528 Loss: 19.647503
2022-06-24 23:34: Train Epoch 27: 420/528 Loss: 21.648632
2022-06-24 23:35: Train Epoch 27: 440/528 Loss: 17.444843
2022-06-24 23:35: Train Epoch 27: 460/528 Loss: 20.046350
2022-06-24 23:36: Train Epoch 27: 480/528 Loss: 21.626823
2022-06-24 23:37: Train Epoch 27: 500/528 Loss: 21.884062
2022-06-24 23:37: Train Epoch 27: 520/528 Loss: 19.621933
2022-06-24 23:38: **********Train Epoch 27: averaged Loss: 20.644745
2022-06-24 23:40: **********Val Epoch 27: average Loss: 21.079926
2022-06-24 23:40: Train Epoch 28: 0/528 Loss: 19.637791
2022-06-24 23:41: Train Epoch 28: 20/528 Loss: 21.903662
2022-06-24 23:42: Train Epoch 28: 40/528 Loss: 19.998749
2022-06-24 23:42: Train Epoch 28: 60/528 Loss: 19.995476
2022-06-24 23:43: Train Epoch 28: 80/528 Loss: 20.866507
2022-06-24 23:44: Train Epoch 28: 100/528 Loss: 21.660038
2022-06-24 23:45: Train Epoch 28: 120/528 Loss: 20.030298
2022-06-24 23:46: Train Epoch 28: 140/528 Loss: 20.020206
2022-06-24 23:47: Train Epoch 28: 160/528 Loss: 19.858850
2022-06-24 23:47: Train Epoch 28: 180/528 Loss: 19.748617
2022-06-24 23:48: Train Epoch 28: 200/528 Loss: 20.201092
2022-06-24 23:49: Train Epoch 28: 220/528 Loss: 22.443464
2022-06-24 23:50: Train Epoch 28: 240/528 Loss: 19.058887
2022-06-24 23:51: Train Epoch 28: 260/528 Loss: 20.367260
2022-06-24 23:51: Train Epoch 28: 280/528 Loss: 17.434256
2022-06-24 23:52: Train Epoch 28: 300/528 Loss: 20.760231
2022-06-24 23:53: Train Epoch 28: 320/528 Loss: 20.327953
2022-06-24 23:54: Train Epoch 28: 340/528 Loss: 19.783646
2022-06-24 23:55: Train Epoch 28: 360/528 Loss: 19.115368
2022-06-24 23:56: Train Epoch 28: 380/528 Loss: 20.204479
2022-06-24 23:57: Train Epoch 28: 400/528 Loss: 20.605091
2022-06-24 23:58: Train Epoch 28: 420/528 Loss: 21.142132
2022-06-24 23:58: Train Epoch 28: 440/528 Loss: 20.824373
2022-06-24 23:59: Train Epoch 28: 460/528 Loss: 20.024380
2022-06-25 00:00: Train Epoch 28: 480/528 Loss: 22.393847
2022-06-25 00:01: Train Epoch 28: 500/528 Loss: 21.307081
2022-06-25 00:02: Train Epoch 28: 520/528 Loss: 20.936525
2022-06-25 00:02: **********Train Epoch 28: averaged Loss: 20.630767
2022-06-25 00:04: **********Val Epoch 28: average Loss: 21.004351
2022-06-25 00:04: *********************************Current best model saved!
2022-06-25 00:04: Train Epoch 29: 0/528 Loss: 19.528837
2022-06-25 00:05: Train Epoch 29: 20/528 Loss: 21.692987
2022-06-25 00:06: Train Epoch 29: 40/528 Loss: 19.486141
2022-06-25 00:06: Train Epoch 29: 60/528 Loss: 19.971081
2022-06-25 00:07: Train Epoch 29: 80/528 Loss: 22.105255
2022-06-25 00:08: Train Epoch 29: 100/528 Loss: 19.788685
2022-06-25 00:09: Train Epoch 29: 120/528 Loss: 20.261703
2022-06-25 00:10: Train Epoch 29: 140/528 Loss: 20.642689
2022-06-25 00:10: Train Epoch 29: 160/528 Loss: 22.140976
2022-06-25 00:11: Train Epoch 29: 180/528 Loss: 20.838308
2022-06-25 00:12: Train Epoch 29: 200/528 Loss: 20.977209
2022-06-25 00:13: Train Epoch 29: 220/528 Loss: 20.523130
2022-06-25 00:14: Train Epoch 29: 240/528 Loss: 19.980179
2022-06-25 00:15: Train Epoch 29: 260/528 Loss: 20.167494
2022-06-25 00:15: Train Epoch 29: 280/528 Loss: 21.813616
2022-06-25 00:16: Train Epoch 29: 300/528 Loss: 19.687559
2022-06-25 00:17: Train Epoch 29: 320/528 Loss: 18.951414
2022-06-25 00:18: Train Epoch 29: 340/528 Loss: 20.859392
2022-06-25 00:19: Train Epoch 29: 360/528 Loss: 20.795723
2022-06-25 00:20: Train Epoch 29: 380/528 Loss: 21.428465
2022-06-25 00:21: Train Epoch 29: 400/528 Loss: 22.005932
2022-06-25 00:21: Train Epoch 29: 420/528 Loss: 22.251316
2022-06-25 00:22: Train Epoch 29: 440/528 Loss: 21.409760
2022-06-25 00:23: Train Epoch 29: 460/528 Loss: 20.054920
2022-06-25 00:24: Train Epoch 29: 480/528 Loss: 19.668413
2022-06-25 00:25: Train Epoch 29: 500/528 Loss: 20.426373
2022-06-25 00:25: Train Epoch 29: 520/528 Loss: 19.121279
2022-06-25 00:26: **********Train Epoch 29: averaged Loss: 20.586866
2022-06-25 00:28: **********Val Epoch 29: average Loss: 21.178076
2022-06-25 00:28: Train Epoch 30: 0/528 Loss: 18.977400
2022-06-25 00:29: Train Epoch 30: 20/528 Loss: 20.052999
2022-06-25 00:30: Train Epoch 30: 40/528 Loss: 19.968761
2022-06-25 00:30: Train Epoch 30: 60/528 Loss: 20.745476
2022-06-25 00:31: Train Epoch 30: 80/528 Loss: 20.033754
2022-06-25 00:32: Train Epoch 30: 100/528 Loss: 21.336678
2022-06-25 00:33: Train Epoch 30: 120/528 Loss: 20.207911
2022-06-25 00:34: Train Epoch 30: 140/528 Loss: 19.661839
2022-06-25 00:34: Train Epoch 30: 160/528 Loss: 20.091908
2022-06-25 00:35: Train Epoch 30: 180/528 Loss: 20.618925
2022-06-25 00:36: Train Epoch 30: 200/528 Loss: 21.271687
2022-06-25 00:37: Train Epoch 30: 220/528 Loss: 20.335989
2022-06-25 00:37: Train Epoch 30: 240/528 Loss: 22.247505
2022-06-25 00:38: Train Epoch 30: 260/528 Loss: 21.024334
2022-06-25 00:39: Train Epoch 30: 280/528 Loss: 19.962873
2022-06-25 00:40: Train Epoch 30: 300/528 Loss: 20.687992
2022-06-25 00:41: Train Epoch 30: 320/528 Loss: 20.032219
2022-06-25 00:42: Train Epoch 30: 340/528 Loss: 21.510458
2022-06-25 00:43: Train Epoch 30: 360/528 Loss: 20.201931
2022-06-25 00:43: Train Epoch 30: 380/528 Loss: 20.033018
2022-06-25 00:44: Train Epoch 30: 400/528 Loss: 20.262699
2022-06-25 00:45: Train Epoch 30: 420/528 Loss: 20.744633
2022-06-25 00:46: Train Epoch 30: 440/528 Loss: 20.598593
2022-06-25 00:47: Train Epoch 30: 460/528 Loss: 19.819092
2022-06-25 00:48: Train Epoch 30: 480/528 Loss: 20.216164
2022-06-25 00:48: Train Epoch 30: 500/528 Loss: 18.642517
2022-06-25 00:49: Train Epoch 30: 520/528 Loss: 17.567522
2022-06-25 00:50: **********Train Epoch 30: averaged Loss: 20.482203
2022-06-25 00:52: **********Val Epoch 30: average Loss: 21.036586
2022-06-25 00:52: Train Epoch 31: 0/528 Loss: 20.901499
2022-06-25 00:53: Train Epoch 31: 20/528 Loss: 19.918083
2022-06-25 00:53: Train Epoch 31: 40/528 Loss: 20.139948
2022-06-25 00:54: Train Epoch 31: 60/528 Loss: 21.293781
2022-06-25 00:55: Train Epoch 31: 80/528 Loss: 20.810938
2022-06-25 00:56: Train Epoch 31: 100/528 Loss: 21.584003
2022-06-25 00:56: Train Epoch 31: 120/528 Loss: 19.891325
2022-06-25 00:57: Train Epoch 31: 140/528 Loss: 21.071758
2022-06-25 00:58: Train Epoch 31: 160/528 Loss: 20.383135
2022-06-25 00:59: Train Epoch 31: 180/528 Loss: 18.423443
2022-06-25 01:00: Train Epoch 31: 200/528 Loss: 20.694635
2022-06-25 01:01: Train Epoch 31: 220/528 Loss: 19.441750
2022-06-25 01:01: Train Epoch 31: 240/528 Loss: 21.027706
2022-06-25 01:02: Train Epoch 31: 260/528 Loss: 21.517567
2022-06-25 01:03: Train Epoch 31: 280/528 Loss: 19.649771
2022-06-25 01:04: Train Epoch 31: 300/528 Loss: 20.609720
2022-06-25 01:05: Train Epoch 31: 320/528 Loss: 21.220032
2022-06-25 01:06: Train Epoch 31: 340/528 Loss: 20.263056
2022-06-25 01:06: Train Epoch 31: 360/528 Loss: 20.386658
2022-06-25 01:07: Train Epoch 31: 380/528 Loss: 21.411488
2022-06-25 01:08: Train Epoch 31: 400/528 Loss: 21.576250
2022-06-25 01:09: Train Epoch 31: 420/528 Loss: 21.951805
2022-06-25 01:10: Train Epoch 31: 440/528 Loss: 21.278111
2022-06-25 01:11: Train Epoch 31: 460/528 Loss: 21.234350
2022-06-25 01:11: Train Epoch 31: 480/528 Loss: 20.068211
2022-06-25 01:12: Train Epoch 31: 500/528 Loss: 19.962517
2022-06-25 01:13: Train Epoch 31: 520/528 Loss: 21.458858
2022-06-25 01:13: **********Train Epoch 31: averaged Loss: 20.478559
2022-06-25 01:16: **********Val Epoch 31: average Loss: 21.026629
2022-06-25 01:16: Train Epoch 32: 0/528 Loss: 21.213640
2022-06-25 01:16: Train Epoch 32: 20/528 Loss: 18.185009
2022-06-25 01:17: Train Epoch 32: 40/528 Loss: 19.750080
2022-06-25 01:18: Train Epoch 32: 60/528 Loss: 20.763178
2022-06-25 01:19: Train Epoch 32: 80/528 Loss: 20.545498
2022-06-25 01:20: Train Epoch 32: 100/528 Loss: 18.800379
2022-06-25 01:21: Train Epoch 32: 120/528 Loss: 20.885288
2022-06-25 01:21: Train Epoch 32: 140/528 Loss: 20.777788
2022-06-25 01:22: Train Epoch 32: 160/528 Loss: 19.023478
2022-06-25 01:23: Train Epoch 32: 180/528 Loss: 18.523811
2022-06-25 01:24: Train Epoch 32: 200/528 Loss: 19.192585
2022-06-25 01:25: Train Epoch 32: 220/528 Loss: 19.880999
2022-06-25 01:26: Train Epoch 32: 240/528 Loss: 20.090433
2022-06-25 01:27: Train Epoch 32: 260/528 Loss: 20.331217
2022-06-25 01:28: Train Epoch 32: 280/528 Loss: 19.746603
2022-06-25 01:29: Train Epoch 32: 300/528 Loss: 18.482548
2022-06-25 01:30: Train Epoch 32: 320/528 Loss: 19.701073
2022-06-25 01:31: Train Epoch 32: 340/528 Loss: 21.217300
2022-06-25 01:32: Train Epoch 32: 360/528 Loss: 20.043877
2022-06-25 01:33: Train Epoch 32: 380/528 Loss: 19.830105
2022-06-25 01:34: Train Epoch 32: 400/528 Loss: 19.478668
2022-06-25 01:35: Train Epoch 32: 420/528 Loss: 20.939890
2022-06-25 01:35: Train Epoch 32: 440/528 Loss: 20.533510
2022-06-25 01:36: Train Epoch 32: 460/528 Loss: 20.429506
2022-06-25 01:37: Train Epoch 32: 480/528 Loss: 20.706244
2022-06-25 01:38: Train Epoch 32: 500/528 Loss: 21.372686
2022-06-25 01:39: Train Epoch 32: 520/528 Loss: 20.595018
2022-06-25 01:39: **********Train Epoch 32: averaged Loss: 20.459792
2022-06-25 01:42: **********Val Epoch 32: average Loss: 20.917029
2022-06-25 01:42: *********************************Current best model saved!
2022-06-25 01:42: Train Epoch 33: 0/528 Loss: 20.091904
2022-06-25 01:43: Train Epoch 33: 20/528 Loss: 19.573610
2022-06-25 01:44: Train Epoch 33: 40/528 Loss: 19.877289
2022-06-25 01:45: Train Epoch 33: 60/528 Loss: 20.406691
2022-06-25 01:46: Train Epoch 33: 80/528 Loss: 21.590094
2022-06-25 01:47: Train Epoch 33: 100/528 Loss: 20.475990
2022-06-25 01:48: Train Epoch 33: 120/528 Loss: 20.766605
2022-06-25 01:49: Train Epoch 33: 140/528 Loss: 19.014513
2022-06-25 01:50: Train Epoch 33: 160/528 Loss: 21.657053
2022-06-25 01:51: Train Epoch 33: 180/528 Loss: 22.156118
2022-06-25 01:52: Train Epoch 33: 200/528 Loss: 20.341351
2022-06-25 01:53: Train Epoch 33: 220/528 Loss: 21.753435
2022-06-25 01:53: Train Epoch 33: 240/528 Loss: 19.255831
2022-06-25 01:54: Train Epoch 33: 260/528 Loss: 19.182604
2022-06-25 01:55: Train Epoch 33: 280/528 Loss: 21.247366
2022-06-25 01:56: Train Epoch 33: 300/528 Loss: 21.462454
2022-06-25 01:57: Train Epoch 33: 320/528 Loss: 19.754215
2022-06-25 01:58: Train Epoch 33: 340/528 Loss: 20.086666
2022-06-25 01:59: Train Epoch 33: 360/528 Loss: 21.645897
2022-06-25 02:00: Train Epoch 33: 380/528 Loss: 20.619055
2022-06-25 02:01: Train Epoch 33: 400/528 Loss: 21.421818
2022-06-25 02:02: Train Epoch 33: 420/528 Loss: 21.721666
2022-06-25 02:02: Train Epoch 33: 440/528 Loss: 21.649187
2022-06-25 02:03: Train Epoch 33: 460/528 Loss: 20.274509
2022-06-25 02:04: Train Epoch 33: 480/528 Loss: 19.418846
2022-06-25 02:05: Train Epoch 33: 500/528 Loss: 18.667923
2022-06-25 02:06: Train Epoch 33: 520/528 Loss: 21.069986
2022-06-25 02:07: **********Train Epoch 33: averaged Loss: 20.406047
2022-06-25 02:09: **********Val Epoch 33: average Loss: 20.827380
2022-06-25 02:09: *********************************Current best model saved!
2022-06-25 02:09: Train Epoch 34: 0/528 Loss: 19.580711
2022-06-25 02:10: Train Epoch 34: 20/528 Loss: 21.743151
2022-06-25 02:11: Train Epoch 34: 40/528 Loss: 19.328674
2022-06-25 02:12: Train Epoch 34: 60/528 Loss: 20.337536
2022-06-25 02:13: Train Epoch 34: 80/528 Loss: 20.495522
2022-06-25 02:14: Train Epoch 34: 100/528 Loss: 19.167284
2022-06-25 02:15: Train Epoch 34: 120/528 Loss: 20.486246
2022-06-25 02:16: Train Epoch 34: 140/528 Loss: 19.704170
2022-06-25 02:17: Train Epoch 34: 160/528 Loss: 19.945250
2022-06-25 02:18: Train Epoch 34: 180/528 Loss: 20.699621
2022-06-25 02:18: Train Epoch 34: 200/528 Loss: 20.601915
2022-06-25 02:19: Train Epoch 34: 220/528 Loss: 21.972538
2022-06-25 02:20: Train Epoch 34: 240/528 Loss: 21.443415
2022-06-25 02:21: Train Epoch 34: 260/528 Loss: 19.909132
2022-06-25 02:22: Train Epoch 34: 280/528 Loss: 21.766451
2022-06-25 02:23: Train Epoch 34: 300/528 Loss: 20.892677
2022-06-25 02:24: Train Epoch 34: 320/528 Loss: 20.947073
2022-06-25 02:25: Train Epoch 34: 340/528 Loss: 20.767525
2022-06-25 02:26: Train Epoch 34: 360/528 Loss: 21.259636
2022-06-25 02:27: Train Epoch 34: 380/528 Loss: 17.994698
2022-06-25 02:28: Train Epoch 34: 400/528 Loss: 20.284660
2022-06-25 02:29: Train Epoch 34: 420/528 Loss: 20.858120
2022-06-25 02:30: Train Epoch 34: 440/528 Loss: 19.380371
2022-06-25 02:31: Train Epoch 34: 460/528 Loss: 21.839882
2022-06-25 02:32: Train Epoch 34: 480/528 Loss: 20.668463
2022-06-25 02:33: Train Epoch 34: 500/528 Loss: 20.289595
2022-06-25 02:34: Train Epoch 34: 520/528 Loss: 20.782991
2022-06-25 02:34: **********Train Epoch 34: averaged Loss: 20.337075
2022-06-25 02:36: **********Val Epoch 34: average Loss: 21.694802
2022-06-25 02:36: Train Epoch 35: 0/528 Loss: 21.065756
2022-06-25 02:37: Train Epoch 35: 20/528 Loss: 21.388649
2022-06-25 02:38: Train Epoch 35: 40/528 Loss: 21.070898
2022-06-25 02:39: Train Epoch 35: 60/528 Loss: 19.604538
2022-06-25 02:40: Train Epoch 35: 80/528 Loss: 21.189457
2022-06-25 02:41: Train Epoch 35: 100/528 Loss: 19.556129
2022-06-25 02:42: Train Epoch 35: 120/528 Loss: 20.882292
2022-06-25 02:43: Train Epoch 35: 140/528 Loss: 20.135042
2022-06-25 02:44: Train Epoch 35: 160/528 Loss: 22.064293
2022-06-25 02:45: Train Epoch 35: 180/528 Loss: 21.118505
2022-06-25 02:46: Train Epoch 35: 200/528 Loss: 19.692753
2022-06-25 02:47: Train Epoch 35: 220/528 Loss: 19.977154
2022-06-25 02:48: Train Epoch 35: 240/528 Loss: 22.643339
2022-06-25 02:49: Train Epoch 35: 260/528 Loss: 18.037436
2022-06-25 02:50: Train Epoch 35: 280/528 Loss: 19.649775
2022-06-25 02:51: Train Epoch 35: 300/528 Loss: 19.916819
2022-06-25 02:52: Train Epoch 35: 320/528 Loss: 19.156755
2022-06-25 02:52: Train Epoch 35: 340/528 Loss: 19.705965
2022-06-25 02:53: Train Epoch 35: 360/528 Loss: 18.392963
2022-06-25 02:54: Train Epoch 35: 380/528 Loss: 18.958540
2022-06-25 02:55: Train Epoch 35: 400/528 Loss: 20.506813
2022-06-25 02:56: Train Epoch 35: 420/528 Loss: 20.599180
2022-06-25 02:57: Train Epoch 35: 440/528 Loss: 19.556990
2022-06-25 02:58: Train Epoch 35: 460/528 Loss: 22.400768
2022-06-25 02:59: Train Epoch 35: 480/528 Loss: 19.981047
2022-06-25 03:00: Train Epoch 35: 500/528 Loss: 20.593264
2022-06-25 03:01: Train Epoch 35: 520/528 Loss: 20.173996
2022-06-25 03:01: **********Train Epoch 35: averaged Loss: 20.273743
2022-06-25 03:04: **********Val Epoch 35: average Loss: 21.128026
2022-06-25 03:04: Train Epoch 36: 0/528 Loss: 19.044790
2022-06-25 03:05: Train Epoch 36: 20/528 Loss: 21.474234
2022-06-25 03:06: Train Epoch 36: 40/528 Loss: 19.964884
2022-06-25 03:07: Train Epoch 36: 60/528 Loss: 22.389355
2022-06-25 03:08: Train Epoch 36: 80/528 Loss: 18.563629
2022-06-25 03:09: Train Epoch 36: 100/528 Loss: 20.977226
2022-06-25 03:10: Train Epoch 36: 120/528 Loss: 20.783669
2022-06-25 03:11: Train Epoch 36: 140/528 Loss: 20.832582
2022-06-25 03:12: Train Epoch 36: 160/528 Loss: 21.584042
2022-06-25 03:13: Train Epoch 36: 180/528 Loss: 21.393625
2022-06-25 03:13: Train Epoch 36: 200/528 Loss: 20.684151
2022-06-25 03:14: Train Epoch 36: 220/528 Loss: 18.908354
2022-06-25 03:15: Train Epoch 36: 240/528 Loss: 18.978651
2022-06-25 03:16: Train Epoch 36: 260/528 Loss: 18.496916
2022-06-25 03:17: Train Epoch 36: 280/528 Loss: 19.106123
2022-06-25 03:18: Train Epoch 36: 300/528 Loss: 19.532347
2022-06-25 03:19: Train Epoch 36: 320/528 Loss: 17.816151
2022-06-25 03:20: Train Epoch 36: 340/528 Loss: 18.800776
2022-06-25 03:21: Train Epoch 36: 360/528 Loss: 20.129230
2022-06-25 03:22: Train Epoch 36: 380/528 Loss: 19.243866
2022-06-25 03:23: Train Epoch 36: 400/528 Loss: 21.041924
2022-06-25 03:24: Train Epoch 36: 420/528 Loss: 19.363689
2022-06-25 03:25: Train Epoch 36: 440/528 Loss: 22.787672
2022-06-25 03:26: Train Epoch 36: 460/528 Loss: 20.050364
2022-06-25 03:27: Train Epoch 36: 480/528 Loss: 19.427107
2022-06-25 03:28: Train Epoch 36: 500/528 Loss: 20.315958
2022-06-25 03:29: Train Epoch 36: 520/528 Loss: 20.501696
2022-06-25 03:29: **********Train Epoch 36: averaged Loss: 20.325020
2022-06-25 03:32: **********Val Epoch 36: average Loss: 22.351011
2022-06-25 03:32: Train Epoch 37: 0/528 Loss: 22.076654
2022-06-25 03:33: Train Epoch 37: 20/528 Loss: 20.410955
2022-06-25 03:34: Train Epoch 37: 40/528 Loss: 21.505079
2022-06-25 03:35: Train Epoch 37: 60/528 Loss: 18.771608
2022-06-25 03:36: Train Epoch 37: 80/528 Loss: 19.948359
2022-06-25 03:37: Train Epoch 37: 100/528 Loss: 19.898323
2022-06-25 03:38: Train Epoch 37: 120/528 Loss: 18.016836
2022-06-25 03:39: Train Epoch 37: 140/528 Loss: 19.546041
2022-06-25 03:40: Train Epoch 37: 160/528 Loss: 20.228554
2022-06-25 03:41: Train Epoch 37: 180/528 Loss: 22.699205
2022-06-25 03:42: Train Epoch 37: 200/528 Loss: 20.038990
2022-06-25 03:43: Train Epoch 37: 220/528 Loss: 18.611877
2022-06-25 03:44: Train Epoch 37: 240/528 Loss: 20.521620
2022-06-25 03:45: Train Epoch 37: 260/528 Loss: 20.130711
2022-06-25 03:46: Train Epoch 37: 280/528 Loss: 19.303871
2022-06-25 03:47: Train Epoch 37: 300/528 Loss: 20.119925
2022-06-25 03:48: Train Epoch 37: 320/528 Loss: 20.849247
2022-06-25 03:49: Train Epoch 37: 340/528 Loss: 18.685146
2022-06-25 03:50: Train Epoch 37: 360/528 Loss: 18.946215
2022-06-25 03:51: Train Epoch 37: 380/528 Loss: 19.473215
2022-06-25 03:52: Train Epoch 37: 400/528 Loss: 20.529694
2022-06-25 03:52: Train Epoch 37: 420/528 Loss: 21.388824
2022-06-25 03:53: Train Epoch 37: 440/528 Loss: 21.642792
2022-06-25 03:54: Train Epoch 37: 460/528 Loss: 19.439293
2022-06-25 03:55: Train Epoch 37: 480/528 Loss: 19.629873
2022-06-25 03:56: Train Epoch 37: 500/528 Loss: 22.664396
2022-06-25 03:57: Train Epoch 37: 520/528 Loss: 20.275568
2022-06-25 03:58: **********Train Epoch 37: averaged Loss: 20.232196
2022-06-25 04:00: **********Val Epoch 37: average Loss: 21.097402
2022-06-25 04:00: Train Epoch 38: 0/528 Loss: 21.516813
2022-06-25 04:01: Train Epoch 38: 20/528 Loss: 21.718929
2022-06-25 04:02: Train Epoch 38: 40/528 Loss: 20.644993
2022-06-25 04:03: Train Epoch 38: 60/528 Loss: 21.696827
2022-06-25 04:04: Train Epoch 38: 80/528 Loss: 18.611551
2022-06-25 04:06: Train Epoch 38: 100/528 Loss: 20.138083
2022-06-25 04:07: Train Epoch 38: 120/528 Loss: 20.653910
2022-06-25 04:08: Train Epoch 38: 140/528 Loss: 19.912806
2022-06-25 04:09: Train Epoch 38: 160/528 Loss: 20.106686
2022-06-25 04:09: Train Epoch 38: 180/528 Loss: 21.689024
2022-06-25 04:10: Train Epoch 38: 200/528 Loss: 20.131952
2022-06-25 04:11: Train Epoch 38: 220/528 Loss: 19.260002
2022-06-25 04:12: Train Epoch 38: 240/528 Loss: 20.507574
2022-06-25 04:13: Train Epoch 38: 260/528 Loss: 20.692789
2022-06-25 04:14: Train Epoch 38: 280/528 Loss: 20.398710
2022-06-25 04:15: Train Epoch 38: 300/528 Loss: 20.332140
2022-06-25 04:16: Train Epoch 38: 320/528 Loss: 20.281530
2022-06-25 04:17: Train Epoch 38: 340/528 Loss: 21.005283
2022-06-25 04:18: Train Epoch 38: 360/528 Loss: 20.880070
2022-06-25 04:19: Train Epoch 38: 380/528 Loss: 20.351599
2022-06-25 04:20: Train Epoch 38: 400/528 Loss: 19.470896
2022-06-25 04:21: Train Epoch 38: 420/528 Loss: 20.865999
2022-06-25 04:22: Train Epoch 38: 440/528 Loss: 21.953108
2022-06-25 04:23: Train Epoch 38: 460/528 Loss: 20.084135
2022-06-25 04:24: Train Epoch 38: 480/528 Loss: 19.315271
2022-06-25 04:25: Train Epoch 38: 500/528 Loss: 20.540197
2022-06-25 04:26: Train Epoch 38: 520/528 Loss: 22.030758
2022-06-25 04:26: **********Train Epoch 38: averaged Loss: 20.260191
2022-06-25 04:29: **********Val Epoch 38: average Loss: 20.898037
2022-06-25 04:29: Train Epoch 39: 0/528 Loss: 20.951397
2022-06-25 04:30: Train Epoch 39: 20/528 Loss: 21.397589
2022-06-25 04:31: Train Epoch 39: 40/528 Loss: 19.356285
2022-06-25 04:32: Train Epoch 39: 60/528 Loss: 18.509567
2022-06-25 04:33: Train Epoch 39: 80/528 Loss: 19.993790
2022-06-25 04:34: Train Epoch 39: 100/528 Loss: 19.174644
2022-06-25 04:34: Train Epoch 39: 120/528 Loss: 20.680883
2022-06-25 04:35: Train Epoch 39: 140/528 Loss: 22.117311
2022-06-25 04:36: Train Epoch 39: 160/528 Loss: 20.144583
2022-06-25 04:37: Train Epoch 39: 180/528 Loss: 19.201214
2022-06-25 04:38: Train Epoch 39: 200/528 Loss: 20.598036
2022-06-25 04:39: Train Epoch 39: 220/528 Loss: 19.801119
2022-06-25 04:40: Train Epoch 39: 240/528 Loss: 19.373608
2022-06-25 04:41: Train Epoch 39: 260/528 Loss: 18.188660
2022-06-25 04:42: Train Epoch 39: 280/528 Loss: 18.549025
2022-06-25 04:44: Train Epoch 39: 300/528 Loss: 19.132265
2022-06-25 04:45: Train Epoch 39: 320/528 Loss: 19.077101
2022-06-25 04:46: Train Epoch 39: 340/528 Loss: 21.051006
2022-06-25 04:46: Train Epoch 39: 360/528 Loss: 20.477179
2022-06-25 04:47: Train Epoch 39: 380/528 Loss: 18.579132
2022-06-25 04:48: Train Epoch 39: 400/528 Loss: 20.406691
2022-06-25 04:49: Train Epoch 39: 420/528 Loss: 19.261679
2022-06-25 04:50: Train Epoch 39: 440/528 Loss: 21.170263
2022-06-25 04:51: Train Epoch 39: 460/528 Loss: 21.115063
2022-06-25 04:52: Train Epoch 39: 480/528 Loss: 19.335613
2022-06-25 04:53: Train Epoch 39: 500/528 Loss: 22.283342
2022-06-25 04:54: Train Epoch 39: 520/528 Loss: 20.311413
2022-06-25 04:54: **********Train Epoch 39: averaged Loss: 20.121855
2022-06-25 04:57: **********Val Epoch 39: average Loss: 20.540588
2022-06-25 04:57: *********************************Current best model saved!
2022-06-25 04:57: Train Epoch 40: 0/528 Loss: 20.616619
2022-06-25 04:58: Train Epoch 40: 20/528 Loss: 19.929302
2022-06-25 04:59: Train Epoch 40: 40/528 Loss: 20.884394
2022-06-25 05:00: Train Epoch 40: 60/528 Loss: 20.070658
2022-06-25 05:01: Train Epoch 40: 80/528 Loss: 17.878691
2022-06-25 05:02: Train Epoch 40: 100/528 Loss: 18.306141
2022-06-25 05:03: Train Epoch 40: 120/528 Loss: 20.553661
2022-06-25 05:04: Train Epoch 40: 140/528 Loss: 21.701008
2022-06-25 05:05: Train Epoch 40: 160/528 Loss: 19.983757
2022-06-25 05:06: Train Epoch 40: 180/528 Loss: 20.842665
2022-06-25 05:07: Train Epoch 40: 200/528 Loss: 20.007206
2022-06-25 05:08: Train Epoch 40: 220/528 Loss: 21.293533
2022-06-25 05:09: Train Epoch 40: 240/528 Loss: 21.058474
2022-06-25 05:10: Train Epoch 40: 260/528 Loss: 20.868277
2022-06-25 05:11: Train Epoch 40: 280/528 Loss: 20.307240
2022-06-25 05:12: Train Epoch 40: 300/528 Loss: 20.946136
2022-06-25 05:13: Train Epoch 40: 320/528 Loss: 20.595917
2022-06-25 05:14: Train Epoch 40: 340/528 Loss: 19.837648
2022-06-25 05:14: Train Epoch 40: 360/528 Loss: 21.046854
2022-06-25 05:16: Train Epoch 40: 380/528 Loss: 19.690514
2022-06-25 05:16: Train Epoch 40: 400/528 Loss: 19.794580
2022-06-25 05:17: Train Epoch 40: 420/528 Loss: 19.987261
2022-06-25 05:18: Train Epoch 40: 440/528 Loss: 19.788361
2022-06-25 05:19: Train Epoch 40: 460/528 Loss: 19.290703
2022-06-25 05:20: Train Epoch 40: 480/528 Loss: 19.405058
2022-06-25 05:22: Train Epoch 40: 500/528 Loss: 17.986280
2022-06-25 05:23: Train Epoch 40: 520/528 Loss: 18.556845
2022-06-25 05:23: **********Train Epoch 40: averaged Loss: 20.219401
2022-06-25 05:26: **********Val Epoch 40: average Loss: 20.690156
2022-06-25 05:26: Train Epoch 41: 0/528 Loss: 20.214705
2022-06-25 05:27: Train Epoch 41: 20/528 Loss: 19.827497
2022-06-25 05:28: Train Epoch 41: 40/528 Loss: 19.157814
2022-06-25 05:28: Train Epoch 41: 60/528 Loss: 19.285089
2022-06-25 05:29: Train Epoch 41: 80/528 Loss: 20.324059
2022-06-25 05:30: Train Epoch 41: 100/528 Loss: 20.727283
2022-06-25 05:31: Train Epoch 41: 120/528 Loss: 21.078987
2022-06-25 05:32: Train Epoch 41: 140/528 Loss: 21.099789
2022-06-25 05:33: Train Epoch 41: 160/528 Loss: 20.395277
2022-06-25 05:34: Train Epoch 41: 180/528 Loss: 19.262661
2022-06-25 05:35: Train Epoch 41: 200/528 Loss: 19.536072
2022-06-25 05:36: Train Epoch 41: 220/528 Loss: 19.248640
2022-06-25 05:37: Train Epoch 41: 240/528 Loss: 20.745638
2022-06-25 05:38: Train Epoch 41: 260/528 Loss: 18.134211
2022-06-25 05:39: Train Epoch 41: 280/528 Loss: 20.129822
2022-06-25 05:40: Train Epoch 41: 300/528 Loss: 20.915121
2022-06-25 05:41: Train Epoch 41: 320/528 Loss: 19.761589
2022-06-25 05:42: Train Epoch 41: 340/528 Loss: 21.367676
2022-06-25 05:43: Train Epoch 41: 360/528 Loss: 20.793953
2022-06-25 05:44: Train Epoch 41: 380/528 Loss: 18.410767
2022-06-25 05:45: Train Epoch 41: 400/528 Loss: 19.396309
2022-06-25 05:46: Train Epoch 41: 420/528 Loss: 20.251799
2022-06-25 05:47: Train Epoch 41: 440/528 Loss: 19.905525
2022-06-25 05:48: Train Epoch 41: 460/528 Loss: 19.901796
2022-06-25 05:49: Train Epoch 41: 480/528 Loss: 21.192181
2022-06-25 05:50: Train Epoch 41: 500/528 Loss: 20.226990
2022-06-25 05:51: Train Epoch 41: 520/528 Loss: 21.132071
2022-06-25 05:51: **********Train Epoch 41: averaged Loss: 20.051668
2022-06-25 05:54: **********Val Epoch 41: average Loss: 21.095896
2022-06-25 05:54: Train Epoch 42: 0/528 Loss: 20.031040
2022-06-25 05:55: Train Epoch 42: 20/528 Loss: 18.359989
2022-06-25 05:56: Train Epoch 42: 40/528 Loss: 20.837418
2022-06-25 05:57: Train Epoch 42: 60/528 Loss: 17.987267
2022-06-25 05:58: Train Epoch 42: 80/528 Loss: 18.991432
2022-06-25 05:59: Train Epoch 42: 100/528 Loss: 21.133350
2022-06-25 06:00: Train Epoch 42: 120/528 Loss: 21.281509
2022-06-25 06:01: Train Epoch 42: 140/528 Loss: 19.650446
2022-06-25 06:02: Train Epoch 42: 160/528 Loss: 21.054407
2022-06-25 06:03: Train Epoch 42: 180/528 Loss: 19.305714
2022-06-25 06:04: Train Epoch 42: 200/528 Loss: 20.849619
2022-06-25 06:05: Train Epoch 42: 220/528 Loss: 20.501057
2022-06-25 06:06: Train Epoch 42: 240/528 Loss: 20.167046
2022-06-25 06:07: Train Epoch 42: 260/528 Loss: 20.692516
2022-06-25 06:07: Train Epoch 42: 280/528 Loss: 20.198221
2022-06-25 06:08: Train Epoch 42: 300/528 Loss: 17.679998
2022-06-25 06:09: Train Epoch 42: 320/528 Loss: 19.034037
2022-06-25 06:10: Train Epoch 42: 340/528 Loss: 20.812775
2022-06-25 06:11: Train Epoch 42: 360/528 Loss: 19.826639
2022-06-25 06:12: Train Epoch 42: 380/528 Loss: 19.525089
2022-06-25 06:13: Train Epoch 42: 400/528 Loss: 20.324242
2022-06-25 06:14: Train Epoch 42: 420/528 Loss: 21.654457
2022-06-25 06:15: Train Epoch 42: 440/528 Loss: 21.216875
2022-06-25 06:16: Train Epoch 42: 460/528 Loss: 21.000786
2022-06-25 06:17: Train Epoch 42: 480/528 Loss: 19.755991
2022-06-25 06:18: Train Epoch 42: 500/528 Loss: 20.688881
2022-06-25 06:19: Train Epoch 42: 520/528 Loss: 21.293699
2022-06-25 06:20: **********Train Epoch 42: averaged Loss: 20.051206
2022-06-25 06:23: **********Val Epoch 42: average Loss: 20.670716
2022-06-25 06:23: Train Epoch 43: 0/528 Loss: 20.416336
2022-06-25 06:23: Train Epoch 43: 20/528 Loss: 21.373333
2022-06-25 06:24: Train Epoch 43: 40/528 Loss: 19.416216
2022-06-25 06:25: Train Epoch 43: 60/528 Loss: 20.129135
2022-06-25 06:26: Train Epoch 43: 80/528 Loss: 19.913862
2022-06-25 06:27: Train Epoch 43: 100/528 Loss: 21.453949
2022-06-25 06:28: Train Epoch 43: 120/528 Loss: 19.251007
2022-06-25 06:29: Train Epoch 43: 140/528 Loss: 20.846737
2022-06-25 06:30: Train Epoch 43: 160/528 Loss: 17.160213
2022-06-25 06:31: Train Epoch 43: 180/528 Loss: 19.420061
2022-06-25 06:32: Train Epoch 43: 200/528 Loss: 19.522917
2022-06-25 06:33: Train Epoch 43: 220/528 Loss: 20.084726
2022-06-25 06:34: Train Epoch 43: 240/528 Loss: 19.057726
2022-06-25 06:35: Train Epoch 43: 260/528 Loss: 20.450089
2022-06-25 06:36: Train Epoch 43: 280/528 Loss: 20.835339
2022-06-25 06:37: Train Epoch 43: 300/528 Loss: 19.408575
2022-06-25 06:38: Train Epoch 43: 320/528 Loss: 18.578140
2022-06-25 06:39: Train Epoch 43: 340/528 Loss: 19.396933
2022-06-25 06:40: Train Epoch 43: 360/528 Loss: 20.675137
2022-06-25 06:41: Train Epoch 43: 380/528 Loss: 19.932226
2022-06-25 06:42: Train Epoch 43: 400/528 Loss: 19.151760
2022-06-25 06:42: Train Epoch 43: 420/528 Loss: 20.007765
2022-06-25 06:43: Train Epoch 43: 440/528 Loss: 19.943251
2022-06-25 06:43: Train Epoch 43: 460/528 Loss: 20.211014
2022-06-25 06:44: Train Epoch 43: 480/528 Loss: 20.281021
2022-06-25 06:45: Train Epoch 43: 500/528 Loss: 20.349855
2022-06-25 06:45: Train Epoch 43: 520/528 Loss: 20.121756
2022-06-25 06:45: **********Train Epoch 43: averaged Loss: 20.074073
2022-06-25 06:47: **********Val Epoch 43: average Loss: 20.832443
2022-06-25 06:47: Train Epoch 44: 0/528 Loss: 21.070528
2022-06-25 06:47: Train Epoch 44: 20/528 Loss: 20.782402
2022-06-25 06:48: Train Epoch 44: 40/528 Loss: 17.767799
2022-06-25 06:49: Train Epoch 44: 60/528 Loss: 20.519690
2022-06-25 06:49: Train Epoch 44: 80/528 Loss: 19.964602
2022-06-25 06:50: Train Epoch 44: 100/528 Loss: 21.570461
2022-06-25 06:50: Train Epoch 44: 120/528 Loss: 19.963564
2022-06-25 06:51: Train Epoch 44: 140/528 Loss: 20.171892
2022-06-25 06:52: Train Epoch 44: 160/528 Loss: 18.525089
2022-06-25 06:52: Train Epoch 44: 180/528 Loss: 20.388430
2022-06-25 06:53: Train Epoch 44: 200/528 Loss: 18.180006
2022-06-25 06:53: Train Epoch 44: 220/528 Loss: 20.471401
2022-06-25 06:54: Train Epoch 44: 240/528 Loss: 19.257597
2022-06-25 06:55: Train Epoch 44: 260/528 Loss: 18.479277
2022-06-25 06:55: Train Epoch 44: 280/528 Loss: 19.819799
2022-06-25 06:56: Train Epoch 44: 300/528 Loss: 20.947464
2022-06-25 06:57: Train Epoch 44: 320/528 Loss: 20.138771
2022-06-25 06:57: Train Epoch 44: 340/528 Loss: 20.651299
2022-06-25 06:58: Train Epoch 44: 360/528 Loss: 19.337074
2022-06-25 06:58: Train Epoch 44: 380/528 Loss: 20.507099
2022-06-25 06:59: Train Epoch 44: 400/528 Loss: 19.727413
2022-06-25 07:00: Train Epoch 44: 420/528 Loss: 19.750013
2022-06-25 07:00: Train Epoch 44: 440/528 Loss: 19.572124
2022-06-25 07:01: Train Epoch 44: 460/528 Loss: 18.575836
2022-06-25 07:02: Train Epoch 44: 480/528 Loss: 19.506916
2022-06-25 07:03: Train Epoch 44: 500/528 Loss: 20.496332
2022-06-25 07:04: Train Epoch 44: 520/528 Loss: 20.939291
2022-06-25 07:04: **********Train Epoch 44: averaged Loss: 20.024089
2022-06-25 07:07: **********Val Epoch 44: average Loss: 20.828817
2022-06-25 07:07: Train Epoch 45: 0/528 Loss: 20.817305
2022-06-25 07:08: Train Epoch 45: 20/528 Loss: 21.336782
2022-06-25 07:09: Train Epoch 45: 40/528 Loss: 19.522150
2022-06-25 07:10: Train Epoch 45: 60/528 Loss: 19.696220
2022-06-25 07:11: Train Epoch 45: 80/528 Loss: 19.658588
2022-06-25 07:12: Train Epoch 45: 100/528 Loss: 23.178745
2022-06-25 07:13: Train Epoch 45: 120/528 Loss: 20.016329
2022-06-25 07:14: Train Epoch 45: 140/528 Loss: 20.172382
2022-06-25 07:15: Train Epoch 45: 160/528 Loss: 20.938412
2022-06-25 07:16: Train Epoch 45: 180/528 Loss: 21.862614
2022-06-25 07:17: Train Epoch 45: 200/528 Loss: 20.477900
2022-06-25 07:18: Train Epoch 45: 220/528 Loss: 21.693975
2022-06-25 07:19: Train Epoch 45: 240/528 Loss: 18.361618
2022-06-25 07:20: Train Epoch 45: 260/528 Loss: 18.375427
2022-06-25 07:21: Train Epoch 45: 280/528 Loss: 20.568998
2022-06-25 07:22: Train Epoch 45: 300/528 Loss: 20.472473
2022-06-25 07:23: Train Epoch 45: 320/528 Loss: 20.387360
2022-06-25 07:24: Train Epoch 45: 340/528 Loss: 22.390499
2022-06-25 07:24: Train Epoch 45: 360/528 Loss: 20.914516
2022-06-25 07:25: Train Epoch 45: 380/528 Loss: 22.055002
2022-06-25 07:26: Train Epoch 45: 400/528 Loss: 20.366032
2022-06-25 07:27: Train Epoch 45: 420/528 Loss: 18.567041
2022-06-25 07:28: Train Epoch 45: 440/528 Loss: 18.374367
2022-06-25 07:29: Train Epoch 45: 460/528 Loss: 19.681019
2022-06-25 07:30: Train Epoch 45: 480/528 Loss: 19.067272
2022-06-25 07:31: Train Epoch 45: 500/528 Loss: 20.535517
2022-06-25 07:32: Train Epoch 45: 520/528 Loss: 19.339405
2022-06-25 07:33: **********Train Epoch 45: averaged Loss: 20.203758
2022-06-25 07:36: **********Val Epoch 45: average Loss: 20.776314
2022-06-25 07:36: Train Epoch 46: 0/528 Loss: 19.918873
2022-06-25 07:37: Train Epoch 46: 20/528 Loss: 20.400299
2022-06-25 07:38: Train Epoch 46: 40/528 Loss: 21.710892
2022-06-25 07:38: Train Epoch 46: 60/528 Loss: 18.647705
2022-06-25 07:39: Train Epoch 46: 80/528 Loss: 18.598354
2022-06-25 07:40: Train Epoch 46: 100/528 Loss: 20.442162
2022-06-25 07:41: Train Epoch 46: 120/528 Loss: 19.938314
2022-06-25 07:42: Train Epoch 46: 140/528 Loss: 21.181267
2022-06-25 07:43: Train Epoch 46: 160/528 Loss: 18.700333
2022-06-25 07:44: Train Epoch 46: 180/528 Loss: 19.875156
2022-06-25 07:45: Train Epoch 46: 200/528 Loss: 21.150820
2022-06-25 07:46: Train Epoch 46: 220/528 Loss: 19.185404
2022-06-25 07:47: Train Epoch 46: 240/528 Loss: 21.447336
2022-06-25 07:48: Train Epoch 46: 260/528 Loss: 20.013437
2022-06-25 07:49: Train Epoch 46: 280/528 Loss: 22.368401
2022-06-25 07:50: Train Epoch 46: 300/528 Loss: 21.110628
2022-06-25 07:51: Train Epoch 46: 320/528 Loss: 17.810154
2022-06-25 07:52: Train Epoch 46: 340/528 Loss: 20.277727
2022-06-25 07:53: Train Epoch 46: 360/528 Loss: 20.299486
2022-06-25 07:54: Train Epoch 46: 380/528 Loss: 18.961948
2022-06-25 07:55: Train Epoch 46: 400/528 Loss: 20.118929
2022-06-25 07:56: Train Epoch 46: 420/528 Loss: 21.025679
2022-06-25 07:57: Train Epoch 46: 440/528 Loss: 20.065264
2022-06-25 07:58: Train Epoch 46: 460/528 Loss: 19.993229
2022-06-25 07:59: Train Epoch 46: 480/528 Loss: 19.460409
2022-06-25 08:00: Train Epoch 46: 500/528 Loss: 21.122597
2022-06-25 08:00: Train Epoch 46: 520/528 Loss: 19.466061
2022-06-25 08:01: **********Train Epoch 46: averaged Loss: 20.030775
2022-06-25 08:04: **********Val Epoch 46: average Loss: 20.669436
2022-06-25 08:04: Train Epoch 47: 0/528 Loss: 20.469824
2022-06-25 08:05: Train Epoch 47: 20/528 Loss: 18.742533
2022-06-25 08:05: Train Epoch 47: 40/528 Loss: 20.953583
2022-06-25 08:06: Train Epoch 47: 60/528 Loss: 20.599564
2022-06-25 08:07: Train Epoch 47: 80/528 Loss: 19.279530
2022-06-25 08:09: Train Epoch 47: 100/528 Loss: 18.012791
2022-06-25 08:10: Train Epoch 47: 120/528 Loss: 19.398340
2022-06-25 08:11: Train Epoch 47: 140/528 Loss: 19.271570
2022-06-25 08:12: Train Epoch 47: 160/528 Loss: 19.204437
2022-06-25 08:13: Train Epoch 47: 180/528 Loss: 20.639273
2022-06-25 08:13: Train Epoch 47: 200/528 Loss: 19.370106
2022-06-25 08:14: Train Epoch 47: 220/528 Loss: 18.714056
2022-06-25 08:15: Train Epoch 47: 240/528 Loss: 19.930958
2022-06-25 08:16: Train Epoch 47: 260/528 Loss: 20.642038
2022-06-25 08:17: Train Epoch 47: 280/528 Loss: 21.180695
2022-06-25 08:18: Train Epoch 47: 300/528 Loss: 19.954906
2022-06-25 08:19: Train Epoch 47: 320/528 Loss: 20.215792
2022-06-25 08:20: Train Epoch 47: 340/528 Loss: 20.263929
2022-06-25 08:21: Train Epoch 47: 360/528 Loss: 19.547764
2022-06-25 08:22: Train Epoch 47: 380/528 Loss: 20.875452
2022-06-25 08:23: Train Epoch 47: 400/528 Loss: 19.710377
2022-06-25 08:24: Train Epoch 47: 420/528 Loss: 21.477823
2022-06-25 08:25: Train Epoch 47: 440/528 Loss: 19.873949
2022-06-25 08:26: Train Epoch 47: 460/528 Loss: 20.201591
2022-06-25 08:27: Train Epoch 47: 480/528 Loss: 19.158991
2022-06-25 08:28: Train Epoch 47: 500/528 Loss: 20.583839
2022-06-25 08:29: Train Epoch 47: 520/528 Loss: 20.842583
2022-06-25 08:29: **********Train Epoch 47: averaged Loss: 19.888854
2022-06-25 08:32: **********Val Epoch 47: average Loss: 20.689515
2022-06-25 08:32: Train Epoch 48: 0/528 Loss: 18.033903
2022-06-25 08:33: Train Epoch 48: 20/528 Loss: 21.444624
2022-06-25 08:34: Train Epoch 48: 40/528 Loss: 20.126696
2022-06-25 08:35: Train Epoch 48: 60/528 Loss: 20.855232
2022-06-25 08:36: Train Epoch 48: 80/528 Loss: 20.430653
2022-06-25 08:37: Train Epoch 48: 100/528 Loss: 19.579681
2022-06-25 08:37: Train Epoch 48: 120/528 Loss: 20.756258
2022-06-25 08:38: Train Epoch 48: 140/528 Loss: 18.983685
2022-06-25 08:39: Train Epoch 48: 160/528 Loss: 21.378265
2022-06-25 08:40: Train Epoch 48: 180/528 Loss: 19.220694
2022-06-25 08:41: Train Epoch 48: 200/528 Loss: 18.990517
2022-06-25 08:42: Train Epoch 48: 220/528 Loss: 19.919907
2022-06-25 08:44: Train Epoch 48: 240/528 Loss: 18.564188
2022-06-25 08:45: Train Epoch 48: 260/528 Loss: 21.289726
2022-06-25 08:45: Train Epoch 48: 280/528 Loss: 21.186462
2022-06-25 08:47: Train Epoch 48: 300/528 Loss: 20.452812
2022-06-25 08:48: Train Epoch 48: 320/528 Loss: 20.163420
2022-06-25 08:49: Train Epoch 48: 340/528 Loss: 18.607815
2022-06-25 08:49: Train Epoch 48: 360/528 Loss: 22.109966
2022-06-25 08:50: Train Epoch 48: 380/528 Loss: 20.407614
2022-06-25 08:51: Train Epoch 48: 400/528 Loss: 18.769693
2022-06-25 08:52: Train Epoch 48: 420/528 Loss: 19.171728
2022-06-25 08:53: Train Epoch 48: 440/528 Loss: 19.395296
2022-06-25 08:54: Train Epoch 48: 460/528 Loss: 21.635035
2022-06-25 08:55: Train Epoch 48: 480/528 Loss: 19.644560
2022-06-25 08:56: Train Epoch 48: 500/528 Loss: 19.239964
2022-06-25 08:57: Train Epoch 48: 520/528 Loss: 20.138470
2022-06-25 08:57: **********Train Epoch 48: averaged Loss: 20.019185
2022-06-25 09:00: **********Val Epoch 48: average Loss: 20.910506
2022-06-25 09:00: Train Epoch 49: 0/528 Loss: 19.299702
2022-06-25 09:01: Train Epoch 49: 20/528 Loss: 19.757734
2022-06-25 09:02: Train Epoch 49: 40/528 Loss: 21.467579
2022-06-25 09:03: Train Epoch 49: 60/528 Loss: 20.229105
2022-06-25 09:04: Train Epoch 49: 80/528 Loss: 22.017933
2022-06-25 09:05: Train Epoch 49: 100/528 Loss: 19.490789
2022-06-25 09:06: Train Epoch 49: 120/528 Loss: 19.455456
2022-06-25 09:07: Train Epoch 49: 140/528 Loss: 19.284147
2022-06-25 09:08: Train Epoch 49: 160/528 Loss: 19.795643
2022-06-25 09:09: Train Epoch 49: 180/528 Loss: 20.519966
2022-06-25 09:10: Train Epoch 49: 200/528 Loss: 21.435488
2022-06-25 09:11: Train Epoch 49: 220/528 Loss: 19.785524
2022-06-25 09:12: Train Epoch 49: 240/528 Loss: 20.397884
2022-06-25 09:13: Train Epoch 49: 260/528 Loss: 20.328831
2022-06-25 09:14: Train Epoch 49: 280/528 Loss: 20.475126
2022-06-25 09:15: Train Epoch 49: 300/528 Loss: 21.332645
2022-06-25 09:17: Train Epoch 49: 320/528 Loss: 19.323072
2022-06-25 09:18: Train Epoch 49: 340/528 Loss: 20.065226
2022-06-25 09:19: Train Epoch 49: 360/528 Loss: 20.070705
2022-06-25 09:20: Train Epoch 49: 380/528 Loss: 19.964510
2022-06-25 09:22: Train Epoch 49: 400/528 Loss: 18.585436
2022-06-25 09:23: Train Epoch 49: 420/528 Loss: 19.324568
2022-06-25 09:24: Train Epoch 49: 440/528 Loss: 20.533884
2022-06-25 09:25: Train Epoch 49: 460/528 Loss: 20.234785
2022-06-25 09:26: Train Epoch 49: 480/528 Loss: 20.475384
2022-06-25 09:27: Train Epoch 49: 500/528 Loss: 20.292669
2022-06-25 09:28: Train Epoch 49: 520/528 Loss: 19.414042
2022-06-25 09:29: **********Train Epoch 49: averaged Loss: 19.882645
2022-06-25 09:32: **********Val Epoch 49: average Loss: 20.719574
2022-06-25 09:33: Train Epoch 50: 0/528 Loss: 20.150839
2022-06-25 09:34: Train Epoch 50: 20/528 Loss: 19.577869
2022-06-25 09:35: Train Epoch 50: 40/528 Loss: 19.567982
2022-06-25 09:36: Train Epoch 50: 60/528 Loss: 20.552086
2022-06-25 09:38: Train Epoch 50: 80/528 Loss: 20.063723
2022-06-25 09:39: Train Epoch 50: 100/528 Loss: 18.173145
2022-06-25 09:40: Train Epoch 50: 120/528 Loss: 20.729492
2022-06-25 09:41: Train Epoch 50: 140/528 Loss: 19.917416
2022-06-25 09:42: Train Epoch 50: 160/528 Loss: 22.255167
2022-06-25 09:43: Train Epoch 50: 180/528 Loss: 18.821957
2022-06-25 09:44: Train Epoch 50: 200/528 Loss: 20.374752
2022-06-25 09:46: Train Epoch 50: 220/528 Loss: 21.279131
2022-06-25 09:47: Train Epoch 50: 240/528 Loss: 19.664608
2022-06-25 09:48: Train Epoch 50: 260/528 Loss: 20.976051
2022-06-25 09:49: Train Epoch 50: 280/528 Loss: 19.966908
2022-06-25 09:50: Train Epoch 50: 300/528 Loss: 19.025906
2022-06-25 09:52: Train Epoch 50: 320/528 Loss: 19.751074
2022-06-25 09:53: Train Epoch 50: 340/528 Loss: 20.759840
2022-06-25 09:54: Train Epoch 50: 360/528 Loss: 19.338446
2022-06-25 09:55: Train Epoch 50: 380/528 Loss: 20.912130
2022-06-25 09:56: Train Epoch 50: 400/528 Loss: 18.992527
2022-06-25 09:57: Train Epoch 50: 420/528 Loss: 19.416945
2022-06-25 09:58: Train Epoch 50: 440/528 Loss: 19.446232
2022-06-25 10:00: Train Epoch 50: 460/528 Loss: 17.636620
2022-06-25 10:01: Train Epoch 50: 480/528 Loss: 20.733818
2022-06-25 10:02: Train Epoch 50: 500/528 Loss: 21.601206
2022-06-25 10:03: Train Epoch 50: 520/528 Loss: 20.286253
2022-06-25 10:03: **********Train Epoch 50: averaged Loss: 19.827957
2022-06-25 10:07: **********Val Epoch 50: average Loss: 20.553526
2022-06-25 10:07: Train Epoch 51: 0/528 Loss: 19.977289
2022-06-25 10:09: Train Epoch 51: 20/528 Loss: 19.548273
2022-06-25 10:10: Train Epoch 51: 40/528 Loss: 19.651342
2022-06-25 10:11: Train Epoch 51: 60/528 Loss: 20.638163
2022-06-25 10:12: Train Epoch 51: 80/528 Loss: 20.853851
2022-06-25 10:13: Train Epoch 51: 100/528 Loss: 19.948435
2022-06-25 10:14: Train Epoch 51: 120/528 Loss: 21.826866
2022-06-25 10:16: Train Epoch 51: 140/528 Loss: 19.114693
2022-06-25 10:17: Train Epoch 51: 160/528 Loss: 19.957804
2022-06-25 10:18: Train Epoch 51: 180/528 Loss: 19.438337
2022-06-25 10:19: Train Epoch 51: 200/528 Loss: 20.198286
2022-06-25 10:20: Train Epoch 51: 220/528 Loss: 20.349234
2022-06-25 10:21: Train Epoch 51: 240/528 Loss: 19.752039
2022-06-25 10:22: Train Epoch 51: 260/528 Loss: 20.228582
2022-06-25 10:24: Train Epoch 51: 280/528 Loss: 20.590876
2022-06-25 10:25: Train Epoch 51: 300/528 Loss: 19.543585
2022-06-25 10:26: Train Epoch 51: 320/528 Loss: 19.271486
2022-06-25 10:27: Train Epoch 51: 340/528 Loss: 20.008656
2022-06-25 10:29: Train Epoch 51: 360/528 Loss: 17.673754
2022-06-25 10:30: Train Epoch 51: 380/528 Loss: 18.393087
2022-06-25 10:31: Train Epoch 51: 400/528 Loss: 19.040297
2022-06-25 10:32: Train Epoch 51: 420/528 Loss: 20.405262
2022-06-25 10:33: Train Epoch 51: 440/528 Loss: 18.421913
2022-06-25 10:34: Train Epoch 51: 460/528 Loss: 21.111280
2022-06-25 10:35: Train Epoch 51: 480/528 Loss: 19.650757
2022-06-25 10:36: Train Epoch 51: 500/528 Loss: 20.977528
2022-06-25 10:38: Train Epoch 51: 520/528 Loss: 20.292074
2022-06-25 10:38: **********Train Epoch 51: averaged Loss: 19.999570
2022-06-25 10:41: **********Val Epoch 51: average Loss: 20.502132
2022-06-25 10:41: *********************************Current best model saved!
2022-06-25 10:41: Train Epoch 52: 0/528 Loss: 20.335361
2022-06-25 10:43: Train Epoch 52: 20/528 Loss: 19.390720
2022-06-25 10:44: Train Epoch 52: 40/528 Loss: 18.914860
2022-06-25 10:45: Train Epoch 52: 60/528 Loss: 18.355772
2022-06-25 10:47: Train Epoch 52: 80/528 Loss: 19.607288
2022-06-25 10:48: Train Epoch 52: 100/528 Loss: 19.630890
2022-06-25 10:49: Train Epoch 52: 120/528 Loss: 21.571611
2022-06-25 10:50: Train Epoch 52: 140/528 Loss: 20.117456
2022-06-25 10:51: Train Epoch 52: 160/528 Loss: 21.255718
2022-06-25 10:52: Train Epoch 52: 180/528 Loss: 19.479557
2022-06-25 10:53: Train Epoch 52: 200/528 Loss: 19.421259
2022-06-25 10:55: Train Epoch 52: 220/528 Loss: 19.813675
2022-06-25 10:56: Train Epoch 52: 240/528 Loss: 19.972027
2022-06-25 10:57: Train Epoch 52: 260/528 Loss: 17.668194
2022-06-25 10:58: Train Epoch 52: 280/528 Loss: 18.441912
2022-06-25 10:59: Train Epoch 52: 300/528 Loss: 19.012739
2022-06-25 11:01: Train Epoch 52: 320/528 Loss: 19.260132
2022-06-25 11:02: Train Epoch 52: 340/528 Loss: 19.365711
2022-06-25 11:03: Train Epoch 52: 360/528 Loss: 19.260359
2022-06-25 11:04: Train Epoch 52: 380/528 Loss: 20.003315
2022-06-25 11:05: Train Epoch 52: 400/528 Loss: 19.159784
2022-06-25 11:06: Train Epoch 52: 420/528 Loss: 20.043295
2022-06-25 11:07: Train Epoch 52: 440/528 Loss: 20.940544
2022-06-25 11:09: Train Epoch 52: 460/528 Loss: 22.196241
2022-06-25 11:10: Train Epoch 52: 480/528 Loss: 19.964930
2022-06-25 11:11: Train Epoch 52: 500/528 Loss: 19.077536
2022-06-25 11:12: Train Epoch 52: 520/528 Loss: 20.442417
2022-06-25 11:12: **********Train Epoch 52: averaged Loss: 19.837030
2022-06-25 11:16: **********Val Epoch 52: average Loss: 20.497424
2022-06-25 11:16: *********************************Current best model saved!
2022-06-25 11:16: Train Epoch 53: 0/528 Loss: 20.266645
2022-06-25 11:17: Train Epoch 53: 20/528 Loss: 21.876286
2022-06-25 11:19: Train Epoch 53: 40/528 Loss: 20.523632
2022-06-25 11:20: Train Epoch 53: 60/528 Loss: 21.024963
2022-06-25 11:21: Train Epoch 53: 80/528 Loss: 20.162004
2022-06-25 11:22: Train Epoch 53: 100/528 Loss: 18.331141
2022-06-25 11:23: Train Epoch 53: 120/528 Loss: 19.546432
2022-06-25 11:24: Train Epoch 53: 140/528 Loss: 19.424208
2022-06-25 11:25: Train Epoch 53: 160/528 Loss: 19.406891
2022-06-25 11:27: Train Epoch 53: 180/528 Loss: 20.782536
2022-06-25 11:28: Train Epoch 53: 200/528 Loss: 18.939358
2022-06-25 11:29: Train Epoch 53: 220/528 Loss: 19.909622
2022-06-25 11:30: Train Epoch 53: 240/528 Loss: 18.158697
2022-06-25 11:31: Train Epoch 53: 260/528 Loss: 17.437565
2022-06-25 11:32: Train Epoch 53: 280/528 Loss: 19.181932
2022-06-25 11:34: Train Epoch 53: 300/528 Loss: 18.795361
2022-06-25 11:35: Train Epoch 53: 320/528 Loss: 21.376143
2022-06-25 11:36: Train Epoch 53: 340/528 Loss: 20.966822
2022-06-25 11:37: Train Epoch 53: 360/528 Loss: 18.755901
2022-06-25 11:39: Train Epoch 53: 380/528 Loss: 18.662743
2022-06-25 11:40: Train Epoch 53: 400/528 Loss: 20.164614
2022-06-25 11:41: Train Epoch 53: 420/528 Loss: 20.370823
2022-06-25 11:42: Train Epoch 53: 440/528 Loss: 20.749008
2022-06-25 11:43: Train Epoch 53: 460/528 Loss: 18.859592
2022-06-25 11:44: Train Epoch 53: 480/528 Loss: 19.026949
2022-06-25 11:45: Train Epoch 53: 500/528 Loss: 20.298969
2022-06-25 11:46: Train Epoch 53: 520/528 Loss: 19.580326
2022-06-25 11:47: **********Train Epoch 53: averaged Loss: 19.792665
2022-06-25 11:50: **********Val Epoch 53: average Loss: 20.534354
2022-06-25 11:50: Train Epoch 54: 0/528 Loss: 22.327747
2022-06-25 11:52: Train Epoch 54: 20/528 Loss: 20.428392
2022-06-25 11:53: Train Epoch 54: 40/528 Loss: 20.672571
2022-06-25 11:54: Train Epoch 54: 60/528 Loss: 20.946760
2022-06-25 11:55: Train Epoch 54: 80/528 Loss: 20.241362
2022-06-25 11:56: Train Epoch 54: 100/528 Loss: 18.958656
2022-06-25 11:58: Train Epoch 54: 120/528 Loss: 18.532692
2022-06-25 11:59: Train Epoch 54: 140/528 Loss: 19.290611
2022-06-25 12:00: Train Epoch 54: 160/528 Loss: 20.698652
2022-06-25 12:01: Train Epoch 54: 180/528 Loss: 18.481911
2022-06-25 12:02: Train Epoch 54: 200/528 Loss: 18.639305
2022-06-25 12:03: Train Epoch 54: 220/528 Loss: 19.955425
2022-06-25 12:05: Train Epoch 54: 240/528 Loss: 19.629208
2022-06-25 12:06: Train Epoch 54: 260/528 Loss: 20.145233
2022-06-25 12:07: Train Epoch 54: 280/528 Loss: 19.254030
2022-06-25 12:08: Train Epoch 54: 300/528 Loss: 18.695398
2022-06-25 12:09: Train Epoch 54: 320/528 Loss: 17.831913
2022-06-25 12:11: Train Epoch 54: 340/528 Loss: 19.342178
2022-06-25 12:12: Train Epoch 54: 360/528 Loss: 20.862236
2022-06-25 12:13: Train Epoch 54: 380/528 Loss: 20.582932
2022-06-25 12:14: Train Epoch 54: 400/528 Loss: 19.793650
2022-06-25 12:15: Train Epoch 54: 420/528 Loss: 19.547955
2022-06-25 12:17: Train Epoch 54: 440/528 Loss: 19.475151
2022-06-25 12:18: Train Epoch 54: 460/528 Loss: 21.371912
2022-06-25 12:19: Train Epoch 54: 480/528 Loss: 20.562277
2022-06-25 12:20: Train Epoch 54: 500/528 Loss: 21.009571
2022-06-25 12:21: Train Epoch 54: 520/528 Loss: 20.740276
2022-06-25 12:21: **********Train Epoch 54: averaged Loss: 19.971344
2022-06-25 12:25: **********Val Epoch 54: average Loss: 20.585123
2022-06-25 12:25: Train Epoch 55: 0/528 Loss: 19.522242
2022-06-25 12:26: Train Epoch 55: 20/528 Loss: 19.800562
2022-06-25 12:28: Train Epoch 55: 40/528 Loss: 18.200886
2022-06-25 12:29: Train Epoch 55: 60/528 Loss: 19.556870
2022-06-25 12:30: Train Epoch 55: 80/528 Loss: 17.536507
2022-06-25 12:31: Train Epoch 55: 100/528 Loss: 19.765007
2022-06-25 12:32: Train Epoch 55: 120/528 Loss: 19.344492
2022-06-25 12:34: Train Epoch 55: 140/528 Loss: 16.804153
2022-06-25 12:35: Train Epoch 55: 160/528 Loss: 17.973425
2022-06-25 12:36: Train Epoch 55: 180/528 Loss: 21.122105
2022-06-25 12:37: Train Epoch 55: 200/528 Loss: 19.354654
2022-06-25 12:38: Train Epoch 55: 220/528 Loss: 20.015316
2022-06-25 12:39: Train Epoch 55: 240/528 Loss: 18.678827
2022-06-25 12:41: Train Epoch 55: 260/528 Loss: 18.761959
2022-06-25 12:42: Train Epoch 55: 280/528 Loss: 20.585325
2022-06-25 12:43: Train Epoch 55: 300/528 Loss: 19.876631
2022-06-25 12:44: Train Epoch 55: 320/528 Loss: 21.307590
2022-06-25 12:46: Train Epoch 55: 340/528 Loss: 19.952219
2022-06-25 12:47: Train Epoch 55: 360/528 Loss: 18.981461
2022-06-25 12:48: Train Epoch 55: 380/528 Loss: 21.260111
2022-06-25 12:50: Train Epoch 55: 400/528 Loss: 20.457867
2022-06-25 12:50: Train Epoch 55: 420/528 Loss: 20.732754
2022-06-25 12:51: Train Epoch 55: 440/528 Loss: 19.616205
2022-06-25 12:53: Train Epoch 55: 460/528 Loss: 19.994194
2022-06-25 12:54: Train Epoch 55: 480/528 Loss: 21.725256
2022-06-25 12:55: Train Epoch 55: 500/528 Loss: 19.428951
2022-06-25 12:56: Train Epoch 55: 520/528 Loss: 20.402073
2022-06-25 12:56: **********Train Epoch 55: averaged Loss: 19.763699
2022-06-25 13:00: **********Val Epoch 55: average Loss: 20.558653
2022-06-25 13:00: Train Epoch 56: 0/528 Loss: 18.445349
2022-06-25 13:01: Train Epoch 56: 20/528 Loss: 19.866198
2022-06-25 13:03: Train Epoch 56: 40/528 Loss: 19.875744
2022-06-25 13:04: Train Epoch 56: 60/528 Loss: 18.536346
2022-06-25 13:05: Train Epoch 56: 80/528 Loss: 19.897827
2022-06-25 13:07: Train Epoch 56: 100/528 Loss: 20.340471
2022-06-25 13:08: Train Epoch 56: 120/528 Loss: 17.911587
2022-06-25 13:09: Train Epoch 56: 140/528 Loss: 17.186819
2022-06-25 13:10: Train Epoch 56: 160/528 Loss: 19.424448
2022-06-25 13:11: Train Epoch 56: 180/528 Loss: 20.843582
2022-06-25 13:12: Train Epoch 56: 200/528 Loss: 17.481695
2022-06-25 13:13: Train Epoch 56: 220/528 Loss: 21.717064
2022-06-25 13:14: Train Epoch 56: 240/528 Loss: 20.562902
2022-06-25 13:15: Train Epoch 56: 260/528 Loss: 21.197464
2022-06-25 13:17: Train Epoch 56: 280/528 Loss: 19.244652
2022-06-25 13:18: Train Epoch 56: 300/528 Loss: 20.794645
2022-06-25 13:19: Train Epoch 56: 320/528 Loss: 19.224684
2022-06-25 13:20: Train Epoch 56: 340/528 Loss: 20.179998
2022-06-25 13:22: Train Epoch 56: 360/528 Loss: 20.217247
2022-06-25 13:23: Train Epoch 56: 380/528 Loss: 20.357548
2022-06-25 13:24: Train Epoch 56: 400/528 Loss: 21.317076
2022-06-25 13:26: Train Epoch 56: 420/528 Loss: 20.326872
2022-06-25 13:27: Train Epoch 56: 440/528 Loss: 19.763683
2022-06-25 13:28: Train Epoch 56: 460/528 Loss: 20.323776
2022-06-25 13:29: Train Epoch 56: 480/528 Loss: 21.072929
2022-06-25 13:30: Train Epoch 56: 500/528 Loss: 20.591204
2022-06-25 13:31: Train Epoch 56: 520/528 Loss: 18.857656
2022-06-25 13:32: **********Train Epoch 56: averaged Loss: 19.744129
2022-06-25 13:35: **********Val Epoch 56: average Loss: 20.997904
2022-06-25 13:35: Train Epoch 57: 0/528 Loss: 18.806555
2022-06-25 13:36: Train Epoch 57: 20/528 Loss: 18.603079
2022-06-25 13:38: Train Epoch 57: 40/528 Loss: 19.310566
2022-06-25 13:39: Train Epoch 57: 60/528 Loss: 20.541142
2022-06-25 13:40: Train Epoch 57: 80/528 Loss: 19.046648
2022-06-25 13:42: Train Epoch 57: 100/528 Loss: 18.929472
2022-06-25 13:43: Train Epoch 57: 120/528 Loss: 19.207066
2022-06-25 13:44: Train Epoch 57: 140/528 Loss: 18.480961
2022-06-25 13:45: Train Epoch 57: 160/528 Loss: 18.603374
2022-06-25 13:46: Train Epoch 57: 180/528 Loss: 20.126587
2022-06-25 13:48: Train Epoch 57: 200/528 Loss: 19.993856
2022-06-25 13:49: Train Epoch 57: 220/528 Loss: 20.397104
2022-06-25 13:50: Train Epoch 57: 240/528 Loss: 21.412996
2022-06-25 13:51: Train Epoch 57: 260/528 Loss: 19.060497
2022-06-25 13:52: Train Epoch 57: 280/528 Loss: 20.222298
2022-06-25 13:53: Train Epoch 57: 300/528 Loss: 19.805180
2022-06-25 13:54: Train Epoch 57: 320/528 Loss: 19.537962
2022-06-25 13:56: Train Epoch 57: 340/528 Loss: 19.881968
2022-06-25 13:57: Train Epoch 57: 360/528 Loss: 19.509640
2022-06-25 13:58: Train Epoch 57: 380/528 Loss: 19.069998
2022-06-25 14:00: Train Epoch 57: 400/528 Loss: 19.555538
2022-06-25 14:01: Train Epoch 57: 420/528 Loss: 19.481325
2022-06-25 14:02: Train Epoch 57: 440/528 Loss: 19.789907
2022-06-25 14:03: Train Epoch 57: 460/528 Loss: 21.489756
2022-06-25 14:04: Train Epoch 57: 480/528 Loss: 19.617542
2022-06-25 14:05: Train Epoch 57: 500/528 Loss: 19.603680
2022-06-25 14:07: Train Epoch 57: 520/528 Loss: 19.848848
2022-06-25 14:07: **********Train Epoch 57: averaged Loss: 19.823185
2022-06-25 14:10: **********Val Epoch 57: average Loss: 21.226137
2022-06-25 14:10: Train Epoch 58: 0/528 Loss: 20.156101
2022-06-25 14:12: Train Epoch 58: 20/528 Loss: 18.997107
2022-06-25 14:13: Train Epoch 58: 40/528 Loss: 20.550188
2022-06-25 14:14: Train Epoch 58: 60/528 Loss: 19.639469
2022-06-25 14:15: Train Epoch 58: 80/528 Loss: 18.095060
2022-06-25 14:17: Train Epoch 58: 100/528 Loss: 19.690945
2022-06-25 14:18: Train Epoch 58: 120/528 Loss: 19.325525
2022-06-25 14:19: Train Epoch 58: 140/528 Loss: 22.616852
2022-06-25 14:20: Train Epoch 58: 160/528 Loss: 20.363441
2022-06-25 14:22: Train Epoch 58: 180/528 Loss: 20.218571
2022-06-25 14:23: Train Epoch 58: 200/528 Loss: 19.999918
2022-06-25 14:24: Train Epoch 58: 220/528 Loss: 18.724340
2022-06-25 14:25: Train Epoch 58: 240/528 Loss: 21.791588
2022-06-25 14:26: Train Epoch 58: 260/528 Loss: 21.068304
2022-06-25 14:27: Train Epoch 58: 280/528 Loss: 20.627775
2022-06-25 14:28: Train Epoch 58: 300/528 Loss: 20.373999
2022-06-25 14:30: Train Epoch 58: 320/528 Loss: 21.896482
2022-06-25 14:31: Train Epoch 58: 340/528 Loss: 21.319382
2022-06-25 14:32: Train Epoch 58: 360/528 Loss: 19.404137
2022-06-25 14:33: Train Epoch 58: 380/528 Loss: 19.113413
2022-06-25 14:35: Train Epoch 58: 400/528 Loss: 18.042677
2022-06-25 14:36: Train Epoch 58: 420/528 Loss: 22.053217
2022-06-25 14:37: Train Epoch 58: 440/528 Loss: 22.079607
2022-06-25 14:38: Train Epoch 58: 460/528 Loss: 18.977797
2022-06-25 14:39: Train Epoch 58: 480/528 Loss: 20.163134
2022-06-25 14:41: Train Epoch 58: 500/528 Loss: 19.207548
2022-06-25 14:42: Train Epoch 58: 520/528 Loss: 18.744720
2022-06-25 14:42: **********Train Epoch 58: averaged Loss: 19.874974
2022-06-25 14:45: **********Val Epoch 58: average Loss: 20.471001
2022-06-25 14:45: *********************************Current best model saved!
2022-06-25 14:45: Train Epoch 59: 0/528 Loss: 21.082930
2022-06-25 14:47: Train Epoch 59: 20/528 Loss: 21.390800
2022-06-25 14:48: Train Epoch 59: 40/528 Loss: 21.258881
2022-06-25 14:49: Train Epoch 59: 60/528 Loss: 20.599504
2022-06-25 14:50: Train Epoch 59: 80/528 Loss: 20.195078
2022-06-25 14:52: Train Epoch 59: 100/528 Loss: 19.785858
2022-06-25 14:53: Train Epoch 59: 120/528 Loss: 20.051586
2022-06-25 14:54: Train Epoch 59: 140/528 Loss: 19.985703
2022-06-25 14:56: Train Epoch 59: 160/528 Loss: 19.032120
2022-06-25 14:57: Train Epoch 59: 180/528 Loss: 19.583275
2022-06-25 14:58: Train Epoch 59: 200/528 Loss: 20.100233
2022-06-25 14:59: Train Epoch 59: 220/528 Loss: 21.408854
2022-06-25 15:00: Train Epoch 59: 240/528 Loss: 19.545898
2022-06-25 15:01: Train Epoch 59: 260/528 Loss: 20.430294
2022-06-25 15:02: Train Epoch 59: 280/528 Loss: 18.917791
2022-06-25 15:03: Train Epoch 59: 300/528 Loss: 21.404062
2022-06-25 15:05: Train Epoch 59: 320/528 Loss: 19.858622
2022-06-25 15:06: Train Epoch 59: 340/528 Loss: 18.962017
2022-06-25 15:07: Train Epoch 59: 360/528 Loss: 19.366552
2022-06-25 15:08: Train Epoch 59: 380/528 Loss: 18.360588
2022-06-25 15:09: Train Epoch 59: 400/528 Loss: 21.158005
2022-06-25 15:11: Train Epoch 59: 420/528 Loss: 17.748623
2022-06-25 15:12: Train Epoch 59: 440/528 Loss: 20.052763
2022-06-25 15:13: Train Epoch 59: 460/528 Loss: 20.344360
2022-06-25 15:14: Train Epoch 59: 480/528 Loss: 20.651361
2022-06-25 15:16: Train Epoch 59: 500/528 Loss: 20.527891
2022-06-25 15:17: Train Epoch 59: 520/528 Loss: 19.362656
2022-06-25 15:17: **********Train Epoch 59: averaged Loss: 19.923082
2022-06-25 15:21: **********Val Epoch 59: average Loss: 20.637954
2022-06-25 15:21: Train Epoch 60: 0/528 Loss: 18.210943
2022-06-25 15:22: Train Epoch 60: 20/528 Loss: 18.607746
2022-06-25 15:23: Train Epoch 60: 40/528 Loss: 19.138876
2022-06-25 15:24: Train Epoch 60: 60/528 Loss: 19.277378
2022-06-25 15:25: Train Epoch 60: 80/528 Loss: 19.162252
2022-06-25 15:26: Train Epoch 60: 100/528 Loss: 20.326998
2022-06-25 15:28: Train Epoch 60: 120/528 Loss: 19.200144
2022-06-25 15:29: Train Epoch 60: 140/528 Loss: 19.623451
2022-06-25 15:30: Train Epoch 60: 160/528 Loss: 20.059858
2022-06-25 15:31: Train Epoch 60: 180/528 Loss: 18.031601
2022-06-25 15:33: Train Epoch 60: 200/528 Loss: 20.859180
2022-06-25 15:34: Train Epoch 60: 220/528 Loss: 19.147055
2022-06-25 15:35: Train Epoch 60: 240/528 Loss: 18.641535
2022-06-25 15:36: Train Epoch 60: 260/528 Loss: 19.091328
2022-06-25 15:37: Train Epoch 60: 280/528 Loss: 20.235651
2022-06-25 15:38: Train Epoch 60: 300/528 Loss: 20.210018
2022-06-25 15:40: Train Epoch 60: 320/528 Loss: 20.687199
2022-06-25 15:41: Train Epoch 60: 340/528 Loss: 19.175003
2022-06-25 15:42: Train Epoch 60: 360/528 Loss: 21.137955
2022-06-25 15:43: Train Epoch 60: 380/528 Loss: 19.210257
2022-06-25 15:44: Train Epoch 60: 400/528 Loss: 20.812477
2022-06-25 15:45: Train Epoch 60: 420/528 Loss: 18.707827
2022-06-25 15:47: Train Epoch 60: 440/528 Loss: 19.485098
2022-06-25 15:48: Train Epoch 60: 460/528 Loss: 19.944157
2022-06-25 15:49: Train Epoch 60: 480/528 Loss: 19.786600
2022-06-25 15:51: Train Epoch 60: 500/528 Loss: 20.208019
2022-06-25 15:52: Train Epoch 60: 520/528 Loss: 21.577431
2022-06-25 15:52: **********Train Epoch 60: averaged Loss: 19.770024
2022-06-25 15:56: **********Val Epoch 60: average Loss: 20.562902
2022-06-25 15:56: Train Epoch 61: 0/528 Loss: 18.288347
2022-06-25 15:57: Train Epoch 61: 20/528 Loss: 18.973082
2022-06-25 15:58: Train Epoch 61: 40/528 Loss: 18.450178
2022-06-25 15:59: Train Epoch 61: 60/528 Loss: 18.824568
2022-06-25 16:00: Train Epoch 61: 80/528 Loss: 20.981449
2022-06-25 16:02: Train Epoch 61: 100/528 Loss: 19.937195
2022-06-25 16:03: Train Epoch 61: 120/528 Loss: 20.356039
2022-06-25 16:04: Train Epoch 61: 140/528 Loss: 20.425205
2022-06-25 16:05: Train Epoch 61: 160/528 Loss: 19.908489
2022-06-25 16:07: Train Epoch 61: 180/528 Loss: 19.488791
2022-06-25 16:08: Train Epoch 61: 200/528 Loss: 19.785078
2022-06-25 16:09: Train Epoch 61: 220/528 Loss: 18.099127
2022-06-25 16:11: Train Epoch 61: 240/528 Loss: 20.494396
2022-06-25 16:12: Train Epoch 61: 260/528 Loss: 20.834299
2022-06-25 16:13: Train Epoch 61: 280/528 Loss: 21.959423
2022-06-25 16:14: Train Epoch 61: 300/528 Loss: 18.977713
2022-06-25 16:15: Train Epoch 61: 320/528 Loss: 19.256800
2022-06-25 16:16: Train Epoch 61: 340/528 Loss: 20.449413
2022-06-25 16:17: Train Epoch 61: 360/528 Loss: 19.738230
2022-06-25 16:18: Train Epoch 61: 380/528 Loss: 19.060114
2022-06-25 16:20: Train Epoch 61: 400/528 Loss: 19.080263
2022-06-25 16:21: Train Epoch 61: 420/528 Loss: 16.497404
2022-06-25 16:22: Train Epoch 61: 440/528 Loss: 19.790840
2022-06-25 16:23: Train Epoch 61: 460/528 Loss: 19.460543
2022-06-25 16:24: Train Epoch 61: 480/528 Loss: 21.417559
2022-06-25 16:26: Train Epoch 61: 500/528 Loss: 19.213852
2022-06-25 16:27: Train Epoch 61: 520/528 Loss: 17.986393
2022-06-25 16:27: **********Train Epoch 61: averaged Loss: 19.761563
2022-06-25 16:31: **********Val Epoch 61: average Loss: 20.593476
2022-06-25 16:31: Train Epoch 62: 0/528 Loss: 21.765884
2022-06-25 16:32: Train Epoch 62: 20/528 Loss: 19.872263
2022-06-25 16:33: Train Epoch 62: 40/528 Loss: 19.670155
2022-06-25 16:34: Train Epoch 62: 60/528 Loss: 21.670746
2022-06-25 16:36: Train Epoch 62: 80/528 Loss: 19.400751
2022-06-25 16:37: Train Epoch 62: 100/528 Loss: 18.842842
2022-06-25 16:38: Train Epoch 62: 120/528 Loss: 18.361902
2022-06-25 16:39: Train Epoch 62: 140/528 Loss: 21.240904
2022-06-25 16:40: Train Epoch 62: 160/528 Loss: 18.716505
2022-06-25 16:41: Train Epoch 62: 180/528 Loss: 19.229609
2022-06-25 16:43: Train Epoch 62: 200/528 Loss: 20.796852
2022-06-25 16:44: Train Epoch 62: 220/528 Loss: 19.789940
2022-06-25 16:45: Train Epoch 62: 240/528 Loss: 19.053392
2022-06-25 16:47: Train Epoch 62: 260/528 Loss: 18.552528
2022-06-25 16:48: Train Epoch 62: 280/528 Loss: 19.445124
2022-06-25 16:49: Train Epoch 62: 300/528 Loss: 20.180487
2022-06-25 16:50: Train Epoch 62: 320/528 Loss: 19.738770
2022-06-25 16:51: Train Epoch 62: 340/528 Loss: 20.556623
2022-06-25 16:52: Train Epoch 62: 360/528 Loss: 17.845242
2022-06-25 16:54: Train Epoch 62: 380/528 Loss: 20.070023
2022-06-25 16:55: Train Epoch 62: 400/528 Loss: 19.340624
2022-06-25 16:56: Train Epoch 62: 420/528 Loss: 19.310699
2022-06-25 16:57: Train Epoch 62: 440/528 Loss: 22.127346
2022-06-25 16:58: Train Epoch 62: 460/528 Loss: 21.535429
2022-06-25 16:59: Train Epoch 62: 480/528 Loss: 20.895353
2022-06-25 17:01: Train Epoch 62: 500/528 Loss: 20.327505
2022-06-25 17:02: Train Epoch 62: 520/528 Loss: 21.118416
2022-06-25 17:02: **********Train Epoch 62: averaged Loss: 19.876020
2022-06-25 17:06: **********Val Epoch 62: average Loss: 20.616893
2022-06-25 17:06: Train Epoch 63: 0/528 Loss: 17.649792
2022-06-25 17:07: Train Epoch 63: 20/528 Loss: 18.184427
2022-06-25 17:09: Train Epoch 63: 40/528 Loss: 21.029484
2022-06-25 17:10: Train Epoch 63: 60/528 Loss: 19.666252
2022-06-25 17:11: Train Epoch 63: 80/528 Loss: 19.263657
2022-06-25 17:12: Train Epoch 63: 100/528 Loss: 18.609375
2022-06-25 17:13: Train Epoch 63: 120/528 Loss: 19.295885
2022-06-25 17:14: Train Epoch 63: 140/528 Loss: 19.734985
2022-06-25 17:15: Train Epoch 63: 160/528 Loss: 21.021257
2022-06-25 17:16: Train Epoch 63: 180/528 Loss: 18.623871
2022-06-25 17:18: Train Epoch 63: 200/528 Loss: 19.907854
2022-06-25 17:19: Train Epoch 63: 220/528 Loss: 21.972797
2022-06-25 17:20: Train Epoch 63: 240/528 Loss: 20.136961
2022-06-25 17:22: Train Epoch 63: 260/528 Loss: 19.130890
2022-06-25 17:23: Train Epoch 63: 280/528 Loss: 19.858522
2022-06-25 17:24: Train Epoch 63: 300/528 Loss: 19.545092
2022-06-25 17:25: Train Epoch 63: 320/528 Loss: 21.621183
2022-06-25 17:27: Train Epoch 63: 340/528 Loss: 20.158642
2022-06-25 17:28: Train Epoch 63: 360/528 Loss: 20.756250
2022-06-25 17:29: Train Epoch 63: 380/528 Loss: 20.122053
2022-06-25 17:30: Train Epoch 63: 400/528 Loss: 18.227699
2022-06-25 17:31: Train Epoch 63: 420/528 Loss: 18.052856
2022-06-25 17:32: Train Epoch 63: 440/528 Loss: 19.215857
2022-06-25 17:33: Train Epoch 63: 460/528 Loss: 17.745274
2022-06-25 17:34: Train Epoch 63: 480/528 Loss: 21.350779
2022-06-25 17:36: Train Epoch 63: 500/528 Loss: 20.614319
2022-06-25 17:37: Train Epoch 63: 520/528 Loss: 20.147573
2022-06-25 17:37: **********Train Epoch 63: averaged Loss: 19.962643
2022-06-25 17:41: **********Val Epoch 63: average Loss: 20.592855
2022-06-25 17:41: Train Epoch 64: 0/528 Loss: 19.094759
2022-06-25 17:42: Train Epoch 64: 20/528 Loss: 19.436888
2022-06-25 17:44: Train Epoch 64: 40/528 Loss: 18.765532
2022-06-25 17:45: Train Epoch 64: 60/528 Loss: 19.663671
2022-06-25 17:46: Train Epoch 64: 80/528 Loss: 20.068388
2022-06-25 17:47: Train Epoch 64: 100/528 Loss: 21.604282
2022-06-25 17:49: Train Epoch 64: 120/528 Loss: 18.385490
2022-06-25 17:50: Train Epoch 64: 140/528 Loss: 21.832987
2022-06-25 17:51: Train Epoch 64: 160/528 Loss: 20.665596
2022-06-25 17:52: Train Epoch 64: 180/528 Loss: 21.251238
2022-06-25 17:53: Train Epoch 64: 200/528 Loss: 19.959623
2022-06-25 17:54: Train Epoch 64: 220/528 Loss: 20.227575
2022-06-25 17:55: Train Epoch 64: 240/528 Loss: 20.343288
2022-06-25 17:56: Train Epoch 64: 260/528 Loss: 20.145437
2022-06-25 17:57: Train Epoch 64: 280/528 Loss: 18.832907
2022-06-25 17:59: Train Epoch 64: 300/528 Loss: 21.214569
2022-06-25 18:00: Train Epoch 64: 320/528 Loss: 19.329941
2022-06-25 18:01: Train Epoch 64: 340/528 Loss: 18.278080
2022-06-25 18:03: Train Epoch 64: 360/528 Loss: 20.276823
2022-06-25 18:04: Train Epoch 64: 380/528 Loss: 20.260149
2022-06-25 18:05: Train Epoch 64: 400/528 Loss: 20.101278
2022-06-25 18:06: Train Epoch 64: 420/528 Loss: 21.073885
2022-06-25 18:08: Train Epoch 64: 440/528 Loss: 19.662951
2022-06-25 18:09: Train Epoch 64: 460/528 Loss: 19.952288
2022-06-25 18:10: Train Epoch 64: 480/528 Loss: 19.252142
2022-06-25 18:11: Train Epoch 64: 500/528 Loss: 18.402111
2022-06-25 18:12: Train Epoch 64: 520/528 Loss: 20.265253
2022-06-25 18:12: **********Train Epoch 64: averaged Loss: 19.692934
2022-06-25 18:16: **********Val Epoch 64: average Loss: 20.641775
2022-06-25 18:16: Train Epoch 65: 0/528 Loss: 19.265223
2022-06-25 18:17: Train Epoch 65: 20/528 Loss: 19.344515
2022-06-25 18:18: Train Epoch 65: 40/528 Loss: 18.459761
2022-06-25 18:19: Train Epoch 65: 60/528 Loss: 19.745153
2022-06-25 18:21: Train Epoch 65: 80/528 Loss: 21.150341
2022-06-25 18:22: Train Epoch 65: 100/528 Loss: 18.835861
2022-06-25 18:23: Train Epoch 65: 120/528 Loss: 21.521322
2022-06-25 18:25: Train Epoch 65: 140/528 Loss: 20.460634
2022-06-25 18:26: Train Epoch 65: 160/528 Loss: 20.860544
2022-06-25 18:27: Train Epoch 65: 180/528 Loss: 20.482336
2022-06-25 18:28: Train Epoch 65: 200/528 Loss: 18.645063
2022-06-25 18:29: Train Epoch 65: 220/528 Loss: 18.352442
2022-06-25 18:31: Train Epoch 65: 240/528 Loss: 19.558107
2022-06-25 18:32: Train Epoch 65: 260/528 Loss: 18.858105
2022-06-25 18:33: Train Epoch 65: 280/528 Loss: 19.613752
2022-06-25 18:34: Train Epoch 65: 300/528 Loss: 17.824568
2022-06-25 18:35: Train Epoch 65: 320/528 Loss: 20.477377
2022-06-25 18:36: Train Epoch 65: 340/528 Loss: 16.665167
2022-06-25 18:38: Train Epoch 65: 360/528 Loss: 17.934702
2022-06-25 18:39: Train Epoch 65: 380/528 Loss: 20.249031
2022-06-25 18:40: Train Epoch 65: 400/528 Loss: 21.812643
2022-06-25 18:41: Train Epoch 65: 420/528 Loss: 20.383621
2022-06-25 18:43: Train Epoch 65: 440/528 Loss: 20.964687
2022-06-25 18:44: Train Epoch 65: 460/528 Loss: 20.722460
2022-06-25 18:45: Train Epoch 65: 480/528 Loss: 18.652277
2022-06-25 18:46: Train Epoch 65: 500/528 Loss: 20.136873
2022-06-25 18:47: Train Epoch 65: 520/528 Loss: 19.952648
2022-06-25 18:48: **********Train Epoch 65: averaged Loss: 19.630177
2022-06-25 18:51: **********Val Epoch 65: average Loss: 20.502157
2022-06-25 18:51: Train Epoch 66: 0/528 Loss: 20.239388
2022-06-25 18:52: Train Epoch 66: 20/528 Loss: 18.876932
2022-06-25 18:53: Train Epoch 66: 40/528 Loss: 19.874901
2022-06-25 18:55: Train Epoch 66: 60/528 Loss: 20.706758
2022-06-25 18:56: Train Epoch 66: 80/528 Loss: 21.303970
2022-06-25 18:57: Train Epoch 66: 100/528 Loss: 18.122612
2022-06-25 18:58: Train Epoch 66: 120/528 Loss: 19.353706
2022-06-25 19:00: Train Epoch 66: 140/528 Loss: 20.206604
2022-06-25 19:01: Train Epoch 66: 160/528 Loss: 19.588732
2022-06-25 19:02: Train Epoch 66: 180/528 Loss: 19.708895
2022-06-25 19:03: Train Epoch 66: 200/528 Loss: 21.631315
2022-06-25 19:05: Train Epoch 66: 220/528 Loss: 20.072515
2022-06-25 19:06: Train Epoch 66: 240/528 Loss: 19.445803
2022-06-25 19:07: Train Epoch 66: 260/528 Loss: 19.153656
2022-06-25 19:08: Train Epoch 66: 280/528 Loss: 20.742964
2022-06-25 19:09: Train Epoch 66: 300/528 Loss: 19.961346
2022-06-25 19:10: Train Epoch 66: 320/528 Loss: 18.266851
2022-06-25 19:12: Train Epoch 66: 340/528 Loss: 19.756342
2022-06-25 19:13: Train Epoch 66: 360/528 Loss: 19.450939
2022-06-25 19:14: Train Epoch 66: 380/528 Loss: 18.484205
2022-06-25 19:15: Train Epoch 66: 400/528 Loss: 20.130928
2022-06-25 19:16: Train Epoch 66: 420/528 Loss: 20.545841
2022-06-25 19:18: Train Epoch 66: 440/528 Loss: 19.248959
2022-06-25 19:19: Train Epoch 66: 460/528 Loss: 17.719873
2022-06-25 19:20: Train Epoch 66: 480/528 Loss: 18.727173
2022-06-25 19:21: Train Epoch 66: 500/528 Loss: 19.908804
2022-06-25 19:23: Train Epoch 66: 520/528 Loss: 19.338404
2022-06-25 19:23: **********Train Epoch 66: averaged Loss: 19.635809
2022-06-25 19:26: **********Val Epoch 66: average Loss: 20.827575
2022-06-25 19:27: Train Epoch 67: 0/528 Loss: 21.777821
2022-06-25 19:28: Train Epoch 67: 20/528 Loss: 19.122486
2022-06-25 19:29: Train Epoch 67: 40/528 Loss: 19.484632
2022-06-25 19:30: Train Epoch 67: 60/528 Loss: 21.467405
2022-06-25 19:31: Train Epoch 67: 80/528 Loss: 20.213699
2022-06-25 19:32: Train Epoch 67: 100/528 Loss: 17.980780
2022-06-25 19:33: Train Epoch 67: 120/528 Loss: 18.291876
2022-06-25 19:35: Train Epoch 67: 140/528 Loss: 18.378513
2022-06-25 19:36: Train Epoch 67: 160/528 Loss: 17.584461
2022-06-25 19:37: Train Epoch 67: 180/528 Loss: 21.847597
2022-06-25 19:38: Train Epoch 67: 200/528 Loss: 20.169502
2022-06-25 19:40: Train Epoch 67: 220/528 Loss: 20.175814
2022-06-25 19:41: Train Epoch 67: 240/528 Loss: 20.446436
2022-06-25 19:42: Train Epoch 67: 260/528 Loss: 21.637320
2022-06-25 19:43: Train Epoch 67: 280/528 Loss: 19.511776
2022-06-25 19:44: Train Epoch 67: 300/528 Loss: 21.238260
2022-06-25 19:46: Train Epoch 67: 320/528 Loss: 20.473900
2022-06-25 19:47: Train Epoch 67: 340/528 Loss: 19.955700
2022-06-25 19:48: Train Epoch 67: 360/528 Loss: 21.195765
2022-06-25 19:49: Train Epoch 67: 380/528 Loss: 19.362951
2022-06-25 19:50: Train Epoch 67: 400/528 Loss: 21.204515
2022-06-25 19:51: Train Epoch 67: 420/528 Loss: 19.223101
2022-06-25 19:52: Train Epoch 67: 440/528 Loss: 19.195534
2022-06-25 19:54: Train Epoch 67: 460/528 Loss: 19.770082
2022-06-25 19:55: Train Epoch 67: 480/528 Loss: 19.116117
2022-06-25 19:56: Train Epoch 67: 500/528 Loss: 18.582991
2022-06-25 19:57: Train Epoch 67: 520/528 Loss: 18.230661
2022-06-25 19:58: **********Train Epoch 67: averaged Loss: 19.680246
2022-06-25 20:02: **********Val Epoch 67: average Loss: 20.533828
2022-06-25 20:02: Train Epoch 68: 0/528 Loss: 21.029890
2022-06-25 20:03: Train Epoch 68: 20/528 Loss: 18.976248
2022-06-25 20:04: Train Epoch 68: 40/528 Loss: 18.252808
2022-06-25 20:05: Train Epoch 68: 60/528 Loss: 20.923822
2022-06-25 20:06: Train Epoch 68: 80/528 Loss: 18.742670
2022-06-25 20:07: Train Epoch 68: 100/528 Loss: 20.112669
2022-06-25 20:08: Train Epoch 68: 120/528 Loss: 20.075285
2022-06-25 20:10: Train Epoch 68: 140/528 Loss: 20.226017
2022-06-25 20:11: Train Epoch 68: 160/528 Loss: 19.279188
2022-06-25 20:12: Train Epoch 68: 180/528 Loss: 18.550169
2022-06-25 20:13: Train Epoch 68: 200/528 Loss: 20.899525
2022-06-25 20:14: Train Epoch 68: 220/528 Loss: 20.711586
2022-06-25 20:16: Train Epoch 68: 240/528 Loss: 18.519812
2022-06-25 20:17: Train Epoch 68: 260/528 Loss: 19.770121
2022-06-25 20:18: Train Epoch 68: 280/528 Loss: 20.849644
2022-06-25 20:19: Train Epoch 68: 300/528 Loss: 20.195759
2022-06-25 20:21: Train Epoch 68: 320/528 Loss: 19.732656
2022-06-25 20:22: Train Epoch 68: 340/528 Loss: 20.884268
2022-06-25 20:23: Train Epoch 68: 360/528 Loss: 19.585548
2022-06-25 20:24: Train Epoch 68: 380/528 Loss: 18.860798
2022-06-25 20:26: Train Epoch 68: 400/528 Loss: 19.459251
2022-06-25 20:27: Train Epoch 68: 420/528 Loss: 18.975027
2022-06-25 20:28: Train Epoch 68: 440/528 Loss: 18.745157
2022-06-25 20:29: Train Epoch 68: 460/528 Loss: 20.347883
2022-06-25 20:30: Train Epoch 68: 480/528 Loss: 18.640026
2022-06-25 20:31: Train Epoch 68: 500/528 Loss: 20.119497
2022-06-25 20:32: Train Epoch 68: 520/528 Loss: 19.977762
2022-06-25 20:33: **********Train Epoch 68: averaged Loss: 19.731431
2022-06-25 20:36: **********Val Epoch 68: average Loss: 20.407176
2022-06-25 20:36: *********************************Current best model saved!
2022-06-25 20:36: Train Epoch 69: 0/528 Loss: 19.923794
2022-06-25 20:38: Train Epoch 69: 20/528 Loss: 18.096563
2022-06-25 20:39: Train Epoch 69: 40/528 Loss: 19.924648
2022-06-25 20:40: Train Epoch 69: 60/528 Loss: 18.874216
2022-06-25 20:42: Train Epoch 69: 80/528 Loss: 19.854288
2022-06-25 20:43: Train Epoch 69: 100/528 Loss: 18.290712
2022-06-25 20:44: Train Epoch 69: 120/528 Loss: 19.327740
2022-06-25 20:46: Train Epoch 69: 140/528 Loss: 19.377821
2022-06-25 20:47: Train Epoch 69: 160/528 Loss: 20.989988
2022-06-25 20:48: Train Epoch 69: 180/528 Loss: 18.210394
2022-06-25 20:49: Train Epoch 69: 200/528 Loss: 21.593916
2022-06-25 20:50: Train Epoch 69: 220/528 Loss: 18.651140
2022-06-25 20:51: Train Epoch 69: 240/528 Loss: 19.051405
2022-06-25 20:52: Train Epoch 69: 260/528 Loss: 18.033127
2022-06-25 20:53: Train Epoch 69: 280/528 Loss: 19.077494
2022-06-25 20:54: Train Epoch 69: 300/528 Loss: 18.701681
2022-06-25 20:56: Train Epoch 69: 320/528 Loss: 21.286547
2022-06-25 20:57: Train Epoch 69: 340/528 Loss: 19.226828
2022-06-25 20:58: Train Epoch 69: 360/528 Loss: 20.150522
2022-06-25 20:59: Train Epoch 69: 380/528 Loss: 20.387829
2022-06-25 21:01: Train Epoch 69: 400/528 Loss: 19.095001
2022-06-25 21:02: Train Epoch 69: 420/528 Loss: 21.665335
2022-06-25 21:03: Train Epoch 69: 440/528 Loss: 20.897627
2022-06-25 21:05: Train Epoch 69: 460/528 Loss: 19.853947
2022-06-25 21:06: Train Epoch 69: 480/528 Loss: 18.499277
2022-06-25 21:07: Train Epoch 69: 500/528 Loss: 21.551117
2022-06-25 21:08: Train Epoch 69: 520/528 Loss: 18.885921
2022-06-25 21:08: **********Train Epoch 69: averaged Loss: 19.664306
2022-06-25 21:12: **********Val Epoch 69: average Loss: 20.370327
2022-06-25 21:12: *********************************Current best model saved!
2022-06-25 21:12: Train Epoch 70: 0/528 Loss: 19.324793
2022-06-25 21:13: Train Epoch 70: 20/528 Loss: 20.210251
2022-06-25 21:14: Train Epoch 70: 40/528 Loss: 20.097507
2022-06-25 21:15: Train Epoch 70: 60/528 Loss: 19.041672
2022-06-25 21:16: Train Epoch 70: 80/528 Loss: 19.705782
2022-06-25 21:18: Train Epoch 70: 100/528 Loss: 20.452400
2022-06-25 21:19: Train Epoch 70: 120/528 Loss: 20.714226
2022-06-25 21:20: Train Epoch 70: 140/528 Loss: 19.953896
2022-06-25 21:21: Train Epoch 70: 160/528 Loss: 18.179420
2022-06-25 21:23: Train Epoch 70: 180/528 Loss: 20.247911
2022-06-25 21:24: Train Epoch 70: 200/528 Loss: 21.143114
2022-06-25 21:26: Train Epoch 70: 220/528 Loss: 20.459209
2022-06-25 21:27: Train Epoch 70: 240/528 Loss: 20.020115
2022-06-25 21:28: Train Epoch 70: 260/528 Loss: 20.985987
2022-06-25 21:29: Train Epoch 70: 280/528 Loss: 19.275288
2022-06-25 21:30: Train Epoch 70: 300/528 Loss: 20.767717
2022-06-25 21:31: Train Epoch 70: 320/528 Loss: 20.939201
2022-06-25 21:32: Train Epoch 70: 340/528 Loss: 20.360615
2022-06-25 21:33: Train Epoch 70: 360/528 Loss: 19.330339
2022-06-25 21:34: Train Epoch 70: 380/528 Loss: 21.339096
2022-06-25 21:35: Train Epoch 70: 400/528 Loss: 20.333771
2022-06-25 21:37: Train Epoch 70: 420/528 Loss: 20.649738
2022-06-25 21:38: Train Epoch 70: 440/528 Loss: 18.561310
2022-06-25 21:39: Train Epoch 70: 460/528 Loss: 21.141029
2022-06-25 21:40: Train Epoch 70: 480/528 Loss: 18.511650
2022-06-25 21:42: Train Epoch 70: 500/528 Loss: 21.797215
2022-06-25 21:43: Train Epoch 70: 520/528 Loss: 19.668520
2022-06-25 21:44: **********Train Epoch 70: averaged Loss: 19.760788
2022-06-25 21:47: **********Val Epoch 70: average Loss: 20.641865
2022-06-25 21:47: Train Epoch 71: 0/528 Loss: 21.645809
2022-06-25 21:48: Train Epoch 71: 20/528 Loss: 21.216677
2022-06-25 21:49: Train Epoch 71: 40/528 Loss: 20.530651
2022-06-25 21:51: Train Epoch 71: 60/528 Loss: 22.647547
2022-06-25 21:52: Train Epoch 71: 80/528 Loss: 20.763012
2022-06-25 21:53: Train Epoch 71: 100/528 Loss: 21.528156
2022-06-25 21:54: Train Epoch 71: 120/528 Loss: 19.855429
2022-06-25 21:55: Train Epoch 71: 140/528 Loss: 18.572693
2022-06-25 21:56: Train Epoch 71: 160/528 Loss: 19.225126
2022-06-25 21:57: Train Epoch 71: 180/528 Loss: 20.874432
2022-06-25 21:58: Train Epoch 71: 200/528 Loss: 20.478926
2022-06-25 22:00: Train Epoch 71: 220/528 Loss: 19.434568
2022-06-25 22:01: Train Epoch 71: 240/528 Loss: 19.659559
2022-06-25 22:02: Train Epoch 71: 260/528 Loss: 18.791075
2022-06-25 22:04: Train Epoch 71: 280/528 Loss: 21.188684
2022-06-25 22:05: Train Epoch 71: 300/528 Loss: 21.105553
2022-06-25 22:06: Train Epoch 71: 320/528 Loss: 18.305849
2022-06-25 22:07: Train Epoch 71: 340/528 Loss: 19.666876
2022-06-25 22:09: Train Epoch 71: 360/528 Loss: 18.363411
2022-06-25 22:10: Train Epoch 71: 380/528 Loss: 20.236027
2022-06-25 22:11: Train Epoch 71: 400/528 Loss: 20.264780
2022-06-25 22:12: Train Epoch 71: 420/528 Loss: 20.504501
2022-06-25 22:13: Train Epoch 71: 440/528 Loss: 19.602224
2022-06-25 22:14: Train Epoch 71: 460/528 Loss: 20.069809
2022-06-25 22:15: Train Epoch 71: 480/528 Loss: 18.849112
2022-06-25 22:16: Train Epoch 71: 500/528 Loss: 19.921736
2022-06-25 22:17: Train Epoch 71: 520/528 Loss: 19.716148
2022-06-25 22:18: **********Train Epoch 71: averaged Loss: 19.713005
2022-06-25 22:21: **********Val Epoch 71: average Loss: 20.869382
2022-06-25 22:22: Train Epoch 72: 0/528 Loss: 19.289263
2022-06-25 22:23: Train Epoch 72: 20/528 Loss: 19.154861
2022-06-25 22:24: Train Epoch 72: 40/528 Loss: 18.232431
2022-06-25 22:25: Train Epoch 72: 60/528 Loss: 19.782732
2022-06-25 22:27: Train Epoch 72: 80/528 Loss: 19.766167
2022-06-25 22:28: Train Epoch 72: 100/528 Loss: 20.726027
2022-06-25 22:29: Train Epoch 72: 120/528 Loss: 21.938480
2022-06-25 22:30: Train Epoch 72: 140/528 Loss: 18.560724
2022-06-25 22:31: Train Epoch 72: 160/528 Loss: 19.035418
2022-06-25 22:32: Train Epoch 72: 180/528 Loss: 24.721083
2022-06-25 22:33: Train Epoch 72: 200/528 Loss: 22.943493
2022-06-25 22:35: Train Epoch 72: 220/528 Loss: 23.230541
2022-06-25 22:36: Train Epoch 72: 240/528 Loss: 22.184952
2022-06-25 22:37: Train Epoch 72: 260/528 Loss: 21.599260
2022-06-25 22:38: Train Epoch 72: 280/528 Loss: 21.639700
2022-06-25 22:39: Train Epoch 72: 300/528 Loss: 20.449745
2022-06-25 22:40: Train Epoch 72: 320/528 Loss: 19.996693
2022-06-25 22:41: Train Epoch 72: 340/528 Loss: 19.335512
2022-06-25 22:43: Train Epoch 72: 360/528 Loss: 20.416798
2022-06-25 22:44: Train Epoch 72: 380/528 Loss: 21.187969
2022-06-25 22:45: Train Epoch 72: 400/528 Loss: 19.950268
2022-06-25 22:47: Train Epoch 72: 420/528 Loss: 19.198530
2022-06-25 22:48: Train Epoch 72: 440/528 Loss: 17.958708
2022-06-25 22:49: Train Epoch 72: 460/528 Loss: 19.898008
2022-06-25 22:50: Train Epoch 72: 480/528 Loss: 19.170490
2022-06-25 22:51: Train Epoch 72: 500/528 Loss: 19.368273
2022-06-25 22:52: Train Epoch 72: 520/528 Loss: 19.991020
2022-06-25 22:53: **********Train Epoch 72: averaged Loss: 20.712505
2022-06-25 22:56: **********Val Epoch 72: average Loss: 20.796006
2022-06-25 22:56: Train Epoch 73: 0/528 Loss: 19.438881
2022-06-25 22:57: Train Epoch 73: 20/528 Loss: 20.817076
2022-06-25 22:58: Train Epoch 73: 40/528 Loss: 20.272615
2022-06-25 22:59: Train Epoch 73: 60/528 Loss: 23.384899
2022-06-25 23:01: Train Epoch 73: 80/528 Loss: 20.691624
2022-06-25 23:02: Train Epoch 73: 100/528 Loss: 22.061213
2022-06-25 23:03: Train Epoch 73: 120/528 Loss: 19.361773
2022-06-25 23:04: Train Epoch 73: 140/528 Loss: 20.155399
2022-06-25 23:06: Train Epoch 73: 160/528 Loss: 20.158710
2022-06-25 23:07: Train Epoch 73: 180/528 Loss: 19.439960
2022-06-25 23:08: Train Epoch 73: 200/528 Loss: 19.125425
2022-06-25 23:09: Train Epoch 73: 220/528 Loss: 20.590057
2022-06-25 23:10: Train Epoch 73: 240/528 Loss: 20.830418
2022-06-25 23:11: Train Epoch 73: 260/528 Loss: 19.636684
2022-06-25 23:13: Train Epoch 73: 280/528 Loss: 20.216284
2022-06-25 23:14: Train Epoch 73: 300/528 Loss: 21.316643
2022-06-25 23:15: Train Epoch 73: 320/528 Loss: 20.709219
2022-06-25 23:16: Train Epoch 73: 340/528 Loss: 19.607994
2022-06-25 23:17: Train Epoch 73: 360/528 Loss: 20.527170
2022-06-25 23:19: Train Epoch 73: 380/528 Loss: 21.314175
2022-06-25 23:20: Train Epoch 73: 400/528 Loss: 19.815397
2022-06-25 23:21: Train Epoch 73: 420/528 Loss: 21.040617
2022-06-25 23:22: Train Epoch 73: 440/528 Loss: 19.257584
2022-06-25 23:23: Train Epoch 73: 460/528 Loss: 21.302670
2022-06-25 23:25: Train Epoch 73: 480/528 Loss: 20.150522
2022-06-25 23:26: Train Epoch 73: 500/528 Loss: 20.066084
2022-06-25 23:27: Train Epoch 73: 520/528 Loss: 21.153708
2022-06-25 23:27: **********Train Epoch 73: averaged Loss: 20.607951
2022-06-25 23:31: **********Val Epoch 73: average Loss: 20.606736
2022-06-25 23:31: Train Epoch 74: 0/528 Loss: 20.547760
2022-06-25 23:32: Train Epoch 74: 20/528 Loss: 20.476660
2022-06-25 23:33: Train Epoch 74: 40/528 Loss: 20.027573
2022-06-25 23:34: Train Epoch 74: 60/528 Loss: 20.705626
2022-06-25 23:35: Train Epoch 74: 80/528 Loss: 29.076103
2022-06-25 23:37: Train Epoch 74: 100/528 Loss: 23.245632
2022-06-25 23:38: Train Epoch 74: 120/528 Loss: 23.043146
2022-06-25 23:39: Train Epoch 74: 140/528 Loss: 21.799980
2022-06-25 23:40: Train Epoch 74: 160/528 Loss: 21.753918
2022-06-25 23:41: Train Epoch 74: 180/528 Loss: 21.798950
2022-06-25 23:43: Train Epoch 74: 200/528 Loss: 21.599844
2022-06-25 23:44: Train Epoch 74: 220/528 Loss: 21.365654
2022-06-25 23:45: Train Epoch 74: 240/528 Loss: 20.713165
2022-06-25 23:46: Train Epoch 74: 260/528 Loss: 22.832531
2022-06-25 23:47: Train Epoch 74: 280/528 Loss: 21.255669
2022-06-25 23:48: Train Epoch 74: 300/528 Loss: 19.685055
2022-06-25 23:49: Train Epoch 74: 320/528 Loss: 22.268860
2022-06-25 23:50: Train Epoch 74: 340/528 Loss: 21.487085
2022-06-25 23:51: Train Epoch 74: 360/528 Loss: 19.515741
2022-06-25 23:52: Train Epoch 74: 380/528 Loss: 19.491526
2022-06-25 23:54: Train Epoch 74: 400/528 Loss: 20.938864
2022-06-25 23:55: Train Epoch 74: 420/528 Loss: 20.159937
2022-06-25 23:56: Train Epoch 74: 440/528 Loss: 18.913803
2022-06-25 23:57: Train Epoch 74: 460/528 Loss: 21.543169
2022-06-25 23:58: Train Epoch 74: 480/528 Loss: 20.359068
2022-06-25 23:59: Train Epoch 74: 500/528 Loss: 19.088020
2022-06-26 00:00: Train Epoch 74: 520/528 Loss: 18.565638
2022-06-26 00:01: **********Train Epoch 74: averaged Loss: 21.538811
2022-06-26 00:04: **********Val Epoch 74: average Loss: 20.903949
2022-06-26 00:04: Train Epoch 75: 0/528 Loss: 19.290222
2022-06-26 00:05: Train Epoch 75: 20/528 Loss: 21.691977
2022-06-26 00:06: Train Epoch 75: 40/528 Loss: 20.753656
2022-06-26 00:07: Train Epoch 75: 60/528 Loss: 19.492140
2022-06-26 00:08: Train Epoch 75: 80/528 Loss: 19.562790
2022-06-26 00:09: Train Epoch 75: 100/528 Loss: 19.483152
2022-06-26 00:10: Train Epoch 75: 120/528 Loss: 20.693207
2022-06-26 00:11: Train Epoch 75: 140/528 Loss: 21.825165
2022-06-26 00:12: Train Epoch 75: 160/528 Loss: 21.130812
2022-06-26 00:13: Train Epoch 75: 180/528 Loss: 17.721632
2022-06-26 00:14: Train Epoch 75: 200/528 Loss: 19.524529
2022-06-26 00:15: Train Epoch 75: 220/528 Loss: 20.884970
2022-06-26 00:17: Train Epoch 75: 240/528 Loss: 18.850887
2022-06-26 00:18: Train Epoch 75: 260/528 Loss: 19.983685
2022-06-26 00:19: Train Epoch 75: 280/528 Loss: 19.832212
2022-06-26 00:20: Train Epoch 75: 300/528 Loss: 18.853235
2022-06-26 00:21: Train Epoch 75: 320/528 Loss: 19.211092
2022-06-26 00:22: Train Epoch 75: 340/528 Loss: 20.118582
2022-06-26 00:23: Train Epoch 75: 360/528 Loss: 20.555681
2022-06-26 00:24: Train Epoch 75: 380/528 Loss: 20.953253
2022-06-26 00:25: Train Epoch 75: 400/528 Loss: 19.680664
2022-06-26 00:26: Train Epoch 75: 420/528 Loss: 18.710321
2022-06-26 00:27: Train Epoch 75: 440/528 Loss: 19.019205
2022-06-26 00:28: Train Epoch 75: 460/528 Loss: 20.730877
2022-06-26 00:29: Train Epoch 75: 480/528 Loss: 19.127159
2022-06-26 00:30: Train Epoch 75: 500/528 Loss: 20.061453
2022-06-26 00:31: Train Epoch 75: 520/528 Loss: 21.337276
2022-06-26 00:31: **********Train Epoch 75: averaged Loss: 20.261022
2022-06-26 00:35: **********Val Epoch 75: average Loss: 20.674743
2022-06-26 00:35: Train Epoch 76: 0/528 Loss: 19.060270
2022-06-26 00:36: Train Epoch 76: 20/528 Loss: 20.723846
2022-06-26 00:37: Train Epoch 76: 40/528 Loss: 20.860764
2022-06-26 00:38: Train Epoch 76: 60/528 Loss: 21.579788
2022-06-26 00:39: Train Epoch 76: 80/528 Loss: 17.679331
2022-06-26 00:40: Train Epoch 76: 100/528 Loss: 21.673004
2022-06-26 00:41: Train Epoch 76: 120/528 Loss: 17.935583
2022-06-26 00:42: Train Epoch 76: 140/528 Loss: 20.397692
2022-06-26 00:43: Train Epoch 76: 160/528 Loss: 18.605749
2022-06-26 00:44: Train Epoch 76: 180/528 Loss: 19.759762
2022-06-26 00:45: Train Epoch 76: 200/528 Loss: 20.296478
2022-06-26 00:46: Train Epoch 76: 220/528 Loss: 19.494831
2022-06-26 00:47: Train Epoch 76: 240/528 Loss: 20.503622
2022-06-26 00:48: Train Epoch 76: 260/528 Loss: 20.137768
2022-06-26 00:49: Train Epoch 76: 280/528 Loss: 19.477770
2022-06-26 00:50: Train Epoch 76: 300/528 Loss: 20.206036
2022-06-26 00:51: Train Epoch 76: 320/528 Loss: 18.839657
2022-06-26 00:52: Train Epoch 76: 340/528 Loss: 17.948654
2022-06-26 00:53: Train Epoch 76: 360/528 Loss: 19.430353
2022-06-26 00:54: Train Epoch 76: 380/528 Loss: 19.099583
2022-06-26 00:55: Train Epoch 76: 400/528 Loss: 17.483269
2022-06-26 00:56: Train Epoch 76: 420/528 Loss: 19.434847
2022-06-26 00:57: Train Epoch 76: 440/528 Loss: 20.279982
2022-06-26 00:58: Train Epoch 76: 460/528 Loss: 21.255884
2022-06-26 00:59: Train Epoch 76: 480/528 Loss: 20.875603
2022-06-26 01:00: Train Epoch 76: 500/528 Loss: 19.855394
2022-06-26 01:02: Train Epoch 76: 520/528 Loss: 18.884529
2022-06-26 01:02: **********Train Epoch 76: averaged Loss: 19.846093
2022-06-26 01:05: **********Val Epoch 76: average Loss: 20.559711
2022-06-26 01:05: Train Epoch 77: 0/528 Loss: 19.403982
2022-06-26 01:06: Train Epoch 77: 20/528 Loss: 19.231827
2022-06-26 01:07: Train Epoch 77: 40/528 Loss: 19.196552
2022-06-26 01:08: Train Epoch 77: 60/528 Loss: 19.411615
2022-06-26 01:09: Train Epoch 77: 80/528 Loss: 18.534285
2022-06-26 01:10: Train Epoch 77: 100/528 Loss: 19.027107
2022-06-26 01:11: Train Epoch 77: 120/528 Loss: 19.899574
2022-06-26 01:12: Train Epoch 77: 140/528 Loss: 20.486219
2022-06-26 01:13: Train Epoch 77: 160/528 Loss: 19.209225
2022-06-26 01:14: Train Epoch 77: 180/528 Loss: 20.469328
2022-06-26 01:15: Train Epoch 77: 200/528 Loss: 20.922993
2022-06-26 01:16: Train Epoch 77: 220/528 Loss: 20.899273
2022-06-26 01:18: Train Epoch 77: 240/528 Loss: 20.518606
2022-06-26 01:19: Train Epoch 77: 260/528 Loss: 19.810101
2022-06-26 01:20: Train Epoch 77: 280/528 Loss: 19.274164
2022-06-26 01:21: Train Epoch 77: 300/528 Loss: 19.955223
2022-06-26 01:22: Train Epoch 77: 320/528 Loss: 21.606482
2022-06-26 01:23: Train Epoch 77: 340/528 Loss: 20.174770
2022-06-26 01:24: Train Epoch 77: 360/528 Loss: 20.796225
2022-06-26 01:25: Train Epoch 77: 380/528 Loss: 19.226643
2022-06-26 01:26: Train Epoch 77: 400/528 Loss: 20.628929
2022-06-26 01:27: Train Epoch 77: 420/528 Loss: 19.103249
2022-06-26 01:28: Train Epoch 77: 440/528 Loss: 22.169018
2022-06-26 01:29: Train Epoch 77: 460/528 Loss: 19.654669
2022-06-26 01:30: Train Epoch 77: 480/528 Loss: 20.988199
2022-06-26 01:31: Train Epoch 77: 500/528 Loss: 18.640917
2022-06-26 01:32: Train Epoch 77: 520/528 Loss: 19.792290
2022-06-26 01:32: **********Train Epoch 77: averaged Loss: 19.798484
2022-06-26 01:36: **********Val Epoch 77: average Loss: 21.317677
2022-06-26 01:36: Train Epoch 78: 0/528 Loss: 21.094629
2022-06-26 01:37: Train Epoch 78: 20/528 Loss: 20.623581
2022-06-26 01:38: Train Epoch 78: 40/528 Loss: 22.072880
2022-06-26 01:39: Train Epoch 78: 60/528 Loss: 19.847258
2022-06-26 01:40: Train Epoch 78: 80/528 Loss: 19.790552
2022-06-26 01:41: Train Epoch 78: 100/528 Loss: 19.674961
2022-06-26 01:42: Train Epoch 78: 120/528 Loss: 18.959723
2022-06-26 01:43: Train Epoch 78: 140/528 Loss: 19.069679
2022-06-26 01:44: Train Epoch 78: 160/528 Loss: 19.926592
2022-06-26 01:45: Train Epoch 78: 180/528 Loss: 19.577753
2022-06-26 01:46: Train Epoch 78: 200/528 Loss: 16.738379
2022-06-26 01:47: Train Epoch 78: 220/528 Loss: 19.303291
2022-06-26 01:48: Train Epoch 78: 240/528 Loss: 19.307806
2022-06-26 01:49: Train Epoch 78: 260/528 Loss: 20.499905
2022-06-26 01:50: Train Epoch 78: 280/528 Loss: 20.212910
2022-06-26 01:51: Train Epoch 78: 300/528 Loss: 19.663782
2022-06-26 01:52: Train Epoch 78: 320/528 Loss: 21.524752
2022-06-26 01:53: Train Epoch 78: 340/528 Loss: 19.813955
2022-06-26 01:54: Train Epoch 78: 360/528 Loss: 19.912952
2022-06-26 01:55: Train Epoch 78: 380/528 Loss: 20.305328
2022-06-26 01:56: Train Epoch 78: 400/528 Loss: 20.950914
2022-06-26 01:58: Train Epoch 78: 420/528 Loss: 20.109325
2022-06-26 01:59: Train Epoch 78: 440/528 Loss: 20.460058
2022-06-26 01:59: Train Epoch 78: 460/528 Loss: 18.700630
2022-06-26 02:01: Train Epoch 78: 480/528 Loss: 19.907848
2022-06-26 02:02: Train Epoch 78: 500/528 Loss: 21.167700
2022-06-26 02:02: Train Epoch 78: 520/528 Loss: 21.018940
2022-06-26 02:03: **********Train Epoch 78: averaged Loss: 19.805956
2022-06-26 02:06: **********Val Epoch 78: average Loss: 20.555083
2022-06-26 02:06: Train Epoch 79: 0/528 Loss: 18.392422
2022-06-26 02:07: Train Epoch 79: 20/528 Loss: 18.672108
2022-06-26 02:08: Train Epoch 79: 40/528 Loss: 19.009426
2022-06-26 02:09: Train Epoch 79: 60/528 Loss: 19.094267
2022-06-26 02:10: Train Epoch 79: 80/528 Loss: 20.019894
2022-06-26 02:11: Train Epoch 79: 100/528 Loss: 19.002048
2022-06-26 02:12: Train Epoch 79: 120/528 Loss: 21.746996
2022-06-26 02:13: Train Epoch 79: 140/528 Loss: 21.309500
2022-06-26 02:14: Train Epoch 79: 160/528 Loss: 19.721443
2022-06-26 02:15: Train Epoch 79: 180/528 Loss: 19.527290
2022-06-26 02:16: Train Epoch 79: 200/528 Loss: 18.254551
2022-06-26 02:17: Train Epoch 79: 220/528 Loss: 19.956694
2022-06-26 02:18: Train Epoch 79: 240/528 Loss: 18.466377
2022-06-26 02:19: Train Epoch 79: 260/528 Loss: 20.158270
2022-06-26 02:20: Train Epoch 79: 280/528 Loss: 19.552229
2022-06-26 02:21: Train Epoch 79: 300/528 Loss: 20.107748
2022-06-26 02:22: Train Epoch 79: 320/528 Loss: 21.097565
2022-06-26 02:23: Train Epoch 79: 340/528 Loss: 19.126814
2022-06-26 02:24: Train Epoch 79: 360/528 Loss: 22.037113
2022-06-26 02:25: Train Epoch 79: 380/528 Loss: 17.865412
2022-06-26 02:26: Train Epoch 79: 400/528 Loss: 46.321548
2022-06-26 02:27: Train Epoch 79: 420/528 Loss: 33.288368
2022-06-26 02:28: Train Epoch 79: 440/528 Loss: 25.393080
2022-06-26 02:29: Train Epoch 79: 460/528 Loss: 23.697958
2022-06-26 02:30: Train Epoch 79: 480/528 Loss: 22.464552
2022-06-26 02:31: Train Epoch 79: 500/528 Loss: 24.845591
2022-06-26 02:32: Train Epoch 79: 520/528 Loss: 20.815475
2022-06-26 02:32: **********Train Epoch 79: averaged Loss: 21.444578
2022-06-26 02:35: **********Val Epoch 79: average Loss: 22.494676
2022-06-26 02:35: Train Epoch 80: 0/528 Loss: 21.746853
2022-06-26 02:37: Train Epoch 80: 20/528 Loss: 21.846638
2022-06-26 02:38: Train Epoch 80: 40/528 Loss: 21.867460
2022-06-26 02:39: Train Epoch 80: 60/528 Loss: 20.380178
2022-06-26 02:40: Train Epoch 80: 80/528 Loss: 22.940874
2022-06-26 02:41: Train Epoch 80: 100/528 Loss: 20.692532
2022-06-26 02:42: Train Epoch 80: 120/528 Loss: 21.966583
2022-06-26 02:43: Train Epoch 80: 140/528 Loss: 21.772051
2022-06-26 02:44: Train Epoch 80: 160/528 Loss: 19.360439
2022-06-26 02:45: Train Epoch 80: 180/528 Loss: 22.339985
2022-06-26 02:46: Train Epoch 80: 200/528 Loss: 20.645046
2022-06-26 02:47: Train Epoch 80: 220/528 Loss: 21.746014
2022-06-26 02:48: Train Epoch 80: 240/528 Loss: 22.941002
2022-06-26 02:49: Train Epoch 80: 260/528 Loss: 21.730255
2022-06-26 02:50: Train Epoch 80: 280/528 Loss: 22.162134
2022-06-26 02:51: Train Epoch 80: 300/528 Loss: 21.026596
2022-06-26 02:52: Train Epoch 80: 320/528 Loss: 21.265331
2022-06-26 02:53: Train Epoch 80: 340/528 Loss: 21.760963
2022-06-26 02:54: Train Epoch 80: 360/528 Loss: 19.421984
2022-06-26 02:55: Train Epoch 80: 380/528 Loss: 20.842943
2022-06-26 02:56: Train Epoch 80: 400/528 Loss: 20.300297
2022-06-26 02:57: Train Epoch 80: 420/528 Loss: 20.948959
2022-06-26 02:58: Train Epoch 80: 440/528 Loss: 22.053934
2022-06-26 02:59: Train Epoch 80: 460/528 Loss: 21.274256
2022-06-26 03:00: Train Epoch 80: 480/528 Loss: 21.354467
2022-06-26 03:01: Train Epoch 80: 500/528 Loss: 21.815718
2022-06-26 03:02: Train Epoch 80: 520/528 Loss: 21.857775
2022-06-26 03:02: **********Train Epoch 80: averaged Loss: 20.933236
2022-06-26 03:05: **********Val Epoch 80: average Loss: 21.019878
2022-06-26 03:05: Train Epoch 81: 0/528 Loss: 20.547794
2022-06-26 03:06: Train Epoch 81: 20/528 Loss: 21.080650
2022-06-26 03:07: Train Epoch 81: 40/528 Loss: 21.828724
2022-06-26 03:08: Train Epoch 81: 60/528 Loss: 20.164581
2022-06-26 03:09: Train Epoch 81: 80/528 Loss: 20.938152
2022-06-26 03:10: Train Epoch 81: 100/528 Loss: 18.892914
2022-06-26 03:11: Train Epoch 81: 120/528 Loss: 19.893713
2022-06-26 03:12: Train Epoch 81: 140/528 Loss: 20.225189
2022-06-26 03:13: Train Epoch 81: 160/528 Loss: 20.304901
2022-06-26 03:14: Train Epoch 81: 180/528 Loss: 20.490545
2022-06-26 03:15: Train Epoch 81: 200/528 Loss: 21.763649
2022-06-26 03:16: Train Epoch 81: 220/528 Loss: 19.728882
2022-06-26 03:17: Train Epoch 81: 240/528 Loss: 19.506910
2022-06-26 03:18: Train Epoch 81: 260/528 Loss: 20.237600
2022-06-26 03:19: Train Epoch 81: 280/528 Loss: 18.309313
2022-06-26 03:19: Train Epoch 81: 300/528 Loss: 20.452366
2022-06-26 03:20: Train Epoch 81: 320/528 Loss: 18.488241
2022-06-26 03:21: Train Epoch 81: 340/528 Loss: 19.946039
2022-06-26 03:22: Train Epoch 81: 360/528 Loss: 18.993996
2022-06-26 03:23: Train Epoch 81: 380/528 Loss: 20.064701
2022-06-26 03:24: Train Epoch 81: 400/528 Loss: 19.582958
2022-06-26 03:25: Train Epoch 81: 420/528 Loss: 19.951571
2022-06-26 03:26: Train Epoch 81: 440/528 Loss: 20.629290
2022-06-26 03:27: Train Epoch 81: 460/528 Loss: 20.379433
2022-06-26 03:28: Train Epoch 81: 480/528 Loss: 18.980043
2022-06-26 03:29: Train Epoch 81: 500/528 Loss: 19.968807
2022-06-26 03:30: Train Epoch 81: 520/528 Loss: 20.595516
2022-06-26 03:31: **********Train Epoch 81: averaged Loss: 20.148561
2022-06-26 03:33: **********Val Epoch 81: average Loss: 20.991099
2022-06-26 03:33: Train Epoch 82: 0/528 Loss: 20.403818
2022-06-26 03:34: Train Epoch 82: 20/528 Loss: 19.404938
2022-06-26 03:35: Train Epoch 82: 40/528 Loss: 20.865252
2022-06-26 03:36: Train Epoch 82: 60/528 Loss: 20.250628
2022-06-26 03:37: Train Epoch 82: 80/528 Loss: 18.818764
2022-06-26 03:38: Train Epoch 82: 100/528 Loss: 21.128340
2022-06-26 03:39: Train Epoch 82: 120/528 Loss: 20.364536
2022-06-26 03:40: Train Epoch 82: 140/528 Loss: 18.963274
2022-06-26 03:41: Train Epoch 82: 160/528 Loss: 20.212490
2022-06-26 03:42: Train Epoch 82: 180/528 Loss: 20.872553
2022-06-26 03:43: Train Epoch 82: 200/528 Loss: 19.466892
2022-06-26 03:44: Train Epoch 82: 220/528 Loss: 19.265316
2022-06-26 03:45: Train Epoch 82: 240/528 Loss: 21.031839
2022-06-26 03:46: Train Epoch 82: 260/528 Loss: 18.610210
2022-06-26 03:47: Train Epoch 82: 280/528 Loss: 19.496078
2022-06-26 03:48: Train Epoch 82: 300/528 Loss: 20.199074
2022-06-26 03:49: Train Epoch 82: 320/528 Loss: 17.835445
2022-06-26 03:50: Train Epoch 82: 340/528 Loss: 18.294771
2022-06-26 03:51: Train Epoch 82: 360/528 Loss: 18.759745
2022-06-26 03:51: Train Epoch 82: 380/528 Loss: 19.638874
2022-06-26 03:52: Train Epoch 82: 400/528 Loss: 20.820402
2022-06-26 03:53: Train Epoch 82: 420/528 Loss: 19.818962
2022-06-26 03:54: Train Epoch 82: 440/528 Loss: 19.652987
2022-06-26 03:55: Train Epoch 82: 460/528 Loss: 20.775738
2022-06-26 03:56: Train Epoch 82: 480/528 Loss: 18.855513
2022-06-26 03:57: Train Epoch 82: 500/528 Loss: 20.117956
2022-06-26 03:58: Train Epoch 82: 520/528 Loss: 21.415228
2022-06-26 03:58: **********Train Epoch 82: averaged Loss: 19.929300
2022-06-26 04:01: **********Val Epoch 82: average Loss: 20.577770
2022-06-26 04:01: Train Epoch 83: 0/528 Loss: 20.358042
2022-06-26 04:02: Train Epoch 83: 20/528 Loss: 21.591391
2022-06-26 04:03: Train Epoch 83: 40/528 Loss: 21.149561
2022-06-26 04:04: Train Epoch 83: 60/528 Loss: 17.757496
2022-06-26 04:05: Train Epoch 83: 80/528 Loss: 18.694668
2022-06-26 04:06: Train Epoch 83: 100/528 Loss: 18.439081
2022-06-26 04:07: Train Epoch 83: 120/528 Loss: 18.464777
2022-06-26 04:08: Train Epoch 83: 140/528 Loss: 19.077728
2022-06-26 04:09: Train Epoch 83: 160/528 Loss: 19.737099
2022-06-26 04:10: Train Epoch 83: 180/528 Loss: 18.976601
2022-06-26 04:11: Train Epoch 83: 200/528 Loss: 18.835833
2022-06-26 04:12: Train Epoch 83: 220/528 Loss: 18.941257
2022-06-26 04:13: Train Epoch 83: 240/528 Loss: 20.161261
2022-06-26 04:14: Train Epoch 83: 260/528 Loss: 18.366739
2022-06-26 04:15: Train Epoch 83: 280/528 Loss: 20.094284
2022-06-26 04:16: Train Epoch 83: 300/528 Loss: 20.460508
2022-06-26 04:17: Train Epoch 83: 320/528 Loss: 20.145082
2022-06-26 04:18: Train Epoch 83: 340/528 Loss: 17.271448
2022-06-26 04:19: Train Epoch 83: 360/528 Loss: 20.492393
2022-06-26 04:20: Train Epoch 83: 380/528 Loss: 19.674181
2022-06-26 04:21: Train Epoch 83: 400/528 Loss: 20.094398
2022-06-26 04:22: Train Epoch 83: 420/528 Loss: 18.691759
2022-06-26 04:23: Train Epoch 83: 440/528 Loss: 21.806208
2022-06-26 04:24: Train Epoch 83: 460/528 Loss: 19.944752
2022-06-26 04:25: Train Epoch 83: 480/528 Loss: 20.035812
2022-06-26 04:26: Train Epoch 83: 500/528 Loss: 17.904840
2022-06-26 04:27: Train Epoch 83: 520/528 Loss: 20.302553
2022-06-26 04:27: **********Train Epoch 83: averaged Loss: 19.773590
2022-06-26 04:30: **********Val Epoch 83: average Loss: 20.683019
2022-06-26 04:30: Train Epoch 84: 0/528 Loss: 20.841902
2022-06-26 04:31: Train Epoch 84: 20/528 Loss: 19.986557
2022-06-26 04:32: Train Epoch 84: 40/528 Loss: 19.175053
2022-06-26 04:33: Train Epoch 84: 60/528 Loss: 21.579449
2022-06-26 04:34: Train Epoch 84: 80/528 Loss: 19.984840
2022-06-26 04:35: Train Epoch 84: 100/528 Loss: 17.195911
2022-06-26 04:36: Train Epoch 84: 120/528 Loss: 18.904898
2022-06-26 04:37: Train Epoch 84: 140/528 Loss: 20.108553
2022-06-26 04:38: Train Epoch 84: 160/528 Loss: 18.589870
2022-06-26 04:39: Train Epoch 84: 180/528 Loss: 18.630472
2022-06-26 04:40: Train Epoch 84: 200/528 Loss: 20.414913
2022-06-26 04:41: Train Epoch 84: 220/528 Loss: 18.931244
2022-06-26 04:42: Train Epoch 84: 240/528 Loss: 20.968164
2022-06-26 04:43: Train Epoch 84: 260/528 Loss: 19.709572
2022-06-26 04:44: Train Epoch 84: 280/528 Loss: 19.082832
2022-06-26 04:44: Train Epoch 84: 300/528 Loss: 20.128685
2022-06-26 04:45: Train Epoch 84: 320/528 Loss: 18.438353
2022-06-26 04:46: Train Epoch 84: 340/528 Loss: 19.560595
2022-06-26 04:47: Train Epoch 84: 360/528 Loss: 20.764095
2022-06-26 04:48: Train Epoch 84: 380/528 Loss: 20.111986
2022-06-26 04:49: Train Epoch 84: 400/528 Loss: 21.250360
2022-06-26 04:50: Train Epoch 84: 420/528 Loss: 20.520090
2022-06-26 04:51: Train Epoch 84: 440/528 Loss: 20.862762
2022-06-26 04:52: Train Epoch 84: 460/528 Loss: 19.040882
2022-06-26 04:53: Train Epoch 84: 480/528 Loss: 21.287516
2022-06-26 04:54: Train Epoch 84: 500/528 Loss: 17.566765
2022-06-26 04:55: Train Epoch 84: 520/528 Loss: 19.373569
2022-06-26 04:55: **********Train Epoch 84: averaged Loss: 19.751314
2022-06-26 04:58: **********Val Epoch 84: average Loss: 20.753902
2022-06-26 04:58: Validation performance didn't improve for 15 epochs. Training stops.
2022-06-26 04:58: Total training time: 2467.4859min, best loss: 20.370327
2022-06-26 04:58: Saving current best model to ../runs/PEMSD7/06-24-11h51m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/best_model.pth
2022-06-26 05:01: Horizon 01, MAE: 17.66, RMSE: 28.14, MAPE: 7.7858%
2022-06-26 05:01: Horizon 02, MAE: 18.65, RMSE: 30.12, MAPE: 8.0126%
2022-06-26 05:01: Horizon 03, MAE: 19.33, RMSE: 31.36, MAPE: 8.2356%
2022-06-26 05:01: Horizon 04, MAE: 19.77, RMSE: 32.31, MAPE: 8.3997%
2022-06-26 05:01: Horizon 05, MAE: 20.21, RMSE: 33.16, MAPE: 8.5945%
2022-06-26 05:01: Horizon 06, MAE: 20.62, RMSE: 33.85, MAPE: 8.8051%
2022-06-26 05:01: Horizon 07, MAE: 21.04, RMSE: 34.51, MAPE: 8.9892%
2022-06-26 05:01: Horizon 08, MAE: 21.38, RMSE: 35.12, MAPE: 9.2223%
2022-06-26 05:01: Horizon 09, MAE: 21.66, RMSE: 35.58, MAPE: 9.3694%
2022-06-26 05:01: Horizon 10, MAE: 21.95, RMSE: 36.06, MAPE: 9.5459%
2022-06-26 05:01: Horizon 11, MAE: 22.34, RMSE: 36.65, MAPE: 9.7722%
2022-06-26 05:01: Horizon 12, MAE: 22.84, RMSE: 37.30, MAPE: 10.0451%
2022-06-26 05:01: Average Horizon, MAE: 20.62, RMSE: 33.78, MAPE: 8.8981%
/home/assassin/project/STG-NCDE-main
Namespace(dataset='PEMSD7', mode='train', device=1, debug=False, model='GCDE', cuda=True, comment='', val_ratio=0.2, test_ratio=0.2, lag=12, horizon=12, num_nodes=883, tod=False, normalizer='std', column_wise=False, default_graph=True, model_type='type1', g_type='agc', input_dim=2, output_dim=1, embed_dim=10, hid_dim=64, hid_hid_dim=64, num_layers=2, cheb_k=2, solver='rk4', loss_func='mae', seed=0, batch_size=32, epochs=200, lr_init=0.001, weight_decay=0.001, lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', early_stop=True, early_stop_patience=15, grad_norm=False, max_grad_norm=5, teacher_forcing=False, real_value=True, missing_test=False, missing_rate=0.1, mae_thresh=None, mape_thresh=0.0, model_path='', log_dir='../runs', log_step=20, plot=False, tensorboard=True)
NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
*****************Model Parameter*****************
node_embeddings torch.Size([883, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([883, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 388424
*****************Finish Parameter****************
Load PEMSD7 Dataset shaped:  (28224, 883, 1) 1498.0 0.0 308.52346223738647 304.0
Normalize the dataset by Standard Normalization
Train:  (16912, 12, 883, 1) (16912, 12, 883, 1)
Val:  (5622, 12, 883, 1) (5622, 12, 883, 1)
Test:  (5621, 12, 883, 1) (5621, 12, 883, 1)
Creat Log File in:  ../runs/PEMSD7/06-24-11h51m_PEMSD7_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/run.log
*****************Model Parameter*****************
node_embeddings torch.Size([883, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([883, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 388424
*****************Finish Parameter****************
epoch_time 1335.7848763465881
epoch_time 1444.1579427719116
epoch_time 1456.7761597633362
epoch_time 1457.7369508743286
epoch_time 1452.7413532733917
epoch_time 1447.1416718959808
epoch_time 1441.0521845817566
epoch_time 1440.5361621379852
epoch_time 1445.8103368282318
epoch_time 1425.7074162960052
epoch_time 1434.7442166805267
epoch_time 1437.4084405899048
epoch_time 1445.4135415554047
epoch_time 1420.1128537654877
epoch_time 1414.0156061649323
epoch_time 1412.3787031173706
epoch_time 1416.387764453888
epoch_time 1404.9587876796722
epoch_time 1399.4058547019958
epoch_time 1399.4568254947662
epoch_time 1408.3513326644897
epoch_time 1390.8198704719543
epoch_time 1418.0647876262665
epoch_time 1418.5002274513245
epoch_time 1388.8985755443573
epoch_time 1310.287523508072
epoch_time 1305.1511554718018
epoch_time 1314.4018161296844
epoch_time 1290.4109289646149
epoch_time 1293.010568857193
epoch_time 1296.0527861118317
epoch_time 1414.9230618476868
epoch_time 1482.5892083644867
epoch_time 1470.0819699764252
epoch_time 1466.5754351615906
epoch_time 1531.746514081955
epoch_time 1530.4977548122406
epoch_time 1549.5961837768555
epoch_time 1510.6592764854431
epoch_time 1550.2027881145477
epoch_time 1516.4641199111938
epoch_time 1552.742439031601
epoch_time 1371.624965429306
epoch_time 1038.1037318706512
epoch_time 1551.7883441448212
epoch_time 1505.6972041130066
epoch_time 1539.2353637218475
epoch_time 1524.7581388950348
epoch_time 1722.4936881065369
epoch_time 1862.698893070221
epoch_time 1858.4672017097473
epoch_time 1856.4382512569427
epoch_time 1832.7822167873383
epoch_time 1873.7983002662659
epoch_time 1878.1403903961182
epoch_time 1914.6044228076935
epoch_time 1918.063499212265
epoch_time 1903.3569226264954
epoch_time 1907.178674697876
epoch_time 1909.6184310913086
epoch_time 1902.7283346652985
epoch_time 1886.823499917984
epoch_time 1858.4977402687073
epoch_time 1894.7577784061432
epoch_time 1917.300148010254
epoch_time 1916.6207134723663
epoch_time 1874.5094230175018
epoch_time 1876.8258655071259
epoch_time 1921.5520384311676
epoch_time 1920.4622752666473
epoch_time 1840.0346722602844
epoch_time 1867.3542864322662
epoch_time 1880.4183840751648
epoch_time 1799.2796993255615
epoch_time 1653.6092388629913
epoch_time 1637.207582950592
epoch_time 1654.3534276485443
epoch_time 1625.7686157226562
epoch_time 1608.4571776390076
epoch_time 1603.5995922088623
epoch_time 1526.4073646068573
epoch_time 1503.659682750702
epoch_time 1536.609773159027
epoch_time 1526.9870064258575
2023-07-05 13:08:18.877513: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2023-07-05 13:08: Experiment log path in: ../runs/shenzhen/07-05-13h08m_shenzhen_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}
2023-07-05 13:08: Argument batch_size: 32
2023-07-05 13:08: Argument cheb_k: 2
2023-07-05 13:08: Argument column_wise: False
2023-07-05 13:08: Argument comment: ''
2023-07-05 13:08: Argument cuda: True
2023-07-05 13:08: Argument dataset: 'shenzhen'
2023-07-05 13:08: Argument debug: False
2023-07-05 13:08: Argument default_graph: True
2023-07-05 13:08: Argument device: 1
2023-07-05 13:08: Argument early_stop: True
2023-07-05 13:08: Argument early_stop_patience: 15
2023-07-05 13:08: Argument embed_dim: 10
2023-07-05 13:08: Argument epochs: 200
2023-07-05 13:08: Argument g_type: 'agc'
2023-07-05 13:08: Argument grad_norm: False
2023-07-05 13:08: Argument hid_dim: 64
2023-07-05 13:08: Argument hid_hid_dim: 64
2023-07-05 13:08: Argument horizon: 12
2023-07-05 13:08: Argument input_dim: 2
2023-07-05 13:08: Argument lag: 12
2023-07-05 13:08: Argument log_dir: '../runs/shenzhen/07-05-13h08m_shenzhen_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}'
2023-07-05 13:08: Argument log_step: 20
2023-07-05 13:08: Argument loss_func: 'mae'
2023-07-05 13:08: Argument lr_decay: False
2023-07-05 13:08: Argument lr_decay_rate: 0.3
2023-07-05 13:08: Argument lr_decay_step: '5,20,40,70'
2023-07-05 13:08: Argument lr_init: 0.001
2023-07-05 13:08: Argument mae_thresh: None
2023-07-05 13:08: Argument mape_thresh: 0.0
2023-07-05 13:08: Argument max_grad_norm: 5
2023-07-05 13:08: Argument missing_rate: 0.1
2023-07-05 13:08: Argument missing_test: False
2023-07-05 13:08: Argument mode: 'train'
2023-07-05 13:08: Argument model: 'GCDE'
2023-07-05 13:08: Argument model_path: ''
2023-07-05 13:08: Argument model_type: 'type1'
2023-07-05 13:08: Argument normalizer: 'std'
2023-07-05 13:08: Argument num_layers: 2
2023-07-05 13:08: Argument num_nodes: 627
2023-07-05 13:08: Argument output_dim: 1
2023-07-05 13:08: Argument plot: False
2023-07-05 13:08: Argument real_value: True
2023-07-05 13:08: Argument seed: 0
2023-07-05 13:08: Argument solver: 'rk4'
2023-07-05 13:08: Argument teacher_forcing: False
2023-07-05 13:08: Argument tensorboard: True
2023-07-05 13:08: Argument test_ratio: 0.2
2023-07-05 13:08: Argument tod: False
2023-07-05 13:08: Argument val_ratio: 0.2
2023-07-05 13:08: Argument weight_decay: 0.001
2023-07-05 13:08: NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
2023-07-05 13:08: Total params: 383304
2023-07-05 13:08: Train Epoch 1: 0/323 Loss: 34.357147
2023-07-05 13:08: Train Epoch 1: 20/323 Loss: 6.090815
2023-07-05 13:08: Train Epoch 1: 40/323 Loss: 3.115428
2023-07-05 13:09: Train Epoch 1: 60/323 Loss: 2.793664
2023-07-05 13:09: Train Epoch 1: 80/323 Loss: 2.836700
2023-07-05 13:09: Train Epoch 1: 100/323 Loss: 2.775199
2023-07-05 13:09: Train Epoch 1: 120/323 Loss: 2.788908
2023-07-05 13:10: Train Epoch 1: 140/323 Loss: 3.092007
2023-07-05 13:10: Train Epoch 1: 160/323 Loss: 2.871571
2023-07-05 13:10: Train Epoch 1: 180/323 Loss: 2.766607
2023-07-05 13:10:33.591647: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2023-07-05 13:10: Experiment log path in: ../runs/chengdu/07-05-13h10m_chengdu_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}
2023-07-05 13:10: Argument batch_size: 32
2023-07-05 13:10: Argument cheb_k: 2
2023-07-05 13:10: Argument column_wise: False
2023-07-05 13:10: Argument comment: ''
2023-07-05 13:10: Argument cuda: True
2023-07-05 13:10: Argument dataset: 'chengdu'
2023-07-05 13:10: Argument debug: False
2023-07-05 13:10: Argument default_graph: True
2023-07-05 13:10: Argument device: 0
2023-07-05 13:10: Argument early_stop: True
2023-07-05 13:10: Argument early_stop_patience: 15
2023-07-05 13:10: Argument embed_dim: 10
2023-07-05 13:10: Argument epochs: 200
2023-07-05 13:10: Argument g_type: 'agc'
2023-07-05 13:10: Argument grad_norm: False
2023-07-05 13:10: Argument hid_dim: 64
2023-07-05 13:10: Argument hid_hid_dim: 64
2023-07-05 13:10: Argument horizon: 12
2023-07-05 13:10: Argument input_dim: 2
2023-07-05 13:10: Argument lag: 12
2023-07-05 13:10: Argument log_dir: '../runs/chengdu/07-05-13h10m_chengdu_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}'
2023-07-05 13:10: Argument log_step: 20
2023-07-05 13:10: Argument loss_func: 'mae'
2023-07-05 13:10: Argument lr_decay: False
2023-07-05 13:10: Argument lr_decay_rate: 0.3
2023-07-05 13:10: Argument lr_decay_step: '5,20,40,70'
2023-07-05 13:10: Argument lr_init: 0.001
2023-07-05 13:10: Argument mae_thresh: None
2023-07-05 13:10: Argument mape_thresh: 0.0
2023-07-05 13:10: Argument max_grad_norm: 5
2023-07-05 13:10: Argument missing_rate: 0.1
2023-07-05 13:10: Argument missing_test: False
2023-07-05 13:10: Argument mode: 'train'
2023-07-05 13:10: Argument model: 'GCDE'
2023-07-05 13:10: Argument model_path: ''
2023-07-05 13:10: Argument model_type: 'type1'
2023-07-05 13:10: Argument normalizer: 'std'
2023-07-05 13:10: Argument num_layers: 2
2023-07-05 13:10: Argument num_nodes: 524
2023-07-05 13:10: Argument output_dim: 1
2023-07-05 13:10: Argument plot: False
2023-07-05 13:10: Argument real_value: True
2023-07-05 13:10: Argument seed: 0
2023-07-05 13:10: Argument solver: 'rk4'
2023-07-05 13:10: Argument teacher_forcing: False
2023-07-05 13:10: Argument tensorboard: True
2023-07-05 13:10: Argument test_ratio: 0.2
2023-07-05 13:10: Argument tod: False
2023-07-05 13:10: Argument val_ratio: 0.2
2023-07-05 13:10: Argument weight_decay: 0.001
2023-07-05 13:10: NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
2023-07-05 13:10: Total params: 381244
2023-07-05 13:10: Train Epoch 1: 200/323 Loss: 2.717975
2023-07-05 13:10: Train Epoch 1: 0/323 Loss: 31.474377
/home/assassin/TRC/STG-NCDE-main
Namespace(dataset='chengdu', mode='train', device=0, debug=False, model='GCDE', cuda=True, comment='', val_ratio=0.2, test_ratio=0.2, lag=12, horizon=12, num_nodes=524, tod=False, normalizer='std', column_wise=False, default_graph=True, model_type='type1', g_type='agc', input_dim=2, output_dim=1, embed_dim=10, hid_dim=64, hid_hid_dim=64, num_layers=2, cheb_k=2, solver='rk4', loss_func='mae', seed=0, batch_size=32, epochs=200, lr_init=0.001, weight_decay=0.001, lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', early_stop=True, early_stop_patience=15, grad_norm=False, max_grad_norm=5, teacher_forcing=False, real_value=True, missing_test=False, missing_rate=0.1, mae_thresh=None, mape_thresh=0.0, model_path='', log_dir='../runs', log_step=20, plot=False, tensorboard=True)
NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
*****************Model Parameter*****************
node_embeddings torch.Size([524, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([524, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 381244
*****************Finish Parameter****************
Load chengdu Dataset shaped:  (17280, 524, 1) 104.85800000005906 5.000000000039382 29.11107818401029 28.144900000043943
Normalize the dataset by Standard Normalization
Train:  (10345, 12, 524, 1) (10345, 12, 524, 1)
Val:  (3433, 12, 524, 1) (3433, 12, 524, 1)
Test:  (3433, 12, 524, 1) (3433, 12, 524, 1)
Creat Log File in:  ../runs/chengdu/07-05-13h10m_chengdu_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/run.log
*****************Model Parameter*****************
node_embeddings torch.Size([524, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([524, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 381244
*****************Finish Parameter****************
Traceback (most recent call last):
  File "/home/assassin/TRC/STG-NCDE-main/model/Run_cde.py", line 200, in <module>
    trainer.train()
  File "/home/assassin/TRC/STG-NCDE-main/model/BasicTrainer_cde.py", line 131, in train
    train_epoch_loss = self.train_epoch(epoch)
  File "/home/assassin/TRC/STG-NCDE-main/model/BasicTrainer_cde.py", line 99, in train_epoch
    loss.backward()
  File "/home/assassin/anaconda3/lib/python3.9/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/assassin/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
2023-07-05 13:10:56.568063: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2023-07-05 13:11: Train Epoch 1: 220/323 Loss: 2.672884
2023-07-05 13:11: Experiment log path in: ../runs/chengdu/07-05-13h10m_chengdu_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}
2023-07-05 13:11: Argument batch_size: 32
2023-07-05 13:11: Argument cheb_k: 2
2023-07-05 13:11: Argument column_wise: False
2023-07-05 13:11: Argument comment: ''
2023-07-05 13:11: Argument cuda: True
2023-07-05 13:11: Argument dataset: 'chengdu'
2023-07-05 13:11: Argument debug: False
2023-07-05 13:11: Argument default_graph: True
2023-07-05 13:11: Argument device: 1
2023-07-05 13:11: Argument early_stop: True
2023-07-05 13:11: Argument early_stop_patience: 15
2023-07-05 13:11: Argument embed_dim: 10
2023-07-05 13:11: Argument epochs: 200
2023-07-05 13:11: Argument g_type: 'agc'
2023-07-05 13:11: Argument grad_norm: False
2023-07-05 13:11: Argument hid_dim: 64
2023-07-05 13:11: Argument hid_hid_dim: 64
2023-07-05 13:11: Argument horizon: 12
2023-07-05 13:11: Argument input_dim: 2
2023-07-05 13:11: Argument lag: 12
2023-07-05 13:11: Argument log_dir: '../runs/chengdu/07-05-13h10m_chengdu_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}'
2023-07-05 13:11: Argument log_step: 20
2023-07-05 13:11: Argument loss_func: 'mae'
2023-07-05 13:11: Argument lr_decay: False
2023-07-05 13:11: Argument lr_decay_rate: 0.3
2023-07-05 13:11: Argument lr_decay_step: '5,20,40,70'
2023-07-05 13:11: Argument lr_init: 0.001
2023-07-05 13:11: Argument mae_thresh: None
2023-07-05 13:11: Argument mape_thresh: 0.0
2023-07-05 13:11: Argument max_grad_norm: 5
2023-07-05 13:11: Argument missing_rate: 0.1
2023-07-05 13:11: Argument missing_test: False
2023-07-05 13:11: Argument mode: 'train'
2023-07-05 13:11: Argument model: 'GCDE'
2023-07-05 13:11: Argument model_path: ''
2023-07-05 13:11: Argument model_type: 'type1'
2023-07-05 13:11: Argument normalizer: 'std'
2023-07-05 13:11: Argument num_layers: 2
2023-07-05 13:11: Argument num_nodes: 524
2023-07-05 13:11: Argument output_dim: 1
2023-07-05 13:11: Argument plot: False
2023-07-05 13:11: Argument real_value: True
2023-07-05 13:11: Argument seed: 0
2023-07-05 13:11: Argument solver: 'rk4'
2023-07-05 13:11: Argument teacher_forcing: False
2023-07-05 13:11: Argument tensorboard: True
2023-07-05 13:11: Argument test_ratio: 0.2
2023-07-05 13:11: Argument tod: False
2023-07-05 13:11: Argument val_ratio: 0.2
2023-07-05 13:11: Argument weight_decay: 0.001
2023-07-05 13:11: NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
2023-07-05 13:11: Total params: 381244
2023-07-05 13:11: Train Epoch 1: 0/323 Loss: 31.474377
2023-07-05 13:11: Train Epoch 1: 240/323 Loss: 3.081341
2023-07-05 13:11: Train Epoch 1: 20/323 Loss: 5.751929
2023-07-05 13:11: Train Epoch 1: 260/323 Loss: 2.975925
2023-07-05 13:12: Train Epoch 1: 40/323 Loss: 3.733726
2023-07-05 13:12: Train Epoch 1: 280/323 Loss: 3.036801
2023-07-05 13:12: Train Epoch 1: 60/323 Loss: 3.730603
2023-07-05 13:12: Train Epoch 1: 300/323 Loss: 2.818984
2023-07-05 13:13: Train Epoch 1: 80/323 Loss: 3.670849
2023-07-05 13:13: Train Epoch 1: 320/323 Loss: 2.585022
2023-07-05 13:13: **********Train Epoch 1: averaged Loss: 3.711323
2023-07-05 13:13: Train Epoch 1: 100/323 Loss: 3.727851
2023-07-05 13:13: Train Epoch 1: 120/323 Loss: 3.274071
2023-07-05 13:14: Train Epoch 1: 140/323 Loss: 3.070955
2023-07-05 13:14: **********Val Epoch 1: average Loss: 2.706460
2023-07-05 13:14: *********************************Current best model saved!
2023-07-05 13:14: Train Epoch 2: 0/323 Loss: 2.534589
2023-07-05 13:14: Train Epoch 1: 160/323 Loss: 3.317358
2023-07-05 13:14: Train Epoch 2: 20/323 Loss: 2.634932
2023-07-05 13:15: Train Epoch 1: 180/323 Loss: 3.295530
2023-07-05 13:15: Train Epoch 2: 40/323 Loss: 2.908306
2023-07-05 13:15: Train Epoch 1: 200/323 Loss: 3.564714
2023-07-05 13:15: Train Epoch 2: 60/323 Loss: 2.547422
2023-07-05 13:16: Train Epoch 1: 220/323 Loss: 3.206823
2023-07-05 13:16: Train Epoch 2: 80/323 Loss: 2.656652
2023-07-05 13:16: Train Epoch 1: 240/323 Loss: 3.588665
2023-07-05 13:17: Train Epoch 2: 100/323 Loss: 2.584962
2023-07-05 13:17: Train Epoch 1: 260/323 Loss: 3.064110
2023-07-05 13:17: Train Epoch 2: 120/323 Loss: 2.653769
2023-07-05 13:17: Train Epoch 1: 280/323 Loss: 3.209347
2023-07-05 13:18: Train Epoch 2: 140/323 Loss: 2.457754
2023-07-05 13:18: Train Epoch 1: 300/323 Loss: 3.206088
2023-07-05 13:18: Train Epoch 2: 160/323 Loss: 2.950431
2023-07-05 13:18: Train Epoch 1: 320/323 Loss: 3.028691
2023-07-05 13:18: **********Train Epoch 1: averaged Loss: 4.125169
2023-07-05 13:19: Train Epoch 2: 180/323 Loss: 2.724409
2023-07-05 13:19: Train Epoch 2: 200/323 Loss: 2.587596
2023-07-05 13:19: **********Val Epoch 1: average Loss: 3.111561
2023-07-05 13:19: *********************************Current best model saved!
2023-07-05 13:19: Train Epoch 2: 0/323 Loss: 3.173357
2023-07-05 13:20: Train Epoch 2: 20/323 Loss: 3.248092
2023-07-05 13:20: Train Epoch 2: 220/323 Loss: 2.866822
2023-07-05 13:20: Train Epoch 2: 40/323 Loss: 3.327130
2023-07-05 13:20: Train Epoch 2: 240/323 Loss: 2.579582
2023-07-05 13:21: Train Epoch 2: 60/323 Loss: 3.034133
2023-07-05 13:21: Train Epoch 2: 260/323 Loss: 2.395816
2023-07-05 13:21: Train Epoch 2: 80/323 Loss: 3.176824
2023-07-05 13:21: Train Epoch 2: 280/323 Loss: 2.701537
2023-07-05 13:21: Train Epoch 2: 100/323 Loss: 2.942556
2023-07-05 13:22: Train Epoch 2: 300/323 Loss: 2.607472
2023-07-05 13:22: Train Epoch 2: 120/323 Loss: 2.884582
2023-07-05 13:22: Train Epoch 2: 320/323 Loss: 2.463095
2023-07-05 13:22: **********Train Epoch 2: averaged Loss: 2.630654
2023-07-05 13:22: Train Epoch 2: 140/323 Loss: 2.913875
2023-07-05 13:23: Train Epoch 2: 160/323 Loss: 2.673602
2023-07-05 13:23: **********Val Epoch 2: average Loss: 2.545210
2023-07-05 13:23: *********************************Current best model saved!
2023-07-05 13:23: Train Epoch 3: 0/323 Loss: 2.516356
2023-07-05 13:23: Train Epoch 2: 180/323 Loss: 2.867831
2023-07-05 13:24: Train Epoch 3: 20/323 Loss: 2.499760
2023-07-05 13:24: Train Epoch 2: 200/323 Loss: 2.869932
2023-07-05 13:24: Train Epoch 3: 40/323 Loss: 2.488164
2023-07-05 13:24: Train Epoch 2: 220/323 Loss: 2.978353
2023-07-05 13:25: Train Epoch 3: 60/323 Loss: 2.628292
2023-07-05 13:25: Train Epoch 2: 240/323 Loss: 2.873232
2023-07-05 13:26: Train Epoch 3: 80/323 Loss: 2.353036
2023-07-05 13:26: Train Epoch 2: 260/323 Loss: 2.811023
2023-07-05 13:26: Train Epoch 2: 280/323 Loss: 3.119193
2023-07-05 13:26: Train Epoch 3: 100/323 Loss: 2.396677
2023-07-05 13:27: Train Epoch 2: 300/323 Loss: 2.770380
2023-07-05 13:27: Train Epoch 3: 120/323 Loss: 2.399733
2023-07-05 13:27: Train Epoch 2: 320/323 Loss: 2.681041
2023-07-05 13:27: **********Train Epoch 2: averaged Loss: 2.931622
2023-07-05 13:27: Train Epoch 3: 140/323 Loss: 2.540940
2023-07-05 13:28: Train Epoch 3: 160/323 Loss: 2.498536
2023-07-05 13:28: **********Val Epoch 2: average Loss: 2.754188
2023-07-05 13:28: *********************************Current best model saved!
2023-07-05 13:28: Train Epoch 3: 0/323 Loss: 2.695455
2023-07-05 13:28: Train Epoch 3: 180/323 Loss: 2.331295
2023-07-05 13:28: Train Epoch 3: 20/323 Loss: 2.664179
2023-07-05 13:29: Train Epoch 3: 200/323 Loss: 2.425064
2023-07-05 13:29: Train Epoch 3: 40/323 Loss: 2.824963
2023-07-05 13:29: Train Epoch 3: 220/323 Loss: 2.215576
2023-07-05 13:29: Train Epoch 3: 60/323 Loss: 2.750074
2023-07-05 13:30: Train Epoch 3: 240/323 Loss: 2.438854
2023-07-05 13:30: Train Epoch 3: 80/323 Loss: 2.782488
2023-07-05 13:30: Train Epoch 3: 260/323 Loss: 2.552107
2023-07-05 13:30: Train Epoch 3: 100/323 Loss: 2.640501
2023-07-05 13:31: Train Epoch 3: 120/323 Loss: 2.619167
2023-07-05 13:31: Train Epoch 3: 280/323 Loss: 2.266351
2023-07-05 13:31: Train Epoch 3: 140/323 Loss: 2.802927
2023-07-05 13:32: Train Epoch 3: 300/323 Loss: 2.359603
2023-07-05 13:32: Train Epoch 3: 160/323 Loss: 2.392906
2023-07-05 13:32: Train Epoch 3: 320/323 Loss: 2.335038
2023-07-05 13:32: **********Train Epoch 3: averaged Loss: 2.420196
2023-07-05 13:32: Train Epoch 3: 180/323 Loss: 2.731225
2023-07-05 13:33: Train Epoch 3: 200/323 Loss: 2.781232
2023-07-05 13:33: **********Val Epoch 3: average Loss: 2.451767
2023-07-05 13:33: *********************************Current best model saved!
2023-07-05 13:33: Train Epoch 4: 0/323 Loss: 2.583470
2023-07-05 13:33: Train Epoch 3: 220/323 Loss: 2.928726
2023-07-05 13:34: Train Epoch 4: 20/323 Loss: 2.471933
2023-07-05 13:34: Train Epoch 3: 240/323 Loss: 2.403583
2023-07-05 13:34: Train Epoch 4: 40/323 Loss: 2.482289
2023-07-05 13:34: Train Epoch 3: 260/323 Loss: 2.583758
2023-07-05 13:35: Train Epoch 4: 60/323 Loss: 2.335145
2023-07-05 13:35: Train Epoch 3: 280/323 Loss: 2.814747
2023-07-05 13:35: Train Epoch 4: 80/323 Loss: 2.435256
2023-07-05 13:35: Train Epoch 3: 300/323 Loss: 2.835406
2023-07-05 13:36: Train Epoch 3: 320/323 Loss: 2.638881
2023-07-05 13:36: Train Epoch 4: 100/323 Loss: 2.342667
2023-07-05 13:36: **********Train Epoch 3: averaged Loss: 2.692654
2023-07-05 13:36: Train Epoch 4: 120/323 Loss: 2.196961
2023-07-05 13:37: **********Val Epoch 3: average Loss: 2.690362
2023-07-05 13:37: *********************************Current best model saved!
2023-07-05 13:37: Train Epoch 4: 0/323 Loss: 2.697686
2023-07-05 13:37: Train Epoch 4: 140/323 Loss: 2.324352
2023-07-05 13:37: Train Epoch 4: 20/323 Loss: 2.727619
2023-07-05 13:37: Train Epoch 4: 160/323 Loss: 2.392568
2023-07-05 13:38: Train Epoch 4: 40/323 Loss: 2.553707
2023-07-05 13:38: Train Epoch 4: 180/323 Loss: 2.387813
2023-07-05 13:38: Train Epoch 4: 60/323 Loss: 2.666145
2023-07-05 13:38: Train Epoch 4: 200/323 Loss: 2.352837
2023-07-05 13:39: Train Epoch 4: 80/323 Loss: 2.512972
2023-07-05 13:39: Train Epoch 4: 220/323 Loss: 2.332063
2023-07-05 13:39: Train Epoch 4: 100/323 Loss: 2.559027
2023-07-05 13:40: Train Epoch 4: 240/323 Loss: 2.376053
2023-07-05 13:40: Train Epoch 4: 120/323 Loss: 2.669684
2023-07-05 13:40: Train Epoch 4: 260/323 Loss: 2.264945
2023-07-05 13:40: Train Epoch 4: 140/323 Loss: 2.791221
2023-07-05 13:41: Train Epoch 4: 280/323 Loss: 2.368105
2023-07-05 13:41: Train Epoch 4: 160/323 Loss: 2.591843
2023-07-05 13:41: Train Epoch 4: 180/323 Loss: 2.545670
2023-07-05 13:41: Train Epoch 4: 300/323 Loss: 2.227445
2023-07-05 13:42: Train Epoch 4: 200/323 Loss: 2.613922
2023-07-05 13:42: Train Epoch 4: 320/323 Loss: 2.312164
2023-07-05 13:42: **********Train Epoch 4: averaged Loss: 2.351232
2023-07-05 13:42: Train Epoch 4: 220/323 Loss: 2.618252
2023-07-05 13:42: Train Epoch 4: 240/323 Loss: 2.674684
2023-07-05 13:43: **********Val Epoch 4: average Loss: 2.357284
2023-07-05 13:43: *********************************Current best model saved!
2023-07-05 13:43: Train Epoch 5: 0/323 Loss: 2.232099
2023-07-05 13:43: Train Epoch 4: 260/323 Loss: 2.434364
2023-07-05 13:43: Train Epoch 5: 20/323 Loss: 2.219166
2023-07-05 13:43: Train Epoch 4: 280/323 Loss: 2.736912
2023-07-05 13:44: Train Epoch 5: 40/323 Loss: 2.366101
2023-07-05 13:44: Train Epoch 4: 300/323 Loss: 2.661810
2023-07-05 13:44: Train Epoch 5: 60/323 Loss: 2.489947
2023-07-05 13:44: Train Epoch 4: 320/323 Loss: 2.531766
2023-07-05 13:44: **********Train Epoch 4: averaged Loss: 2.633696
2023-07-05 13:45: Train Epoch 5: 80/323 Loss: 2.320880
2023-07-05 13:46: **********Val Epoch 4: average Loss: 2.696363
2023-07-05 13:46: Train Epoch 5: 100/323 Loss: 2.441013
2023-07-05 13:46: Train Epoch 5: 0/323 Loss: 2.523839
2023-07-05 13:46: Train Epoch 5: 20/323 Loss: 2.498165
2023-07-05 13:46: Train Epoch 5: 120/323 Loss: 2.327719
2023-07-05 13:46: Train Epoch 5: 40/323 Loss: 2.603051
2023-07-05 13:47: Train Epoch 5: 140/323 Loss: 2.357243
2023-07-05 13:47: Train Epoch 5: 60/323 Loss: 2.540833
2023-07-05 13:47: Train Epoch 5: 160/323 Loss: 2.102187
2023-07-05 13:47: Train Epoch 5: 80/323 Loss: 2.747001
2023-07-05 13:48: Train Epoch 5: 180/323 Loss: 2.372890
2023-07-05 13:48: Train Epoch 5: 100/323 Loss: 2.636396
2023-07-05 13:48: Train Epoch 5: 200/323 Loss: 2.297943
2023-07-05 13:48: Train Epoch 5: 120/323 Loss: 2.664743
2023-07-05 13:49: Train Epoch 5: 220/323 Loss: 2.278464
2023-07-05 13:49: Train Epoch 5: 140/323 Loss: 2.649506
2023-07-05 13:49: Train Epoch 5: 240/323 Loss: 2.250414
2023-07-05 13:49: Train Epoch 5: 160/323 Loss: 2.550724
2023-07-05 13:50: Train Epoch 5: 180/323 Loss: 2.386066
2023-07-05 13:50: Train Epoch 5: 260/323 Loss: 2.298164
2023-07-05 13:50: Train Epoch 5: 200/323 Loss: 2.756173
2023-07-05 13:50: Train Epoch 5: 280/323 Loss: 2.295827
2023-07-05 13:51: Train Epoch 5: 220/323 Loss: 2.607621
2023-07-05 13:51: Train Epoch 5: 300/323 Loss: 2.153496
2023-07-05 13:51: Train Epoch 5: 240/323 Loss: 2.688704
2023-07-05 13:51: Train Epoch 5: 320/323 Loss: 2.329503
2023-07-05 13:51: **********Train Epoch 5: averaged Loss: 2.329106
2023-07-05 13:52: Train Epoch 5: 260/323 Loss: 2.619074
2023-07-05 13:52: Train Epoch 5: 280/323 Loss: 2.576920
2023-07-05 13:52: **********Val Epoch 5: average Loss: 2.339269
2023-07-05 13:52: *********************************Current best model saved!
2023-07-05 13:52: Train Epoch 6: 0/323 Loss: 2.278146
2023-07-05 13:52: Train Epoch 5: 300/323 Loss: 2.566678
2023-07-05 13:53: Train Epoch 5: 320/323 Loss: 2.571553
2023-07-05 13:53: **********Train Epoch 5: averaged Loss: 2.600667
2023-07-05 13:53: Train Epoch 6: 20/323 Loss: 2.250375
2023-07-05 13:54: Train Epoch 6: 40/323 Loss: 2.170715
2023-07-05 13:54: **********Val Epoch 5: average Loss: 2.691819
2023-07-05 13:54: Train Epoch 6: 0/323 Loss: 2.604891
2023-07-05 13:54: Train Epoch 6: 60/323 Loss: 2.261581
2023-07-05 13:54: Train Epoch 6: 20/323 Loss: 2.855325
2023-07-05 13:55: Train Epoch 6: 80/323 Loss: 2.140400
2023-07-05 13:55: Train Epoch 6: 40/323 Loss: 2.464867
2023-07-05 13:55: Train Epoch 6: 100/323 Loss: 2.327883
2023-07-05 13:55: Train Epoch 6: 60/323 Loss: 2.489417
2023-07-05 13:56: Train Epoch 6: 120/323 Loss: 2.398560
2023-07-05 13:56: Train Epoch 6: 80/323 Loss: 2.422018
2023-07-05 13:56: Train Epoch 6: 100/323 Loss: 2.627464
2023-07-05 13:56: Train Epoch 6: 140/323 Loss: 2.390265
2023-07-05 13:57: Train Epoch 6: 120/323 Loss: 2.559559
2023-07-05 13:57: Train Epoch 6: 160/323 Loss: 2.300738
2023-07-05 13:57: Train Epoch 6: 140/323 Loss: 2.493258
2023-07-05 13:57: Train Epoch 6: 180/323 Loss: 2.186215
2023-07-05 13:58: Train Epoch 6: 160/323 Loss: 2.517006
2023-07-05 13:58: Train Epoch 6: 200/323 Loss: 2.360577
2023-07-05 13:58: Train Epoch 6: 180/323 Loss: 2.478413
2023-07-05 13:58: Train Epoch 6: 220/323 Loss: 2.401891
2023-07-05 13:59: Train Epoch 6: 200/323 Loss: 2.472271
2023-07-05 13:59: Train Epoch 6: 240/323 Loss: 2.245410
2023-07-05 13:59: Train Epoch 6: 220/323 Loss: 2.598287
2023-07-05 13:59: Train Epoch 6: 260/323 Loss: 2.473339
2023-07-05 14:00: Train Epoch 6: 240/323 Loss: 2.474648
2023-07-05 14:01: Train Epoch 6: 280/323 Loss: 2.155959
2023-07-05 14:01: Train Epoch 6: 260/323 Loss: 2.480771
2023-07-05 14:02: Train Epoch 6: 300/323 Loss: 2.345089
2023-07-05 14:02: Train Epoch 6: 280/323 Loss: 2.762793
2023-07-05 14:04: Train Epoch 6: 320/323 Loss: 2.275885
2023-07-05 14:04: Train Epoch 6: 300/323 Loss: 2.576947
2023-07-05 14:04: **********Train Epoch 6: averaged Loss: 2.293359
2023-07-05 14:05: Train Epoch 6: 320/323 Loss: 2.583641
2023-07-05 14:05: **********Train Epoch 6: averaged Loss: 2.572614
2023-07-05 14:06: **********Val Epoch 6: average Loss: 2.262637
2023-07-05 14:06: *********************************Current best model saved!
2023-07-05 14:07: Train Epoch 7: 0/323 Loss: 2.544315
2023-07-05 14:07: **********Val Epoch 6: average Loss: 2.605535
2023-07-05 14:07: *********************************Current best model saved!
2023-07-05 14:08: Train Epoch 7: 0/323 Loss: 2.666795
2023-07-05 14:08: Train Epoch 7: 20/323 Loss: 2.338995
2023-07-05 14:09: Train Epoch 7: 20/323 Loss: 2.530868
2023-07-05 14:09: Train Epoch 7: 40/323 Loss: 2.298243
2023-07-05 14:10: Train Epoch 7: 40/323 Loss: 2.541967
2023-07-05 14:11: Train Epoch 7: 60/323 Loss: 2.468185
2023-07-05 14:11: Train Epoch 7: 60/323 Loss: 2.732628
2023-07-05 14:12: Train Epoch 7: 80/323 Loss: 2.249890
2023-07-05 14:13: Train Epoch 7: 80/323 Loss: 2.650946
2023-07-05 14:14: Train Epoch 7: 100/323 Loss: 2.158068
2023-07-05 14:14: Train Epoch 7: 100/323 Loss: 2.515492
2023-07-05 14:15: Train Epoch 7: 120/323 Loss: 2.137519
2023-07-05 14:15: Train Epoch 7: 120/323 Loss: 2.684437
2023-07-05 14:16: Train Epoch 7: 140/323 Loss: 2.294634
2023-07-05 14:17: Train Epoch 7: 140/323 Loss: 2.462587
2023-07-05 14:18: Train Epoch 7: 160/323 Loss: 2.453394
2023-07-05 14:18: Train Epoch 7: 160/323 Loss: 2.679129
2023-07-05 14:19: Train Epoch 7: 180/323 Loss: 2.248137
2023-07-05 14:19: Train Epoch 7: 180/323 Loss: 2.549677
2023-07-05 14:20: Train Epoch 7: 200/323 Loss: 2.557842
2023-07-05 14:20: Train Epoch 7: 200/323 Loss: 2.161907
2023-07-05 14:22: Train Epoch 7: 220/323 Loss: 2.602839
2023-07-05 14:22: Train Epoch 7: 220/323 Loss: 2.263556
2023-07-05 14:23: Train Epoch 7: 240/323 Loss: 2.646131
2023-07-05 14:23: Train Epoch 7: 240/323 Loss: 2.180699
2023-07-05 14:24: Train Epoch 7: 260/323 Loss: 2.558572
2023-07-05 14:24: Train Epoch 7: 260/323 Loss: 2.190442
2023-07-05 14:25: Train Epoch 7: 280/323 Loss: 2.591425
2023-07-05 14:26: Train Epoch 7: 280/323 Loss: 2.244846
2023-07-05 14:26: Train Epoch 7: 300/323 Loss: 2.658364
2023-07-05 14:27: Train Epoch 7: 300/323 Loss: 2.292492
2023-07-05 14:28: Train Epoch 7: 320/323 Loss: 2.679990
2023-07-05 14:28: **********Train Epoch 7: averaged Loss: 2.549712
2023-07-05 14:28: Train Epoch 7: 320/323 Loss: 2.201927
2023-07-05 14:29: **********Train Epoch 7: averaged Loss: 2.265634
2023-07-05 14:30: **********Val Epoch 7: average Loss: 2.606229
2023-07-05 14:30: Train Epoch 8: 0/323 Loss: 2.451204
2023-07-05 14:31: **********Val Epoch 7: average Loss: 2.282683
2023-07-05 14:31: Train Epoch 8: 0/323 Loss: 2.125625
2023-07-05 14:31: Train Epoch 8: 20/323 Loss: 2.463026
2023-07-05 14:32: Train Epoch 8: 20/323 Loss: 2.246394
2023-07-05 14:33: Train Epoch 8: 40/323 Loss: 2.847523
2023-07-05 14:34: Train Epoch 8: 60/323 Loss: 2.440728
2023-07-05 14:34: Train Epoch 8: 40/323 Loss: 2.443220
2023-07-05 14:35: Train Epoch 8: 80/323 Loss: 2.533378
2023-07-05 14:35: Train Epoch 8: 60/323 Loss: 2.417576
2023-07-05 14:36: Train Epoch 8: 100/323 Loss: 2.618941
2023-07-05 14:37: Train Epoch 8: 80/323 Loss: 2.309064
2023-07-05 14:38: Train Epoch 8: 120/323 Loss: 2.399031
2023-07-05 14:38: Train Epoch 8: 100/323 Loss: 2.253289
2023-07-05 14:39: Train Epoch 8: 140/323 Loss: 2.582751
2023-07-05 14:39: Train Epoch 8: 120/323 Loss: 2.231590
2023-07-05 14:40: Train Epoch 8: 160/323 Loss: 2.484302
2023-07-05 14:41: Train Epoch 8: 140/323 Loss: 2.341919
2023-07-05 14:41: Train Epoch 8: 180/323 Loss: 2.387476
2023-07-05 14:42: Train Epoch 8: 160/323 Loss: 2.199929
2023-07-05 14:43: Train Epoch 8: 200/323 Loss: 2.385251
2023-07-05 14:44: Train Epoch 8: 180/323 Loss: 2.137007
2023-07-05 14:44: Train Epoch 8: 220/323 Loss: 2.440712
2023-07-05 14:45: Train Epoch 8: 200/323 Loss: 2.346442
2023-07-05 14:45: Train Epoch 8: 240/323 Loss: 2.490613
2023-07-05 14:46: Train Epoch 8: 220/323 Loss: 2.158794
2023-07-05 14:47: Train Epoch 8: 260/323 Loss: 2.670300
2023-07-05 14:48: Train Epoch 8: 240/323 Loss: 2.280615
2023-07-05 14:48: Train Epoch 8: 280/323 Loss: 2.708021
2023-07-05 14:49: Train Epoch 8: 260/323 Loss: 2.307913
2023-07-05 14:49: Train Epoch 8: 300/323 Loss: 2.521732
2023-07-05 14:50: Train Epoch 8: 320/323 Loss: 2.474520
2023-07-05 14:51: Train Epoch 8: 280/323 Loss: 2.176651
2023-07-05 14:51: **********Train Epoch 8: averaged Loss: 2.526275
2023-07-05 14:52: Train Epoch 8: 300/323 Loss: 2.198340
2023-07-05 14:53: **********Val Epoch 8: average Loss: 2.540587
2023-07-05 14:53: *********************************Current best model saved!
2023-07-05 14:53: Train Epoch 9: 0/323 Loss: 2.624727
2023-07-05 14:53: Train Epoch 8: 320/323 Loss: 2.264872
2023-07-05 14:53: **********Train Epoch 8: averaged Loss: 2.272296
2023-07-05 14:54: Train Epoch 9: 20/323 Loss: 2.713418
2023-07-05 14:55: Train Epoch 9: 40/323 Loss: 2.284093
2023-07-05 14:56: **********Val Epoch 8: average Loss: 2.275488
2023-07-05 14:56: Train Epoch 9: 0/323 Loss: 2.090868
2023-07-05 14:57: Train Epoch 9: 60/323 Loss: 2.466377
2023-07-05 14:57: Train Epoch 9: 20/323 Loss: 2.301385
2023-07-05 14:58: Train Epoch 9: 80/323 Loss: 2.431396
2023-07-05 14:59: Train Epoch 9: 40/323 Loss: 2.317769
2023-07-05 14:59: Train Epoch 9: 100/323 Loss: 2.556231
2023-07-05 15:00: Train Epoch 9: 60/323 Loss: 2.354266
2023-07-05 15:00: Train Epoch 9: 120/323 Loss: 2.663331
2023-07-05 15:01: Train Epoch 9: 80/323 Loss: 2.102024
2023-07-05 15:02: Train Epoch 9: 140/323 Loss: 2.545379
2023-07-05 15:03: Train Epoch 9: 100/323 Loss: 2.179482
2023-07-05 15:03: Train Epoch 9: 160/323 Loss: 2.588850
2023-07-05 15:04: Train Epoch 9: 120/323 Loss: 2.151490
2023-07-05 15:04: Train Epoch 9: 180/323 Loss: 2.556510
2023-07-05 15:05: Train Epoch 9: 200/323 Loss: 2.335864
2023-07-05 15:05: Train Epoch 9: 140/323 Loss: 2.353065
2023-07-05 15:06: Train Epoch 9: 220/323 Loss: 2.428953
2023-07-05 15:07: Train Epoch 9: 160/323 Loss: 2.148837
2023-07-05 15:08: Train Epoch 9: 240/323 Loss: 2.444205
2023-07-05 15:08: Train Epoch 9: 180/323 Loss: 2.290529
2023-07-05 15:09: Train Epoch 9: 260/323 Loss: 2.378581
2023-07-05 15:09: Train Epoch 9: 200/323 Loss: 2.366565
2023-07-05 15:10: Train Epoch 9: 280/323 Loss: 2.656210
2023-07-05 15:11: Train Epoch 9: 220/323 Loss: 2.403514
2023-07-05 15:11: Train Epoch 9: 300/323 Loss: 2.591248
2023-07-05 15:12: Train Epoch 9: 240/323 Loss: 2.155207
2023-07-05 15:13: Train Epoch 9: 320/323 Loss: 2.603997
2023-07-05 15:13: **********Train Epoch 9: averaged Loss: 2.512852
2023-07-05 15:13: Train Epoch 9: 260/323 Loss: 2.082667
2023-07-05 15:15: Train Epoch 9: 280/323 Loss: 2.263630
2023-07-05 15:15: **********Val Epoch 9: average Loss: 2.604448
2023-07-05 15:15: Train Epoch 10: 0/323 Loss: 2.518022
2023-07-05 15:16: Train Epoch 9: 300/323 Loss: 2.287984
2023-07-05 15:16: Train Epoch 10: 20/323 Loss: 2.504788
2023-07-05 15:17: Train Epoch 9: 320/323 Loss: 2.232076
2023-07-05 15:18: **********Train Epoch 9: averaged Loss: 2.232905
2023-07-05 15:18: Train Epoch 10: 40/323 Loss: 2.373836
2023-07-05 15:19: Train Epoch 10: 60/323 Loss: 2.611413
2023-07-05 15:20: Train Epoch 10: 80/323 Loss: 2.467556
2023-07-05 15:20: **********Val Epoch 9: average Loss: 2.303430
2023-07-05 15:20: Train Epoch 10: 0/323 Loss: 2.109286
2023-07-05 15:21: Train Epoch 10: 100/323 Loss: 2.539485
2023-07-05 15:22: Train Epoch 10: 20/323 Loss: 2.344081
2023-07-05 15:23: Train Epoch 10: 120/323 Loss: 2.529521
2023-07-05 15:23: Train Epoch 10: 40/323 Loss: 2.367003
2023-07-05 15:24: Train Epoch 10: 140/323 Loss: 2.301568
2023-07-05 15:25: Train Epoch 10: 60/323 Loss: 2.313743
2023-07-05 15:25: Train Epoch 10: 160/323 Loss: 2.651390
2023-07-05 15:26: Train Epoch 10: 80/323 Loss: 2.227757
2023-07-05 15:27: Train Epoch 10: 180/323 Loss: 2.394827
2023-07-05 15:27: Train Epoch 10: 100/323 Loss: 2.081773
2023-07-05 15:28: Train Epoch 10: 200/323 Loss: 2.499091
2023-07-05 15:29: Train Epoch 10: 120/323 Loss: 2.470186
2023-07-05 15:29: Train Epoch 10: 220/323 Loss: 2.882512
2023-07-05 15:30: Train Epoch 10: 140/323 Loss: 2.279283
2023-07-05 15:30: Train Epoch 10: 240/323 Loss: 2.287518
2023-07-05 15:31: Train Epoch 10: 160/323 Loss: 2.337645
2023-07-05 15:32: Train Epoch 10: 260/323 Loss: 2.523312
2023-07-05 15:33: Train Epoch 10: 180/323 Loss: 2.282139
2023-07-05 15:33: Train Epoch 10: 280/323 Loss: 2.236449
2023-07-05 15:34: Train Epoch 10: 300/323 Loss: 2.314548
2023-07-05 15:34: Train Epoch 10: 200/323 Loss: 2.209979
2023-07-05 15:35: Train Epoch 10: 320/323 Loss: 2.536647
2023-07-05 15:35: **********Train Epoch 10: averaged Loss: 2.504240
2023-07-05 15:35: Train Epoch 10: 220/323 Loss: 2.080596
2023-07-05 15:37: Train Epoch 10: 240/323 Loss: 2.482147
2023-07-05 15:38: **********Val Epoch 10: average Loss: 2.569573
2023-07-05 15:38: Train Epoch 11: 0/323 Loss: 2.554267
2023-07-05 15:38: Train Epoch 10: 260/323 Loss: 2.355412
2023-07-05 15:39: Train Epoch 11: 20/323 Loss: 2.493351
2023-07-05 15:39: Train Epoch 10: 280/323 Loss: 2.254651
2023-07-05 15:40: Train Epoch 11: 40/323 Loss: 2.639350
2023-07-05 15:41: Train Epoch 10: 300/323 Loss: 2.339704
2023-07-05 15:41: Train Epoch 11: 60/323 Loss: 2.674661
2023-07-05 15:42: Train Epoch 10: 320/323 Loss: 2.262843
2023-07-05 15:42: **********Train Epoch 10: averaged Loss: 2.232754
2023-07-05 15:43: Train Epoch 11: 80/323 Loss: 2.483851
2023-07-05 15:44: Train Epoch 11: 100/323 Loss: 2.634254
2023-07-05 15:45: **********Val Epoch 10: average Loss: 2.224672
2023-07-05 15:45: *********************************Current best model saved!
2023-07-05 15:45: Train Epoch 11: 0/323 Loss: 2.015576
2023-07-05 15:45: Train Epoch 11: 120/323 Loss: 2.557134
2023-07-05 15:46: Train Epoch 11: 20/323 Loss: 2.158362
2023-07-05 15:46: Train Epoch 11: 140/323 Loss: 2.357342
2023-07-05 15:47: Train Epoch 11: 160/323 Loss: 2.282115
2023-07-05 15:47: Train Epoch 11: 40/323 Loss: 2.207176
2023-07-05 15:49: Train Epoch 11: 180/323 Loss: 2.468988
2023-07-05 15:49: Train Epoch 11: 60/323 Loss: 2.305517
2023-07-05 15:50: Train Epoch 11: 200/323 Loss: 2.402061
2023-07-05 15:50: Train Epoch 11: 80/323 Loss: 2.201838
2023-07-05 15:51: Train Epoch 11: 220/323 Loss: 2.342002
2023-07-05 15:51: Train Epoch 11: 100/323 Loss: 2.310946
2023-07-05 15:52: Train Epoch 11: 240/323 Loss: 2.476838
2023-07-05 15:53: Train Epoch 11: 120/323 Loss: 2.185877
2023-07-05 15:53: Train Epoch 11: 260/323 Loss: 2.442557
2023-07-05 15:54: Train Epoch 11: 140/323 Loss: 2.323040
2023-07-05 15:55: Train Epoch 11: 280/323 Loss: 2.374305
2023-07-05 15:56: Train Epoch 11: 160/323 Loss: 2.249425
2023-07-05 15:56: Train Epoch 11: 300/323 Loss: 2.516874
2023-07-05 15:57: Train Epoch 11: 180/323 Loss: 2.085797
2023-07-05 15:57: Train Epoch 11: 320/323 Loss: 2.369652
2023-07-05 15:57: **********Train Epoch 11: averaged Loss: 2.495096
2023-07-05 15:58: Train Epoch 11: 200/323 Loss: 2.261203
2023-07-05 16:00: Train Epoch 11: 220/323 Loss: 2.265408
2023-07-05 16:00: **********Val Epoch 11: average Loss: 2.520715
2023-07-05 16:00: *********************************Current best model saved!
2023-07-05 16:00: Train Epoch 12: 0/323 Loss: 2.563551
2023-07-05 16:01: Train Epoch 11: 240/323 Loss: 2.242975
2023-07-05 16:01: Train Epoch 12: 20/323 Loss: 2.646259
2023-07-05 16:03: Train Epoch 11: 260/323 Loss: 2.085444
2023-07-05 16:03: Train Epoch 12: 40/323 Loss: 2.531369
2023-07-05 16:04: Train Epoch 12: 60/323 Loss: 2.725453
2023-07-05 16:04: Train Epoch 11: 280/323 Loss: 2.281273
2023-07-05 16:05: Train Epoch 12: 80/323 Loss: 2.583964
2023-07-05 16:05: Train Epoch 11: 300/323 Loss: 2.374764
2023-07-05 16:06: Train Epoch 12: 100/323 Loss: 2.537367
2023-07-05 16:07: Train Epoch 11: 320/323 Loss: 2.426832
2023-07-05 16:07: **********Train Epoch 11: averaged Loss: 2.222763
2023-07-05 16:08: Train Epoch 12: 120/323 Loss: 2.435985
2023-07-05 16:09: Train Epoch 12: 140/323 Loss: 2.346958
2023-07-05 16:10: **********Val Epoch 11: average Loss: 2.218638
2023-07-05 16:10: *********************************Current best model saved!
2023-07-05 16:10: Train Epoch 12: 0/323 Loss: 2.183544
2023-07-05 16:10: Train Epoch 12: 160/323 Loss: 2.500696
2023-07-05 16:11: Train Epoch 12: 20/323 Loss: 2.190763
2023-07-05 16:11: Train Epoch 12: 180/323 Loss: 2.505650
2023-07-05 16:12: Train Epoch 12: 40/323 Loss: 2.154349
2023-07-05 16:13: Train Epoch 12: 200/323 Loss: 2.492687
2023-07-05 16:14: Train Epoch 12: 60/323 Loss: 2.242773
2023-07-05 16:14: Train Epoch 12: 220/323 Loss: 2.549055
2023-07-05 16:15: Train Epoch 12: 80/323 Loss: 2.210554
2023-07-05 16:15: Train Epoch 12: 240/323 Loss: 2.621551
2023-07-05 16:16: Train Epoch 12: 260/323 Loss: 2.571071
2023-07-05 16:16: Train Epoch 12: 100/323 Loss: 2.085716
2023-07-05 16:18: Train Epoch 12: 280/323 Loss: 2.330952
2023-07-05 16:18: Train Epoch 12: 120/323 Loss: 2.258220
2023-07-05 16:19: Train Epoch 12: 300/323 Loss: 2.476026
2023-07-05 16:19: Train Epoch 12: 140/323 Loss: 2.099345
2023-07-05 16:20: Train Epoch 12: 320/323 Loss: 2.279818
2023-07-05 16:20: **********Train Epoch 12: averaged Loss: 2.483624
2023-07-05 16:20: Train Epoch 12: 160/323 Loss: 2.219510
2023-07-05 16:22: Train Epoch 12: 180/323 Loss: 2.149333
2023-07-05 16:22: **********Val Epoch 12: average Loss: 2.510635
2023-07-05 16:22: *********************************Current best model saved!
2023-07-05 16:22: Train Epoch 13: 0/323 Loss: 2.439654
2023-07-05 16:23: Train Epoch 12: 200/323 Loss: 2.176714
2023-07-05 16:24: Train Epoch 13: 20/323 Loss: 2.586904
2023-07-05 16:24: Train Epoch 12: 220/323 Loss: 2.281620
2023-07-05 16:25: Train Epoch 13: 40/323 Loss: 2.452146
2023-07-05 16:26: Train Epoch 12: 240/323 Loss: 2.176071
2023-07-05 16:26: Train Epoch 13: 60/323 Loss: 2.530735
2023-07-05 16:27: Train Epoch 12: 260/323 Loss: 2.242123
2023-07-05 16:27: Train Epoch 13: 80/323 Loss: 2.464149
2023-07-05 16:28: Train Epoch 12: 280/323 Loss: 2.192336
2023-07-05 16:29: Train Epoch 13: 100/323 Loss: 2.455575
2023-07-05 16:30: Train Epoch 12: 300/323 Loss: 2.073157
2023-07-05 16:30: Train Epoch 13: 120/323 Loss: 2.461692
2023-07-05 16:31: Train Epoch 12: 320/323 Loss: 2.189226
2023-07-05 16:31: Train Epoch 13: 140/323 Loss: 2.427382
2023-07-05 16:31: **********Train Epoch 12: averaged Loss: 2.204305
2023-07-05 16:32: Train Epoch 13: 160/323 Loss: 2.483297
2023-07-05 16:34: Train Epoch 13: 180/323 Loss: 2.602722
2023-07-05 16:34: **********Val Epoch 12: average Loss: 2.275106
2023-07-05 16:34: Train Epoch 13: 0/323 Loss: 2.267139
2023-07-05 16:35: Train Epoch 13: 200/323 Loss: 2.336617
2023-07-05 16:35: Train Epoch 13: 20/323 Loss: 2.201205
2023-07-05 16:36: Train Epoch 13: 220/323 Loss: 2.473066
2023-07-05 16:37: Train Epoch 13: 40/323 Loss: 2.456248
2023-07-05 16:38: Train Epoch 13: 240/323 Loss: 2.585907
2023-07-05 16:38: Train Epoch 13: 60/323 Loss: 2.093654
2023-07-05 16:39: Train Epoch 13: 260/323 Loss: 2.437557
2023-07-05 16:40: Train Epoch 13: 80/323 Loss: 2.052053
2023-07-05 16:40: Train Epoch 13: 280/323 Loss: 2.454788
2023-07-05 16:41: Train Epoch 13: 100/323 Loss: 2.315953
2023-07-05 16:41: Train Epoch 13: 300/323 Loss: 2.505524
2023-07-05 16:42: Train Epoch 13: 120/323 Loss: 2.177060
2023-07-05 16:43: Train Epoch 13: 320/323 Loss: 2.570928
2023-07-05 16:43: **********Train Epoch 13: averaged Loss: 2.468993
2023-07-05 16:44: Train Epoch 13: 140/323 Loss: 2.163352
2023-07-05 16:45: Train Epoch 13: 160/323 Loss: 2.280198
2023-07-05 16:45: **********Val Epoch 13: average Loss: 2.570045
2023-07-05 16:45: Train Epoch 14: 0/323 Loss: 2.636621
2023-07-05 16:46: Train Epoch 13: 180/323 Loss: 2.216592
2023-07-05 16:47: Train Epoch 14: 20/323 Loss: 2.584693
2023-07-05 16:48: Train Epoch 13: 200/323 Loss: 2.289758
2023-07-05 16:48: Train Epoch 14: 40/323 Loss: 2.449609
2023-07-05 16:49: Train Epoch 14: 60/323 Loss: 2.340373
2023-07-05 16:49: Train Epoch 13: 220/323 Loss: 2.091582
2023-07-05 16:50: Train Epoch 14: 80/323 Loss: 2.646982
2023-07-05 16:50: Train Epoch 13: 240/323 Loss: 2.209535
2023-07-05 16:51: Train Epoch 14: 100/323 Loss: 2.616752
2023-07-05 16:52: Train Epoch 13: 260/323 Loss: 2.069135
2023-07-05 16:53: Train Epoch 14: 120/323 Loss: 2.324407
2023-07-05 16:53: Train Epoch 13: 280/323 Loss: 2.393321
2023-07-05 16:54: Train Epoch 14: 140/323 Loss: 2.500842
2023-07-05 16:55: Train Epoch 13: 300/323 Loss: 2.398818
2023-07-05 16:55: Train Epoch 14: 160/323 Loss: 2.545338
2023-07-05 16:56: Train Epoch 13: 320/323 Loss: 2.422967
2023-07-05 16:56: **********Train Epoch 13: averaged Loss: 2.201931
2023-07-05 16:56: Train Epoch 14: 180/323 Loss: 2.389776
2023-07-05 16:58: Train Epoch 14: 200/323 Loss: 2.583167
2023-07-05 16:59: **********Val Epoch 13: average Loss: 2.200053
2023-07-05 16:59: *********************************Current best model saved!
2023-07-05 16:59: Train Epoch 14: 0/323 Loss: 2.439746
2023-07-05 16:59: Train Epoch 14: 220/323 Loss: 2.441849
2023-07-05 17:00: Train Epoch 14: 20/323 Loss: 2.181609
2023-07-05 17:00: Train Epoch 14: 240/323 Loss: 2.515289
2023-07-05 17:01: Train Epoch 14: 260/323 Loss: 2.355001
2023-07-05 17:01: Train Epoch 14: 40/323 Loss: 2.242593
2023-07-05 17:02: Train Epoch 14: 280/323 Loss: 2.355124
2023-07-05 17:03: Train Epoch 14: 60/323 Loss: 2.211311
2023-07-05 17:04: Train Epoch 14: 300/323 Loss: 2.496698
2023-07-05 17:04: Train Epoch 14: 80/323 Loss: 2.180612
2023-07-05 17:05: Train Epoch 14: 320/323 Loss: 2.295639
2023-07-05 17:05: **********Train Epoch 14: averaged Loss: 2.452080
2023-07-05 17:05: Train Epoch 14: 100/323 Loss: 2.231221
2023-07-05 17:07: Train Epoch 14: 120/323 Loss: 2.141093
2023-07-05 17:07: **********Val Epoch 14: average Loss: 2.514877
2023-07-05 17:07: Train Epoch 15: 0/323 Loss: 2.364518
2023-07-05 17:08: Train Epoch 14: 140/323 Loss: 2.169824
2023-07-05 17:09: Train Epoch 15: 20/323 Loss: 2.636320
2023-07-05 17:09: Train Epoch 14: 160/323 Loss: 2.177919
2023-07-05 17:10: Train Epoch 15: 40/323 Loss: 2.549161
2023-07-05 17:11: Train Epoch 14: 180/323 Loss: 2.238740
2023-07-05 17:12: Train Epoch 15: 60/323 Loss: 2.395010
2023-07-05 17:12: Train Epoch 14: 200/323 Loss: 2.208884
2023-07-05 17:13: Train Epoch 15: 80/323 Loss: 2.533596
2023-07-05 17:14: Train Epoch 14: 220/323 Loss: 2.288078
2023-07-05 17:14: Train Epoch 15: 100/323 Loss: 2.631463
2023-07-05 17:15: Train Epoch 14: 240/323 Loss: 2.303634
2023-07-05 17:15: Train Epoch 15: 120/323 Loss: 2.458016
2023-07-05 17:17: Train Epoch 14: 260/323 Loss: 2.060734
2023-07-05 17:17: Train Epoch 15: 140/323 Loss: 2.527589
2023-07-05 17:18: Train Epoch 14: 280/323 Loss: 2.126902
2023-07-05 17:18: Train Epoch 15: 160/323 Loss: 2.395432
2023-07-05 17:19: Train Epoch 15: 180/323 Loss: 2.385205
2023-07-05 17:19: Train Epoch 14: 300/323 Loss: 2.188707
2023-07-05 17:20: Train Epoch 15: 200/323 Loss: 2.274793
2023-07-05 17:21: Train Epoch 14: 320/323 Loss: 2.178083
2023-07-05 17:21: **********Train Epoch 14: averaged Loss: 2.196475
2023-07-05 17:22: Train Epoch 15: 220/323 Loss: 2.551085
2023-07-05 17:23: Train Epoch 15: 240/323 Loss: 2.502444
2023-07-05 17:23: **********Val Epoch 14: average Loss: 2.190124
2023-07-05 17:23: *********************************Current best model saved!
2023-07-05 17:24: Train Epoch 15: 0/323 Loss: 2.118689
2023-07-05 17:24: Train Epoch 15: 260/323 Loss: 2.602521
2023-07-05 17:25: Train Epoch 15: 20/323 Loss: 2.127347
2023-07-05 17:25: Train Epoch 15: 280/323 Loss: 2.595961
2023-07-05 17:26: Train Epoch 15: 40/323 Loss: 2.095613
2023-07-05 17:27: Train Epoch 15: 300/323 Loss: 2.395325
2023-07-05 17:28: Train Epoch 15: 60/323 Loss: 2.317925
2023-07-05 17:28: Train Epoch 15: 320/323 Loss: 2.452742
2023-07-05 17:28: **********Train Epoch 15: averaged Loss: 2.463676
2023-07-05 17:29: Train Epoch 15: 80/323 Loss: 2.195730
2023-07-05 17:30: Train Epoch 15: 100/323 Loss: 2.257278
2023-07-05 17:30: **********Val Epoch 15: average Loss: 2.546454
2023-07-05 17:30: Train Epoch 16: 0/323 Loss: 2.443261
2023-07-05 17:32: Train Epoch 15: 120/323 Loss: 2.321816
2023-07-05 17:32: Train Epoch 16: 20/323 Loss: 2.445395
2023-07-05 17:33: Train Epoch 16: 40/323 Loss: 2.383870
2023-07-05 17:33: Train Epoch 15: 140/323 Loss: 2.255573
2023-07-05 17:34: Train Epoch 16: 60/323 Loss: 2.294389
2023-07-05 17:34: Train Epoch 15: 160/323 Loss: 2.198012
2023-07-05 17:35: Train Epoch 16: 80/323 Loss: 2.614506
2023-07-05 17:36: Train Epoch 15: 180/323 Loss: 2.395935
2023-07-05 17:37: Train Epoch 16: 100/323 Loss: 2.373197
2023-07-05 17:37: Train Epoch 15: 200/323 Loss: 2.367949
2023-07-05 17:38: Train Epoch 16: 120/323 Loss: 2.502867
2023-07-05 17:38: Train Epoch 15: 220/323 Loss: 2.295731
2023-07-05 17:39: Train Epoch 16: 140/323 Loss: 2.570944
2023-07-05 17:40: Train Epoch 15: 240/323 Loss: 2.190559
2023-07-05 17:40: Train Epoch 16: 160/323 Loss: 2.393239
2023-07-05 17:41: Train Epoch 15: 260/323 Loss: 2.241110
2023-07-05 17:42: Train Epoch 16: 180/323 Loss: 2.372421
2023-07-05 17:42: Train Epoch 15: 280/323 Loss: 2.084526
2023-07-05 17:43: Train Epoch 16: 200/323 Loss: 2.361228
2023-07-05 17:44: Train Epoch 15: 300/323 Loss: 2.052910
2023-07-05 17:44: Train Epoch 16: 220/323 Loss: 2.468895
2023-07-05 17:45: Train Epoch 15: 320/323 Loss: 2.339037
2023-07-05 17:45: **********Train Epoch 15: averaged Loss: 2.193832
2023-07-05 17:45: Train Epoch 16: 240/323 Loss: 2.559307
2023-07-05 17:47: Train Epoch 16: 260/323 Loss: 2.425072
2023-07-05 17:48: Train Epoch 16: 280/323 Loss: 2.251257
2023-07-05 17:48: **********Val Epoch 15: average Loss: 2.159577
2023-07-05 17:48: *********************************Current best model saved!
2023-07-05 17:48: Train Epoch 16: 0/323 Loss: 2.125345
2023-07-05 17:49: Train Epoch 16: 300/323 Loss: 2.258699
2023-07-05 17:50: Train Epoch 16: 20/323 Loss: 2.231327
2023-07-05 17:51: Train Epoch 16: 320/323 Loss: 2.593373
2023-07-05 17:51: **********Train Epoch 16: averaged Loss: 2.441152
2023-07-05 17:51: Train Epoch 16: 40/323 Loss: 2.220617
2023-07-05 17:52: Train Epoch 16: 60/323 Loss: 2.288249
2023-07-05 17:53: **********Val Epoch 16: average Loss: 2.553399
2023-07-05 17:53: Train Epoch 17: 0/323 Loss: 2.227122
2023-07-05 17:54: Train Epoch 16: 80/323 Loss: 2.354220
2023-07-05 17:54: Train Epoch 17: 20/323 Loss: 2.377603
2023-07-05 17:55: Train Epoch 16: 100/323 Loss: 2.232551
2023-07-05 17:56: Train Epoch 17: 40/323 Loss: 2.357285
2023-07-05 17:56: Train Epoch 16: 120/323 Loss: 2.402380
2023-07-05 17:57: Train Epoch 17: 60/323 Loss: 2.364802
2023-07-05 17:58: Train Epoch 16: 140/323 Loss: 2.147171
2023-07-05 17:58: Train Epoch 17: 80/323 Loss: 2.340836
2023-07-05 17:59: Train Epoch 16: 160/323 Loss: 2.119717
2023-07-05 18:00: Train Epoch 17: 100/323 Loss: 2.526583
2023-07-05 18:00: Train Epoch 16: 180/323 Loss: 2.128374
2023-07-05 18:01: Train Epoch 17: 120/323 Loss: 2.409920
2023-07-05 18:02: Train Epoch 16: 200/323 Loss: 2.095926
2023-07-05 18:02: Train Epoch 17: 140/323 Loss: 2.502342
2023-07-05 18:03: Train Epoch 16: 220/323 Loss: 2.216401
2023-07-05 18:03: Train Epoch 17: 160/323 Loss: 2.507566
2023-07-05 18:04: Train Epoch 17: 180/323 Loss: 2.632117
2023-07-05 18:05: Train Epoch 16: 240/323 Loss: 2.188226
2023-07-05 18:06: Train Epoch 17: 200/323 Loss: 2.836740
2023-07-05 18:06: Train Epoch 16: 260/323 Loss: 2.243333
2023-07-05 18:07: Train Epoch 17: 220/323 Loss: 2.453240
2023-07-05 18:07: Train Epoch 16: 280/323 Loss: 2.202580
2023-07-05 18:08: Train Epoch 17: 240/323 Loss: 2.606648
2023-07-05 18:09: Train Epoch 16: 300/323 Loss: 2.175809
2023-07-05 18:09: Train Epoch 17: 260/323 Loss: 2.410006
2023-07-05 18:10: Train Epoch 16: 320/323 Loss: 2.304711
2023-07-05 18:10: **********Train Epoch 16: averaged Loss: 2.184713
2023-07-05 18:11: Train Epoch 17: 280/323 Loss: 2.552212
2023-07-05 18:12: Train Epoch 17: 300/323 Loss: 2.619990
2023-07-05 18:13: **********Val Epoch 16: average Loss: 2.204258
2023-07-05 18:13: Train Epoch 17: 0/323 Loss: 2.380116
2023-07-05 18:13: Train Epoch 17: 320/323 Loss: 2.617850
2023-07-05 18:13: **********Train Epoch 17: averaged Loss: 2.483886
2023-07-05 18:14: Train Epoch 17: 20/323 Loss: 2.060316
2023-07-05 18:15: Train Epoch 17: 40/323 Loss: 2.150103
2023-07-05 18:15: **********Val Epoch 17: average Loss: 2.567632
2023-07-05 18:15: Train Epoch 18: 0/323 Loss: 2.363717
2023-07-05 18:17: Train Epoch 17: 60/323 Loss: 2.284258
2023-07-05 18:17: Train Epoch 18: 20/323 Loss: 2.315208
2023-07-05 18:18: Train Epoch 18: 40/323 Loss: 2.540039
2023-07-05 18:18: Train Epoch 17: 80/323 Loss: 2.239280
2023-07-05 18:19: Train Epoch 18: 60/323 Loss: 3.237736
2023-07-05 18:19: Train Epoch 17: 100/323 Loss: 2.236912
2023-07-05 18:20: Train Epoch 18: 80/323 Loss: 3.013648
2023-07-05 18:21: Train Epoch 17: 120/323 Loss: 2.255938
2023-07-05 18:22: Train Epoch 18: 100/323 Loss: 3.062951
2023-07-05 18:22: Train Epoch 17: 140/323 Loss: 2.162773
2023-07-05 18:23: Train Epoch 18: 120/323 Loss: 5.472254
2023-07-05 18:24: Train Epoch 17: 160/323 Loss: 2.038133
2023-07-05 18:24: Train Epoch 18: 140/323 Loss: 4.246489
2023-07-05 18:25: Train Epoch 17: 180/323 Loss: 2.151244
2023-07-05 18:26: Train Epoch 18: 160/323 Loss: 3.334098
2023-07-05 18:26: Train Epoch 17: 200/323 Loss: 2.092095
2023-07-05 18:27: Train Epoch 18: 180/323 Loss: 3.283493
2023-07-05 18:28: Train Epoch 17: 220/323 Loss: 2.201540
2023-07-05 18:28: Train Epoch 18: 200/323 Loss: 3.098113
2023-07-05 18:29: Train Epoch 17: 240/323 Loss: 2.334368
2023-07-05 18:29: Train Epoch 18: 220/323 Loss: 3.392787
2023-07-05 18:30: Train Epoch 17: 260/323 Loss: 2.136525
2023-07-05 18:31: Train Epoch 18: 240/323 Loss: 3.296727
2023-07-05 18:32: Train Epoch 17: 280/323 Loss: 2.276088
2023-07-05 18:32: Train Epoch 18: 260/323 Loss: 3.220245
2023-07-05 18:33: Train Epoch 18: 280/323 Loss: 3.481696
2023-07-05 18:33: Train Epoch 17: 300/323 Loss: 2.260178
2023-07-05 18:34: Train Epoch 18: 300/323 Loss: 3.099140
2023-07-05 18:34: Train Epoch 17: 320/323 Loss: 2.199486
2023-07-05 18:35: **********Train Epoch 17: averaged Loss: 2.176089
2023-07-05 18:35: Train Epoch 18: 320/323 Loss: 3.381323
2023-07-05 18:36: **********Train Epoch 18: averaged Loss: 3.409344
2023-07-05 18:37: **********Val Epoch 17: average Loss: 2.181044
2023-07-05 18:37: Train Epoch 18: 0/323 Loss: 2.335950
2023-07-05 18:38: **********Val Epoch 18: average Loss: 3.098772
2023-07-05 18:38: Train Epoch 19: 0/323 Loss: 2.742614
2023-07-05 18:39: Train Epoch 18: 20/323 Loss: 2.093861
2023-07-05 18:39: Train Epoch 19: 20/323 Loss: 3.147229
2023-07-05 18:40: Train Epoch 18: 40/323 Loss: 2.172553
2023-07-05 18:40: Train Epoch 19: 40/323 Loss: 3.535067
2023-07-05 18:41: Train Epoch 18: 60/323 Loss: 2.083677
2023-07-05 18:42: Train Epoch 19: 60/323 Loss: 2.998147
2023-07-05 18:43: Train Epoch 18: 80/323 Loss: 2.241621
2023-07-05 18:43: Train Epoch 19: 80/323 Loss: 2.973933
2023-07-05 18:44: Train Epoch 18: 100/323 Loss: 2.116751
2023-07-05 18:44: Train Epoch 19: 100/323 Loss: 3.035528
2023-07-05 18:45: Train Epoch 18: 120/323 Loss: 2.277187
2023-07-05 18:45: Train Epoch 19: 120/323 Loss: 3.242118
2023-07-05 18:46: Train Epoch 19: 140/323 Loss: 3.144293
2023-07-05 18:47: Train Epoch 18: 140/323 Loss: 2.210269
2023-07-05 18:48: Train Epoch 19: 160/323 Loss: 3.140114
2023-07-05 18:48: Train Epoch 18: 160/323 Loss: 2.117226
2023-07-05 18:49: Train Epoch 19: 180/323 Loss: 2.690589
2023-07-05 18:49: Train Epoch 18: 180/323 Loss: 2.188454
2023-07-05 18:50: Train Epoch 19: 200/323 Loss: 2.688060
2023-07-05 18:51: Train Epoch 18: 200/323 Loss: 2.090418
2023-07-05 18:51: Train Epoch 19: 220/323 Loss: 3.006567
2023-07-05 18:52: Train Epoch 18: 220/323 Loss: 2.246528
2023-07-05 18:53: Train Epoch 19: 240/323 Loss: 2.762683
2023-07-05 18:53: Train Epoch 18: 240/323 Loss: 2.233799
2023-07-05 18:54: Train Epoch 19: 260/323 Loss: 2.865215
2023-07-05 18:54: Train Epoch 18: 260/323 Loss: 2.117258
2023-07-05 18:55: Train Epoch 19: 280/323 Loss: 2.978992
2023-07-05 18:56: Train Epoch 18: 280/323 Loss: 2.251691
2023-07-05 18:56: Train Epoch 19: 300/323 Loss: 2.874816
2023-07-05 18:57: Train Epoch 18: 300/323 Loss: 2.195553
2023-07-05 18:58: Train Epoch 19: 320/323 Loss: 2.752337
2023-07-05 18:58: **********Train Epoch 19: averaged Loss: 3.012328
2023-07-05 18:59: Train Epoch 18: 320/323 Loss: 1.931114
2023-07-05 18:59: **********Train Epoch 18: averaged Loss: 2.164151
2023-07-05 19:00: **********Val Epoch 19: average Loss: 2.890974
2023-07-05 19:00: Train Epoch 20: 0/323 Loss: 3.106127
2023-07-05 19:01: **********Val Epoch 18: average Loss: 2.148934
2023-07-05 19:01: *********************************Current best model saved!
2023-07-05 19:01: Train Epoch 20: 20/323 Loss: 2.856305
2023-07-05 19:02: Train Epoch 19: 0/323 Loss: 2.018968
2023-07-05 19:03: Train Epoch 20: 40/323 Loss: 2.764122
2023-07-05 19:03: Train Epoch 19: 20/323 Loss: 2.087045
2023-07-05 19:04: Train Epoch 20: 60/323 Loss: 2.756128
2023-07-05 19:04: Train Epoch 19: 40/323 Loss: 2.198287
2023-07-05 19:05: Train Epoch 20: 80/323 Loss: 2.878038
2023-07-05 19:06: Train Epoch 19: 60/323 Loss: 2.172653
2023-07-05 19:07: Train Epoch 20: 100/323 Loss: 2.855420
2023-07-05 19:07: Train Epoch 19: 80/323 Loss: 1.978054
2023-07-05 19:08: Train Epoch 20: 120/323 Loss: 2.929189
2023-07-05 19:08: Train Epoch 19: 100/323 Loss: 2.201640
2023-07-05 19:09: Train Epoch 20: 140/323 Loss: 2.742346
2023-07-05 19:10: Train Epoch 19: 120/323 Loss: 2.102294
2023-07-05 19:10: Train Epoch 20: 160/323 Loss: 2.688079
2023-07-05 19:11: Train Epoch 19: 140/323 Loss: 2.126156
2023-07-05 19:11: Train Epoch 20: 180/323 Loss: 3.004640
2023-07-05 19:12: Train Epoch 19: 160/323 Loss: 2.222522
2023-07-05 19:13: Train Epoch 20: 200/323 Loss: 2.889862
2023-07-05 19:14: Train Epoch 19: 180/323 Loss: 2.047628
2023-07-05 19:14: Train Epoch 20: 220/323 Loss: 2.909870
2023-07-05 19:15: Train Epoch 19: 200/323 Loss: 2.126786
2023-07-05 19:15: Train Epoch 20: 240/323 Loss: 2.807292
2023-07-05 19:16: Train Epoch 20: 260/323 Loss: 2.886585
2023-07-05 19:16: Train Epoch 19: 220/323 Loss: 2.142317
2023-07-05 19:17: Train Epoch 20: 280/323 Loss: 2.785234
2023-07-05 19:18: Train Epoch 19: 240/323 Loss: 2.305717
2023-07-05 19:19: Train Epoch 20: 300/323 Loss: 2.884531
2023-07-05 19:19: Train Epoch 19: 260/323 Loss: 2.120684
2023-07-05 19:20: Train Epoch 20: 320/323 Loss: 2.902740
2023-07-05 19:20: **********Train Epoch 20: averaged Loss: 2.829792
2023-07-05 19:20: Train Epoch 19: 280/323 Loss: 2.255982
2023-07-05 19:22: Train Epoch 19: 300/323 Loss: 2.272848
2023-07-05 19:22: **********Val Epoch 20: average Loss: 2.842171
2023-07-05 19:22: Train Epoch 21: 0/323 Loss: 2.836447
2023-07-05 19:23: Train Epoch 19: 320/323 Loss: 2.081377
2023-07-05 19:23: **********Train Epoch 19: averaged Loss: 2.155640
2023-07-05 19:24: Train Epoch 21: 20/323 Loss: 2.579672
2023-07-05 19:25: Train Epoch 21: 40/323 Loss: 2.800884
2023-07-05 19:26: **********Val Epoch 19: average Loss: 2.168170
2023-07-05 19:26: Train Epoch 20: 0/323 Loss: 2.139745
2023-07-05 19:26: Train Epoch 21: 60/323 Loss: 2.675870
2023-07-05 19:27: Train Epoch 20: 20/323 Loss: 2.213670
2023-07-05 19:27: Train Epoch 21: 80/323 Loss: 2.832378
2023-07-05 19:28: Train Epoch 20: 40/323 Loss: 2.299418
2023-07-05 19:28: Train Epoch 21: 100/323 Loss: 2.890697
2023-07-05 19:30: Train Epoch 21: 120/323 Loss: 2.941845
2023-07-05 19:30: Train Epoch 20: 60/323 Loss: 2.185255
2023-07-05 19:31: Train Epoch 21: 140/323 Loss: 2.874602
2023-07-05 19:31: Train Epoch 20: 80/323 Loss: 2.154307
2023-07-05 19:32: Train Epoch 21: 160/323 Loss: 2.729441
2023-07-05 19:32: Train Epoch 20: 100/323 Loss: 2.309439
2023-07-05 19:33: Train Epoch 21: 180/323 Loss: 2.915433
2023-07-05 19:34: Train Epoch 20: 120/323 Loss: 2.189237
2023-07-05 19:35: Train Epoch 21: 200/323 Loss: 2.961786
2023-07-05 19:35: Train Epoch 20: 140/323 Loss: 2.309445
2023-07-05 19:36: Train Epoch 21: 220/323 Loss: 2.819047
2023-07-05 19:37: Train Epoch 20: 160/323 Loss: 2.099239
2023-07-05 19:37: Train Epoch 21: 240/323 Loss: 2.785613
2023-07-05 19:38: Train Epoch 20: 180/323 Loss: 2.158881
2023-07-05 19:39: Train Epoch 21: 260/323 Loss: 2.781191
2023-07-05 19:39: Train Epoch 20: 200/323 Loss: 2.000138
2023-07-05 19:40: Train Epoch 21: 280/323 Loss: 2.772562
2023-07-05 19:41: Train Epoch 20: 220/323 Loss: 2.093787
2023-07-05 19:41: Train Epoch 21: 300/323 Loss: 2.683384
2023-07-05 19:42: Train Epoch 20: 240/323 Loss: 2.126438
2023-07-05 19:42: Train Epoch 21: 320/323 Loss: 2.894680
2023-07-05 19:42: **********Train Epoch 21: averaged Loss: 2.811013
2023-07-05 19:43: Train Epoch 20: 260/323 Loss: 2.148867
2023-07-05 19:45: Train Epoch 20: 280/323 Loss: 2.271235
2023-07-05 19:45: **********Val Epoch 21: average Loss: 2.781147
2023-07-05 19:45: Train Epoch 22: 0/323 Loss: 3.048750
2023-07-05 19:46: Train Epoch 20: 300/323 Loss: 2.098485
2023-07-05 19:46: Train Epoch 22: 20/323 Loss: 2.835294
2023-07-05 19:47: Train Epoch 22: 40/323 Loss: 2.676868
2023-07-05 19:47: Train Epoch 20: 320/323 Loss: 2.161446
2023-07-05 19:48: **********Train Epoch 20: averaged Loss: 2.157969
2023-07-05 19:48: Train Epoch 22: 60/323 Loss: 3.109773
2023-07-05 19:50: Train Epoch 22: 80/323 Loss: 2.586608
2023-07-05 19:50: **********Val Epoch 20: average Loss: 2.159540
2023-07-05 19:50: Train Epoch 21: 0/323 Loss: 2.000902
2023-07-05 19:51: Train Epoch 22: 100/323 Loss: 2.722339
2023-07-05 19:51: Train Epoch 21: 20/323 Loss: 2.281636
2023-07-05 19:52: Train Epoch 22: 120/323 Loss: 2.729405
2023-07-05 19:53: Train Epoch 21: 40/323 Loss: 2.318295
2023-07-05 19:53: Train Epoch 22: 140/323 Loss: 2.746255
2023-07-05 19:54: Train Epoch 21: 60/323 Loss: 2.148253
2023-07-05 19:55: Train Epoch 22: 160/323 Loss: 2.590392
2023-07-05 19:55: Train Epoch 21: 80/323 Loss: 2.104015
2023-07-05 19:56: Train Epoch 22: 180/323 Loss: 2.641323
2023-07-05 19:57: Train Epoch 21: 100/323 Loss: 2.229651
2023-07-05 19:57: Train Epoch 22: 200/323 Loss: 2.948919
2023-07-05 19:58: Train Epoch 21: 120/323 Loss: 2.229489
2023-07-05 19:58: Train Epoch 22: 220/323 Loss: 3.114017
2023-07-05 19:59: Train Epoch 22: 240/323 Loss: 2.850670
2023-07-05 19:59: Train Epoch 21: 140/323 Loss: 2.011118
2023-07-05 20:01: Train Epoch 22: 260/323 Loss: 2.673637
2023-07-05 20:01: Train Epoch 21: 160/323 Loss: 2.395847
2023-07-05 20:02: Train Epoch 22: 280/323 Loss: 2.922200
2023-07-05 20:02: Train Epoch 21: 180/323 Loss: 2.202843
2023-07-05 20:03: Train Epoch 22: 300/323 Loss: 2.592190
2023-07-05 20:03: Train Epoch 21: 200/323 Loss: 2.186895
2023-07-05 20:04: Train Epoch 22: 320/323 Loss: 2.564739
2023-07-05 20:04: **********Train Epoch 22: averaged Loss: 2.749166
2023-07-05 20:05: Train Epoch 21: 220/323 Loss: 2.054098
2023-07-05 20:06: Train Epoch 21: 240/323 Loss: 2.127481
2023-07-05 20:07: **********Val Epoch 22: average Loss: 2.776261
2023-07-05 20:07: Train Epoch 23: 0/323 Loss: 2.727402
2023-07-05 20:07: Train Epoch 21: 260/323 Loss: 2.221709
2023-07-05 20:08: Train Epoch 23: 20/323 Loss: 2.628673
2023-07-05 20:09: Train Epoch 21: 280/323 Loss: 2.119712
2023-07-05 20:09: Train Epoch 23: 40/323 Loss: 2.585857
2023-07-05 20:10: Train Epoch 21: 300/323 Loss: 2.094998
2023-07-05 20:11: Train Epoch 23: 60/323 Loss: 2.735286
2023-07-05 20:12: Train Epoch 21: 320/323 Loss: 2.186635
2023-07-05 20:12: **********Train Epoch 21: averaged Loss: 2.158445
2023-07-05 20:12: Train Epoch 23: 80/323 Loss: 2.998929
2023-07-05 20:13: Train Epoch 23: 100/323 Loss: 2.797172
2023-07-05 20:14: Train Epoch 23: 120/323 Loss: 2.781915
2023-07-05 20:14: **********Val Epoch 21: average Loss: 2.151674
2023-07-05 20:15: Train Epoch 22: 0/323 Loss: 2.303764
2023-07-05 20:16: Train Epoch 23: 140/323 Loss: 2.774261
2023-07-05 20:16: Train Epoch 22: 20/323 Loss: 2.089088
2023-07-05 20:17: Train Epoch 23: 160/323 Loss: 2.731832
2023-07-05 20:17: Train Epoch 22: 40/323 Loss: 2.104335
2023-07-05 20:18: Train Epoch 23: 180/323 Loss: 2.744653
2023-07-05 20:19: Train Epoch 22: 60/323 Loss: 2.186843
2023-07-05 20:19: Train Epoch 23: 200/323 Loss: 2.815986
2023-07-05 20:20: Train Epoch 22: 80/323 Loss: 2.150085
2023-07-05 20:21: Train Epoch 23: 220/323 Loss: 2.867207
2023-07-05 20:21: Train Epoch 22: 100/323 Loss: 2.090895
2023-07-05 20:22: Train Epoch 23: 240/323 Loss: 2.633776
2023-07-05 20:23: Train Epoch 22: 120/323 Loss: 2.114353
2023-07-05 20:23: Train Epoch 23: 260/323 Loss: 2.648447
2023-07-05 20:24: Train Epoch 22: 140/323 Loss: 2.267291
2023-07-05 20:24: Train Epoch 23: 280/323 Loss: 2.704905
2023-07-05 20:25: Train Epoch 22: 160/323 Loss: 2.130404
2023-07-05 20:25: Train Epoch 23: 300/323 Loss: 2.554503
2023-07-05 20:26: Train Epoch 22: 180/323 Loss: 2.236681
2023-07-05 20:27: Train Epoch 23: 320/323 Loss: 2.821542
2023-07-05 20:27: **********Train Epoch 23: averaged Loss: 2.720718
2023-07-05 20:28: Train Epoch 22: 200/323 Loss: 2.117270
2023-07-05 20:29: Train Epoch 22: 220/323 Loss: 2.005167
2023-07-05 20:29: **********Val Epoch 23: average Loss: 2.750625
2023-07-05 20:29: Train Epoch 24: 0/323 Loss: 2.801140
2023-07-05 20:30: Train Epoch 22: 240/323 Loss: 2.282949
2023-07-05 20:30: Train Epoch 24: 20/323 Loss: 2.799162
2023-07-05 20:31: Train Epoch 24: 40/323 Loss: 2.868508
2023-07-05 20:31: Train Epoch 22: 260/323 Loss: 2.126848
2023-07-05 20:33: Train Epoch 24: 60/323 Loss: 2.620027
2023-07-05 20:33: Train Epoch 22: 280/323 Loss: 2.094931
2023-07-05 20:34: Train Epoch 24: 80/323 Loss: 2.662161
2023-07-05 20:34: Train Epoch 22: 300/323 Loss: 2.176760
2023-07-05 20:35: Train Epoch 24: 100/323 Loss: 2.676319
2023-07-05 20:35: Train Epoch 22: 320/323 Loss: 2.047304
2023-07-05 20:35: **********Train Epoch 22: averaged Loss: 2.151566
2023-07-05 20:36: Train Epoch 24: 120/323 Loss: 2.748000
2023-07-05 20:37: Train Epoch 24: 140/323 Loss: 2.743036
2023-07-05 20:38: **********Val Epoch 22: average Loss: 2.138348
2023-07-05 20:38: *********************************Current best model saved!
2023-07-05 20:38: Train Epoch 23: 0/323 Loss: 2.101548
2023-07-05 20:38: Train Epoch 24: 160/323 Loss: 2.598542
2023-07-05 20:39: Train Epoch 23: 20/323 Loss: 2.044583
2023-07-05 20:39: Train Epoch 24: 180/323 Loss: 2.872940
2023-07-05 20:41: Train Epoch 23: 40/323 Loss: 2.094036
2023-07-05 20:41: Train Epoch 24: 200/323 Loss: 2.680940
2023-07-05 20:42: Train Epoch 23: 60/323 Loss: 2.011899
2023-07-05 20:42: Train Epoch 24: 220/323 Loss: 2.753762
2023-07-05 20:43: Train Epoch 24: 240/323 Loss: 2.960669
2023-07-05 20:43: Train Epoch 23: 80/323 Loss: 2.185715
2023-07-05 20:44: Train Epoch 24: 260/323 Loss: 5.032644
2023-07-05 20:45: Train Epoch 23: 100/323 Loss: 2.158078
2023-07-05 20:46: Train Epoch 24: 280/323 Loss: 6.675664
2023-07-05 20:46: Train Epoch 23: 120/323 Loss: 2.143520
2023-07-05 20:47: Train Epoch 24: 300/323 Loss: 3.924278
2023-07-05 20:47: Train Epoch 23: 140/323 Loss: 2.186044
2023-07-05 20:48: Train Epoch 24: 320/323 Loss: 3.764794
2023-07-05 20:48: **********Train Epoch 24: averaged Loss: 3.255984
2023-07-05 20:49: Train Epoch 23: 160/323 Loss: 2.100056
2023-07-05 20:50: Train Epoch 23: 180/323 Loss: 2.197846
2023-07-05 20:50: **********Val Epoch 24: average Loss: 3.538556
2023-07-05 20:51: Train Epoch 25: 0/323 Loss: 3.633677
2023-07-05 20:51: Train Epoch 23: 200/323 Loss: 2.225968
2023-07-05 20:52: Train Epoch 25: 20/323 Loss: 3.287008
2023-07-05 20:52: Train Epoch 23: 220/323 Loss: 2.031923
2023-07-05 20:53: Train Epoch 25: 40/323 Loss: 3.641527
2023-07-05 20:54: Train Epoch 23: 240/323 Loss: 2.259998
2023-07-05 20:54: Train Epoch 25: 60/323 Loss: 3.102246
2023-07-05 20:55: Train Epoch 23: 260/323 Loss: 2.006362
2023-07-05 20:55: Train Epoch 25: 80/323 Loss: 3.114372
2023-07-05 20:56: Train Epoch 23: 280/323 Loss: 2.201549
2023-07-05 20:57: Train Epoch 25: 100/323 Loss: 3.354402
2023-07-05 20:58: Train Epoch 23: 300/323 Loss: 2.042451
2023-07-05 20:58: Train Epoch 25: 120/323 Loss: 2.714156
2023-07-05 20:59: Train Epoch 25: 140/323 Loss: 3.152672
2023-07-05 20:59: Train Epoch 23: 320/323 Loss: 2.281044
2023-07-05 20:59: **********Train Epoch 23: averaged Loss: 2.143227
2023-07-05 21:00: Train Epoch 25: 160/323 Loss: 3.155885
2023-07-05 21:01: Train Epoch 25: 180/323 Loss: 3.335330
2023-07-05 21:02: **********Val Epoch 23: average Loss: 2.122987
2023-07-05 21:02: *********************************Current best model saved!
2023-07-05 21:02: Train Epoch 24: 0/323 Loss: 2.148543
2023-07-05 21:02: Train Epoch 25: 200/323 Loss: 3.043960
2023-07-05 21:03: Train Epoch 24: 20/323 Loss: 2.099925
2023-07-05 21:03: Train Epoch 25: 220/323 Loss: 2.892381
2023-07-05 21:04: Train Epoch 24: 40/323 Loss: 2.014580
2023-07-05 21:05: Train Epoch 25: 240/323 Loss: 3.169033
2023-07-05 21:05: Train Epoch 24: 60/323 Loss: 2.191109
2023-07-05 21:06: Train Epoch 25: 260/323 Loss: 2.977855
2023-07-05 21:07: Train Epoch 24: 80/323 Loss: 2.019409
2023-07-05 21:07: Train Epoch 25: 280/323 Loss: 2.818908
2023-07-05 21:08: Train Epoch 24: 100/323 Loss: 2.124969
2023-07-05 21:08: Train Epoch 25: 300/323 Loss: 2.809860
2023-07-05 21:09: Train Epoch 25: 320/323 Loss: 3.074485
2023-07-05 21:09: Train Epoch 24: 120/323 Loss: 2.323947
2023-07-05 21:09: **********Train Epoch 25: averaged Loss: 3.117202
2023-07-05 21:11: Train Epoch 24: 140/323 Loss: 2.213376
2023-07-05 21:12: **********Val Epoch 25: average Loss: 3.513854
2023-07-05 21:12: Train Epoch 26: 0/323 Loss: 2.923779
2023-07-05 21:12: Train Epoch 24: 160/323 Loss: 2.141320
2023-07-05 21:13: Train Epoch 26: 20/323 Loss: 2.684347
2023-07-05 21:13: Train Epoch 24: 180/323 Loss: 2.209569
2023-07-05 21:14: Train Epoch 26: 40/323 Loss: 2.989130
2023-07-05 21:14: Train Epoch 24: 200/323 Loss: 2.200904
2023-07-05 21:15: Train Epoch 26: 60/323 Loss: 2.841479
2023-07-05 21:16: Train Epoch 24: 220/323 Loss: 2.190250
2023-07-05 21:17: Train Epoch 26: 80/323 Loss: 2.812711
2023-07-05 21:17: Train Epoch 24: 240/323 Loss: 2.146420
2023-07-05 21:18: Train Epoch 26: 100/323 Loss: 2.684923
2023-07-05 21:18: Train Epoch 24: 260/323 Loss: 2.230633
2023-07-05 21:19: Train Epoch 26: 120/323 Loss: 2.821311
2023-07-05 21:20: Train Epoch 24: 280/323 Loss: 2.056455
2023-07-05 21:20: Train Epoch 26: 140/323 Loss: 2.948617
2023-07-05 21:21: Train Epoch 24: 300/323 Loss: 2.062291
2023-07-05 21:21: Train Epoch 26: 160/323 Loss: 2.825959
2023-07-05 21:22: Train Epoch 24: 320/323 Loss: 2.112971
2023-07-05 21:23: **********Train Epoch 24: averaged Loss: 2.133282
2023-07-05 21:23: Train Epoch 26: 180/323 Loss: 2.882332
2023-07-05 21:24: Train Epoch 26: 200/323 Loss: 3.185000
2023-07-05 21:25: Train Epoch 26: 220/323 Loss: 2.843594
2023-07-05 21:25: **********Val Epoch 24: average Loss: 2.120213
2023-07-05 21:25: *********************************Current best model saved!
2023-07-05 21:25: Train Epoch 25: 0/323 Loss: 2.078109
2023-07-05 21:26: Train Epoch 26: 240/323 Loss: 3.108467
2023-07-05 21:26: Train Epoch 25: 20/323 Loss: 2.107758
2023-07-05 21:27: Train Epoch 26: 260/323 Loss: 2.926332
2023-07-05 21:28: Train Epoch 25: 40/323 Loss: 2.053027
2023-07-05 21:28: Train Epoch 26: 280/323 Loss: 2.889815
2023-07-05 21:29: Train Epoch 25: 60/323 Loss: 2.243588
2023-07-05 21:30: Train Epoch 26: 300/323 Loss: 2.996904
2023-07-05 21:30: Train Epoch 25: 80/323 Loss: 2.121658
2023-07-05 21:31: Train Epoch 26: 320/323 Loss: 2.978518
2023-07-05 21:31: **********Train Epoch 26: averaged Loss: 2.896193
2023-07-05 21:31: Train Epoch 25: 100/323 Loss: 2.130320
2023-07-05 21:33: Train Epoch 25: 120/323 Loss: 2.154764
2023-07-05 21:33: **********Val Epoch 26: average Loss: 2.959347
2023-07-05 21:33: Train Epoch 27: 0/323 Loss: 2.826234
2023-07-05 21:34: Train Epoch 25: 140/323 Loss: 2.125542
2023-07-05 21:34: Train Epoch 27: 20/323 Loss: 3.100428
2023-07-05 21:35: Train Epoch 25: 160/323 Loss: 2.104522
2023-07-05 21:36: Train Epoch 27: 40/323 Loss: 2.786332
2023-07-05 21:37: Train Epoch 25: 180/323 Loss: 2.222202
2023-07-05 21:37: Train Epoch 27: 60/323 Loss: 2.741627
2023-07-05 21:38: Train Epoch 25: 200/323 Loss: 2.266499
2023-07-05 21:38: Train Epoch 27: 80/323 Loss: 2.793646
2023-07-05 21:39: Train Epoch 25: 220/323 Loss: 2.147174
2023-07-05 21:39: Train Epoch 27: 100/323 Loss: 2.620990
2023-07-05 21:40: Train Epoch 27: 120/323 Loss: 2.728616
2023-07-05 21:40: Train Epoch 25: 240/323 Loss: 2.094244
2023-07-05 21:41: Train Epoch 27: 140/323 Loss: 2.665553
2023-07-05 21:42: Train Epoch 25: 260/323 Loss: 2.082483
2023-07-05 21:43: Train Epoch 27: 160/323 Loss: 2.666060
2023-07-05 21:43: Train Epoch 25: 280/323 Loss: 2.053359
2023-07-05 21:44: Train Epoch 27: 180/323 Loss: 2.664328
2023-07-05 21:44: Train Epoch 25: 300/323 Loss: 2.291502
2023-07-05 21:45: Train Epoch 27: 200/323 Loss: 2.883627
2023-07-05 21:46: Train Epoch 25: 320/323 Loss: 2.158474
2023-07-05 21:46: **********Train Epoch 25: averaged Loss: 2.130199
2023-07-05 21:46: Train Epoch 27: 220/323 Loss: 2.846002
2023-07-05 21:47: Train Epoch 27: 240/323 Loss: 2.649299
2023-07-05 21:48: **********Val Epoch 25: average Loss: 2.122291
2023-07-05 21:48: Train Epoch 26: 0/323 Loss: 2.111285
2023-07-05 21:48: Train Epoch 27: 260/323 Loss: 2.703151
2023-07-05 21:50: Train Epoch 26: 20/323 Loss: 2.216242
2023-07-05 21:50: Train Epoch 27: 280/323 Loss: 2.735342
2023-07-05 21:51: Train Epoch 27: 300/323 Loss: 2.716892
2023-07-05 21:51: Train Epoch 26: 40/323 Loss: 2.195404
2023-07-05 21:52: Train Epoch 27: 320/323 Loss: 2.858330
2023-07-05 21:52: **********Train Epoch 27: averaged Loss: 2.805015
2023-07-05 21:52: Train Epoch 26: 60/323 Loss: 2.059031
2023-07-05 21:54: Train Epoch 26: 80/323 Loss: 2.268647
2023-07-05 21:55: **********Val Epoch 27: average Loss: 2.970970
2023-07-05 21:55: Validation performance didn't improve for 15 epochs. Training stops.
2023-07-05 21:55: Total training time: 523.9807min, best loss: 2.510635
2023-07-05 21:55: Saving current best model to ../runs/chengdu/07-05-13h10m_chengdu_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/best_model.pth
2023-07-05 21:55: Train Epoch 26: 100/323 Loss: 2.105798
2023-07-05 21:56: Train Epoch 26: 120/323 Loss: 2.203896
2023-07-05 21:57: Horizon 01, MAE: 1.98, RMSE: 2.90, MAPE: 8.5780, SMAPE: 3.9865, WMAPE: 6.9915%
2023-07-05 21:57: Horizon 02, MAE: 2.27, RMSE: 3.38, MAPE: 10.2424, SMAPE: 4.5894, WMAPE: 8.0165%
2023-07-05 21:57: Horizon 03, MAE: 2.38, RMSE: 3.56, MAPE: 10.8702, SMAPE: 4.8022, WMAPE: 8.4056%
2023-07-05 21:57: Horizon 04, MAE: 2.43, RMSE: 3.66, MAPE: 11.2230, SMAPE: 4.8941, WMAPE: 8.5705%
2023-07-05 21:57: Horizon 05, MAE: 2.47, RMSE: 3.73, MAPE: 11.4955, SMAPE: 4.9764, WMAPE: 8.7199%
2023-07-05 21:57: Horizon 06, MAE: 2.51, RMSE: 3.80, MAPE: 11.7615, SMAPE: 5.0699, WMAPE: 8.8799%
2023-07-05 21:57: Horizon 07, MAE: 2.55, RMSE: 3.84, MAPE: 11.9417, SMAPE: 5.1311, WMAPE: 8.9974%
2023-07-05 21:57: Horizon 08, MAE: 2.58, RMSE: 3.88, MAPE: 12.0720, SMAPE: 5.1909, WMAPE: 9.1128%
2023-07-05 21:57: Horizon 09, MAE: 2.61, RMSE: 3.92, MAPE: 12.2826, SMAPE: 5.2507, WMAPE: 9.2108%
2023-07-05 21:57: Horizon 10, MAE: 2.64, RMSE: 3.98, MAPE: 12.4735, SMAPE: 5.3105, WMAPE: 9.3246%
2023-07-05 21:57: Horizon 11, MAE: 2.68, RMSE: 4.06, MAPE: 12.7676, SMAPE: 5.3932, WMAPE: 9.4874%
2023-07-05 21:57: Horizon 12, MAE: 2.72, RMSE: 4.10, MAPE: 12.9393, SMAPE: 5.4615, WMAPE: 9.6153%
2023-07-05 21:57: Average Horizon, MAE: 2.48, RMSE: 3.75, MAPE: 11.5540, SMAPE: 5.0047, WMAPE: 8.7775%
/home/assassin/TRC/STG-NCDE-main
Namespace(dataset='chengdu', mode='train', device=1, debug=False, model='GCDE', cuda=True, comment='', val_ratio=0.2, test_ratio=0.2, lag=12, horizon=12, num_nodes=524, tod=False, normalizer='std', column_wise=False, default_graph=True, model_type='type1', g_type='agc', input_dim=2, output_dim=1, embed_dim=10, hid_dim=64, hid_hid_dim=64, num_layers=2, cheb_k=2, solver='rk4', loss_func='mae', seed=0, batch_size=32, epochs=200, lr_init=0.001, weight_decay=0.001, lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', early_stop=True, early_stop_patience=15, grad_norm=False, max_grad_norm=5, teacher_forcing=False, real_value=True, missing_test=False, missing_rate=0.1, mae_thresh=None, mape_thresh=0.0, model_path='', log_dir='../runs', log_step=20, plot=False, tensorboard=True)
has model save path
NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
*****************Model Parameter*****************
node_embeddings torch.Size([524, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([524, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 381244
*****************Finish Parameter****************
Load chengdu Dataset shaped:  (17280, 524, 1) 104.85800000005906 5.000000000039382 29.11107818401029 28.144900000043943
Normalize the dataset by Standard Normalization
Train:  (10345, 12, 524, 1) (10345, 12, 524, 1)
Val:  (3433, 12, 524, 1) (3433, 12, 524, 1)
Test:  (3433, 12, 524, 1) (3433, 12, 524, 1)
Creat Log File in:  ../runs/chengdu/07-05-13h10m_chengdu_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/run.log
*****************Model Parameter*****************
node_embeddings torch.Size([524, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([524, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 381244
*****************Finish Parameter****************
an epoch time:  455.95662021636963
an epoch time:  484.9150857925415
an epoch time:  468.7442021369934
an epoch time:  467.6750283241272
an epoch time:  446.9512701034546
an epoch time:  673.7127208709717
an epoch time:  1214.9677202701569
an epoch time:  1232.08549118042
an epoch time:  1183.907954454422
an epoch time:  1217.2376012802124
an epoch time:  1187.6712617874146
an epoch time:  1211.4299550056458
an epoch time:  1223.8212912082672
an epoch time:  1190.1951541900635
an epoch time:  1234.1869804859161
an epoch time:  1225.7531955242157
an epoch time:  1185.06600689888
an epoch time:  1211.3742010593414
an epoch time:  1198.0900201797485
an epoch time:  1189.6379001140594
an epoch time:  1202.5299189090729
an epoch time:  1176.1309468746185
an epoch time:  1189.4251520633698
an epoch time:  1151.9718379974365
an epoch time:  1136.2776312828064
an epoch time:  1155.1107320785522
an epoch time:  1147.1553394794464
2023-07-05 21:57: Train Epoch 26: 140/323 Loss: 2.010132
2023-07-05 21:58: Train Epoch 26: 160/323 Loss: 1.951013
2023-07-05 21:59: Train Epoch 26: 180/323 Loss: 1.961370
2023-07-05 22:00: Train Epoch 26: 200/323 Loss: 1.956570
2023-07-05 22:01: Train Epoch 26: 220/323 Loss: 2.033412
2023-07-05 22:02: Train Epoch 26: 240/323 Loss: 2.242182
2023-07-05 22:03: Train Epoch 26: 260/323 Loss: 2.204320
2023-07-05 22:04: Train Epoch 26: 280/323 Loss: 2.141516
2023-07-05 22:05: Train Epoch 26: 300/323 Loss: 2.020932
2023-07-05 22:06: Train Epoch 26: 320/323 Loss: 2.154147
2023-07-05 22:06: **********Train Epoch 26: averaged Loss: 2.134031
2023-07-05 22:08: **********Val Epoch 26: average Loss: 2.134933
2023-07-05 22:08: Train Epoch 27: 0/323 Loss: 2.103165
2023-07-05 22:09: Train Epoch 27: 20/323 Loss: 2.018213
2023-07-05 22:10: Train Epoch 27: 40/323 Loss: 2.052879
2023-07-05 22:11: Train Epoch 27: 60/323 Loss: 2.113855
2023-07-05 22:12: Train Epoch 27: 80/323 Loss: 2.098348
2023-07-05 22:13: Train Epoch 27: 100/323 Loss: 2.101669
2023-07-05 22:14: Train Epoch 27: 120/323 Loss: 2.151513
2023-07-05 22:15: Train Epoch 27: 140/323 Loss: 1.903266
2023-07-05 22:16: Train Epoch 27: 160/323 Loss: 2.073704
2023-07-05 22:17: Train Epoch 27: 180/323 Loss: 2.107114
2023-07-05 22:18: Train Epoch 27: 200/323 Loss: 1.988192
2023-07-05 22:19: Train Epoch 27: 220/323 Loss: 2.479358
2023-07-05 22:20: Train Epoch 27: 240/323 Loss: 2.320856
2023-07-05 22:21: Train Epoch 27: 260/323 Loss: 2.787978
2023-07-05 22:22: Train Epoch 27: 280/323 Loss: 2.284776
2023-07-05 22:23: Train Epoch 27: 300/323 Loss: 2.215883
2023-07-05 22:24: Train Epoch 27: 320/323 Loss: 2.539404
2023-07-05 22:24: **********Train Epoch 27: averaged Loss: 2.236775
2023-07-05 22:26: **********Val Epoch 27: average Loss: 2.550723
2023-07-05 22:26: Train Epoch 28: 0/323 Loss: 2.505956
2023-07-05 22:27: Train Epoch 28: 20/323 Loss: 2.931750
2023-07-05 22:28: Train Epoch 28: 40/323 Loss: 3.081059
2023-07-05 22:29: Train Epoch 28: 60/323 Loss: 29.502560
2023-07-05 22:30: Train Epoch 28: 80/323 Loss: 18.392719
2023-07-05 22:31: Train Epoch 28: 100/323 Loss: 11.395520
2023-07-05 22:32: Train Epoch 28: 120/323 Loss: 8.174956
2023-07-05 22:33: Train Epoch 28: 140/323 Loss: 6.702945
2023-07-05 22:34: Train Epoch 28: 160/323 Loss: 6.354008
2023-07-05 22:35: Train Epoch 28: 180/323 Loss: 5.466514
2023-07-05 22:36: Train Epoch 28: 200/323 Loss: 4.916108
2023-07-05 22:37: Train Epoch 28: 220/323 Loss: 4.649242
2023-07-05 22:38: Train Epoch 28: 240/323 Loss: 4.795340
2023-07-05 22:39: Train Epoch 28: 260/323 Loss: 4.383928
2023-07-05 22:40: Train Epoch 28: 280/323 Loss: 4.085145
2023-07-05 22:41: Train Epoch 28: 300/323 Loss: 4.205911
2023-07-05 22:42: Train Epoch 28: 320/323 Loss: 4.195163
2023-07-05 22:42: **********Train Epoch 28: averaged Loss: 7.120069
2023-07-05 22:44: **********Val Epoch 28: average Loss: 4.008573
2023-07-05 22:44: Train Epoch 29: 0/323 Loss: 4.037823
2023-07-05 22:45: Train Epoch 29: 20/323 Loss: 4.152332
2023-07-05 22:46: Train Epoch 29: 40/323 Loss: 3.969544
2023-07-05 22:47: Train Epoch 29: 60/323 Loss: 3.692527
2023-07-05 22:48: Train Epoch 29: 80/323 Loss: 3.666095
2023-07-05 22:49: Train Epoch 29: 100/323 Loss: 3.543553
2023-07-05 22:50: Train Epoch 29: 120/323 Loss: 3.339121
2023-07-05 22:51: Train Epoch 29: 140/323 Loss: 4.141620
2023-07-05 22:52: Train Epoch 29: 160/323 Loss: 3.255303
2023-07-05 22:53: Train Epoch 29: 180/323 Loss: 3.771660
2023-07-05 22:54: Train Epoch 29: 200/323 Loss: 3.555356
2023-07-05 22:55: Train Epoch 29: 220/323 Loss: 3.213110
2023-07-05 22:56: Train Epoch 29: 240/323 Loss: 3.420661
2023-07-05 22:57: Train Epoch 29: 260/323 Loss: 3.879091
2023-07-05 22:58: Train Epoch 29: 280/323 Loss: 3.459196
2023-07-05 22:59: Train Epoch 29: 300/323 Loss: 3.403503
2023-07-05 23:00: Train Epoch 29: 320/323 Loss: 3.345248
2023-07-05 23:00: **********Train Epoch 29: averaged Loss: 3.644191
2023-07-05 23:02: **********Val Epoch 29: average Loss: 3.379807
2023-07-05 23:02: Train Epoch 30: 0/323 Loss: 3.481927
2023-07-05 23:03: Train Epoch 30: 20/323 Loss: 3.537722
2023-07-05 23:04: Train Epoch 30: 40/323 Loss: 3.240147
2023-07-05 23:05: Train Epoch 30: 60/323 Loss: 3.307197
2023-07-05 23:06: Train Epoch 30: 80/323 Loss: 3.427959
2023-07-05 23:07: Train Epoch 30: 100/323 Loss: 3.455471
2023-07-05 23:08: Train Epoch 30: 120/323 Loss: 3.102205
2023-07-05 23:09: Train Epoch 30: 140/323 Loss: 3.310264
2023-07-05 23:10: Train Epoch 30: 160/323 Loss: 4.315825
2023-07-05 23:11: Train Epoch 30: 180/323 Loss: 4.266257
2023-07-05 23:12: Train Epoch 30: 200/323 Loss: 3.312622
2023-07-05 23:13: Train Epoch 30: 220/323 Loss: 3.509566
2023-07-05 23:14: Train Epoch 30: 240/323 Loss: 3.016187
2023-07-05 23:15: Train Epoch 30: 260/323 Loss: 3.495785
2023-07-05 23:16: Train Epoch 30: 280/323 Loss: 3.544358
2023-07-05 23:17: Train Epoch 30: 300/323 Loss: 3.005620
2023-07-05 23:18: Train Epoch 30: 320/323 Loss: 3.154187
2023-07-05 23:18: **********Train Epoch 30: averaged Loss: 3.391889
2023-07-05 23:20: **********Val Epoch 30: average Loss: 3.243401
2023-07-05 23:20: Train Epoch 31: 0/323 Loss: 3.137906
2023-07-05 23:21: Train Epoch 31: 20/323 Loss: 3.075950
2023-07-05 23:22: Train Epoch 31: 40/323 Loss: 3.312124
2023-07-05 23:23: Train Epoch 31: 60/323 Loss: 3.232491
2023-07-05 23:24: Train Epoch 31: 80/323 Loss: 3.101883
2023-07-05 23:25: Train Epoch 31: 100/323 Loss: 3.060552
2023-07-05 23:26: Train Epoch 31: 120/323 Loss: 3.409087
2023-07-05 23:27: Train Epoch 31: 140/323 Loss: 3.210864
2023-07-05 23:28: Train Epoch 31: 160/323 Loss: 3.218267
2023-07-05 23:29: Train Epoch 31: 180/323 Loss: 3.121794
2023-07-05 23:30: Train Epoch 31: 200/323 Loss: 3.142815
2023-07-05 23:31: Train Epoch 31: 220/323 Loss: 3.364721
2023-07-05 23:32: Train Epoch 31: 240/323 Loss: 3.199621
2023-07-05 23:33: Train Epoch 31: 260/323 Loss: 2.911137
2023-07-05 23:34: Train Epoch 31: 280/323 Loss: 3.380935
2023-07-05 23:35: Train Epoch 31: 300/323 Loss: 3.271060
2023-07-05 23:36: Train Epoch 31: 320/323 Loss: 3.025032
2023-07-05 23:36: **********Train Epoch 31: averaged Loss: 3.175551
2023-07-05 23:38: **********Val Epoch 31: average Loss: 3.231630
2023-07-05 23:38: Train Epoch 32: 0/323 Loss: 3.110776
2023-07-05 23:39: Train Epoch 32: 20/323 Loss: 3.169721
2023-07-05 23:40: Train Epoch 32: 40/323 Loss: 3.136535
2023-07-05 23:41: Train Epoch 32: 60/323 Loss: 3.535555
2023-07-05 23:42: Train Epoch 32: 80/323 Loss: 3.349827
2023-07-05 23:43: Train Epoch 32: 100/323 Loss: 3.311742
2023-07-05 23:44: Train Epoch 32: 120/323 Loss: 2.985368
2023-07-05 23:45: Train Epoch 32: 140/323 Loss: 3.183857
2023-07-05 23:46: Train Epoch 32: 160/323 Loss: 2.960039
2023-07-05 23:47: Train Epoch 32: 180/323 Loss: 2.986570
2023-07-05 23:48: Train Epoch 32: 200/323 Loss: 3.555665
2023-07-05 23:48: Train Epoch 32: 220/323 Loss: 3.856778
2023-07-05 23:49: Train Epoch 32: 240/323 Loss: 3.004899
2023-07-05 23:50: Train Epoch 32: 260/323 Loss: 3.129737
2023-07-05 23:51: Train Epoch 32: 280/323 Loss: 3.253495
2023-07-05 23:52: Train Epoch 32: 300/323 Loss: 2.902888
2023-07-05 23:53: Train Epoch 32: 320/323 Loss: 3.248997
2023-07-05 23:53: **********Train Epoch 32: averaged Loss: 3.205226
2023-07-05 23:55: **********Val Epoch 32: average Loss: 3.198509
2023-07-05 23:55: Train Epoch 33: 0/323 Loss: 3.276612
2023-07-05 23:56: Train Epoch 33: 20/323 Loss: 3.531494
2023-07-05 23:57: Train Epoch 33: 40/323 Loss: 3.220015
2023-07-05 23:58: Train Epoch 33: 60/323 Loss: 3.187093
2023-07-05 23:59: Train Epoch 33: 80/323 Loss: 3.055027
2023-07-06 00:00: Train Epoch 33: 100/323 Loss: 2.909034
2023-07-06 00:01: Train Epoch 33: 120/323 Loss: 2.775373
2023-07-06 00:02: Train Epoch 33: 140/323 Loss: 3.116241
2023-07-06 00:03: Train Epoch 33: 160/323 Loss: 3.272980
2023-07-06 00:04: Train Epoch 33: 180/323 Loss: 3.223736
2023-07-06 00:05: Train Epoch 33: 200/323 Loss: 3.026122
2023-07-06 00:06: Train Epoch 33: 220/323 Loss: 3.234485
2023-07-06 00:07: Train Epoch 33: 240/323 Loss: 2.957954
2023-07-06 00:08: Train Epoch 33: 260/323 Loss: 3.181130
2023-07-06 00:09: Train Epoch 33: 280/323 Loss: 2.948205
2023-07-06 00:10: Train Epoch 33: 300/323 Loss: 2.970154
2023-07-06 00:11: Train Epoch 33: 320/323 Loss: 3.099401
2023-07-06 00:11: **********Train Epoch 33: averaged Loss: 3.064981
2023-07-06 00:13: **********Val Epoch 33: average Loss: 3.081840
2023-07-06 00:13: Train Epoch 34: 0/323 Loss: 2.777561
2023-07-06 00:14: Train Epoch 34: 20/323 Loss: 3.017696
2023-07-06 00:15: Train Epoch 34: 40/323 Loss: 3.484404
2023-07-06 00:16: Train Epoch 34: 60/323 Loss: 2.806731
2023-07-06 00:17: Train Epoch 34: 80/323 Loss: 3.345442
2023-07-06 00:18: Train Epoch 34: 100/323 Loss: 3.048691
2023-07-06 00:19: Train Epoch 34: 120/323 Loss: 3.633001
2023-07-06 00:20: Train Epoch 34: 140/323 Loss: 3.013210
2023-07-06 00:21: Train Epoch 34: 160/323 Loss: 3.278341
2023-07-06 00:22: Train Epoch 34: 180/323 Loss: 3.100063
2023-07-06 00:23: Train Epoch 34: 200/323 Loss: 2.902168
2023-07-06 00:24: Train Epoch 34: 220/323 Loss: 3.093585
2023-07-06 00:25: Train Epoch 34: 240/323 Loss: 3.078603
2023-07-06 00:26: Train Epoch 34: 260/323 Loss: 3.187967
2023-07-06 00:27: Train Epoch 34: 280/323 Loss: 3.168036
2023-07-06 00:28: Train Epoch 34: 300/323 Loss: 3.082737
2023-07-06 00:29: Train Epoch 34: 320/323 Loss: 3.067624
2023-07-06 00:29: **********Train Epoch 34: averaged Loss: 3.096928
2023-07-06 00:31: **********Val Epoch 34: average Loss: 3.125209
2023-07-06 00:31: Train Epoch 35: 0/323 Loss: 3.316448
2023-07-06 00:32: Train Epoch 35: 20/323 Loss: 3.354685
2023-07-06 00:33: Train Epoch 35: 40/323 Loss: 2.742725
2023-07-06 00:34: Train Epoch 35: 60/323 Loss: 2.924513
2023-07-06 00:35: Train Epoch 35: 80/323 Loss: 3.043839
2023-07-06 00:36: Train Epoch 35: 100/323 Loss: 2.831739
2023-07-06 00:37: Train Epoch 35: 120/323 Loss: 2.902328
2023-07-06 00:38: Train Epoch 35: 140/323 Loss: 3.000159
2023-07-06 00:39: Train Epoch 35: 160/323 Loss: 2.814395
2023-07-06 00:40: Train Epoch 35: 180/323 Loss: 3.285124
2023-07-06 00:40: Train Epoch 35: 200/323 Loss: 2.883239
2023-07-06 00:40: Train Epoch 35: 220/323 Loss: 2.878474
2023-07-06 00:41: Train Epoch 35: 240/323 Loss: 2.707482
2023-07-06 00:41: Train Epoch 35: 260/323 Loss: 2.928114
2023-07-06 00:41: Train Epoch 35: 280/323 Loss: 3.355519
2023-07-06 00:41: Train Epoch 35: 300/323 Loss: 3.092978
2023-07-06 00:42: Train Epoch 35: 320/323 Loss: 2.867700
2023-07-06 00:42: **********Train Epoch 35: averaged Loss: 2.980063
2023-07-06 00:42: **********Val Epoch 35: average Loss: 2.968080
2023-07-06 00:42: Train Epoch 36: 0/323 Loss: 2.860002
2023-07-06 00:42: Train Epoch 36: 20/323 Loss: 3.207916
2023-07-06 00:43: Train Epoch 36: 40/323 Loss: 2.965911
2023-07-06 00:43: Train Epoch 36: 60/323 Loss: 2.840302
2023-07-06 00:43: Train Epoch 36: 80/323 Loss: 2.692128
2023-07-06 00:43: Train Epoch 36: 100/323 Loss: 3.054531
2023-07-06 00:43: Train Epoch 36: 120/323 Loss: 2.944779
2023-07-06 00:44: Train Epoch 36: 140/323 Loss: 2.850433
2023-07-06 00:44: Train Epoch 36: 160/323 Loss: 3.098628
2023-07-06 00:44: Train Epoch 36: 180/323 Loss: 2.886821
2023-07-06 00:44: Train Epoch 36: 200/323 Loss: 2.696455
2023-07-06 00:45: Train Epoch 36: 220/323 Loss: 3.090449
2023-07-06 00:45: Train Epoch 36: 240/323 Loss: 2.756257
2023-07-06 00:45: Train Epoch 36: 260/323 Loss: 2.799305
2023-07-06 00:45: Train Epoch 36: 280/323 Loss: 3.426466
2023-07-06 00:46: Train Epoch 36: 300/323 Loss: 2.812137
2023-07-06 00:46: Train Epoch 36: 320/323 Loss: 3.011328
2023-07-06 00:46: **********Train Epoch 36: averaged Loss: 2.991589
2023-07-06 00:46: **********Val Epoch 36: average Loss: 2.971656
2023-07-06 00:46: Train Epoch 37: 0/323 Loss: 3.001183
2023-07-06 00:47: Train Epoch 37: 20/323 Loss: 2.698144
2023-07-06 00:47: Train Epoch 37: 40/323 Loss: 3.810995
2023-07-06 00:47: Train Epoch 37: 60/323 Loss: 3.216678
2023-07-06 00:47: Train Epoch 37: 80/323 Loss: 2.997003
2023-07-06 00:48: Train Epoch 37: 100/323 Loss: 2.814675
2023-07-06 00:48: Train Epoch 37: 120/323 Loss: 3.010047
2023-07-06 00:48: Train Epoch 37: 140/323 Loss: 3.086021
2023-07-06 00:48: Train Epoch 37: 160/323 Loss: 3.019303
2023-07-06 00:49: Train Epoch 37: 180/323 Loss: 2.948293
2023-07-06 00:49: Train Epoch 37: 200/323 Loss: 2.833704
2023-07-06 00:49: Train Epoch 37: 220/323 Loss: 2.888276
2023-07-06 00:49: Train Epoch 37: 240/323 Loss: 2.766340
2023-07-06 00:50: Train Epoch 37: 260/323 Loss: 3.129784
2023-07-06 00:50: Train Epoch 37: 280/323 Loss: 2.871181
2023-07-06 00:50: Train Epoch 37: 300/323 Loss: 3.094186
2023-07-06 00:50: Train Epoch 37: 320/323 Loss: 2.702344
2023-07-06 00:50: **********Train Epoch 37: averaged Loss: 2.930926
2023-07-06 00:51: **********Val Epoch 37: average Loss: 3.054522
2023-07-06 00:51: Train Epoch 38: 0/323 Loss: 2.872282
2023-07-06 00:51: Train Epoch 38: 20/323 Loss: 2.767132
2023-07-06 00:51: Train Epoch 38: 40/323 Loss: 2.999642
2023-07-06 00:51: Train Epoch 38: 60/323 Loss: 2.957681
2023-07-06 00:52: Train Epoch 38: 80/323 Loss: 3.161692
2023-07-06 00:52: Train Epoch 38: 100/323 Loss: 3.012587
2023-07-06 00:52: Train Epoch 38: 120/323 Loss: 3.174959
2023-07-06 00:52: Train Epoch 38: 140/323 Loss: 2.734215
2023-07-06 00:53: Train Epoch 38: 160/323 Loss: 2.801414
2023-07-06 00:53: Train Epoch 38: 180/323 Loss: 2.819255
2023-07-06 00:53: Train Epoch 38: 200/323 Loss: 2.949595
2023-07-06 00:53: Train Epoch 38: 220/323 Loss: 3.474007
2023-07-06 00:54: Train Epoch 38: 240/323 Loss: 3.334872
2023-07-06 00:54: Train Epoch 38: 260/323 Loss: 2.946080
2023-07-06 00:54: Train Epoch 38: 280/323 Loss: 2.729177
2023-07-06 00:54: Train Epoch 38: 300/323 Loss: 2.665561
2023-07-06 00:55: Train Epoch 38: 320/323 Loss: 2.896110
2023-07-06 00:55: **********Train Epoch 38: averaged Loss: 2.941966
2023-07-06 00:55: **********Val Epoch 38: average Loss: 3.057490
2023-07-06 00:55: Train Epoch 39: 0/323 Loss: 3.013149
2023-07-06 00:55: Train Epoch 39: 20/323 Loss: 2.854602
2023-07-06 00:55: Train Epoch 39: 40/323 Loss: 2.803015
2023-07-06 00:56: Train Epoch 39: 60/323 Loss: 3.245884
2023-07-06 00:56: Train Epoch 39: 80/323 Loss: 2.924523
2023-07-06 00:56: Train Epoch 39: 100/323 Loss: 3.124251
2023-07-06 00:56: Train Epoch 39: 120/323 Loss: 2.712565
2023-07-06 00:57: Train Epoch 39: 140/323 Loss: 2.706961
2023-07-06 00:57: Train Epoch 39: 160/323 Loss: 2.651790
2023-07-06 00:57: Train Epoch 39: 180/323 Loss: 3.027707
2023-07-06 00:57: Train Epoch 39: 200/323 Loss: 2.949488
2023-07-06 00:58: Train Epoch 39: 220/323 Loss: 2.941647
2023-07-06 00:58: Train Epoch 39: 240/323 Loss: 2.685951
2023-07-06 00:58: Train Epoch 39: 260/323 Loss: 3.001293
2023-07-06 00:58: Train Epoch 39: 280/323 Loss: 2.919628
2023-07-06 00:59: Train Epoch 39: 300/323 Loss: 2.759279
2023-07-06 00:59: Train Epoch 39: 320/323 Loss: 3.354984
2023-07-06 00:59: **********Train Epoch 39: averaged Loss: 2.909325
2023-07-06 00:59: **********Val Epoch 39: average Loss: 3.313460
2023-07-06 00:59: Validation performance didn't improve for 15 epochs. Training stops.
2023-07-06 00:59: Total training time: 711.2218min, best loss: 2.120213
2023-07-06 00:59: Saving current best model to ../runs/shenzhen/07-05-13h08m_shenzhen_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/best_model.pth
2023-07-06 01:00: Horizon 01, MAE: 1.71, RMSE: 2.56, MAPE: 7.0475, SMAPE: 3.3296, WMAPE: 5.7763%
2023-07-06 01:00: Horizon 02, MAE: 1.97, RMSE: 3.00, MAPE: 8.3365, SMAPE: 3.8290, WMAPE: 6.6592%
2023-07-06 01:00: Horizon 03, MAE: 2.07, RMSE: 3.22, MAPE: 8.8845, SMAPE: 4.0001, WMAPE: 6.9811%
2023-07-06 01:00: Horizon 04, MAE: 2.14, RMSE: 3.36, MAPE: 9.2860, SMAPE: 4.1184, WMAPE: 7.2128%
2023-07-06 01:00: Horizon 05, MAE: 2.18, RMSE: 3.47, MAPE: 9.5504, SMAPE: 4.1960, WMAPE: 7.3679%
2023-07-06 01:00: Horizon 06, MAE: 2.21, RMSE: 3.54, MAPE: 9.6949, SMAPE: 4.2419, WMAPE: 7.4621%
2023-07-06 01:00: Horizon 07, MAE: 2.24, RMSE: 3.60, MAPE: 9.8555, SMAPE: 4.2884, WMAPE: 7.5550%
2023-07-06 01:00: Horizon 08, MAE: 2.26, RMSE: 3.64, MAPE: 9.9628, SMAPE: 4.3271, WMAPE: 7.6288%
2023-07-06 01:00: Horizon 09, MAE: 2.28, RMSE: 3.66, MAPE: 10.0362, SMAPE: 4.3571, WMAPE: 7.6872%
2023-07-06 01:00: Horizon 10, MAE: 2.31, RMSE: 3.72, MAPE: 10.2189, SMAPE: 4.4088, WMAPE: 7.7897%
2023-07-06 01:00: Horizon 11, MAE: 2.34, RMSE: 3.77, MAPE: 10.3581, SMAPE: 4.4606, WMAPE: 7.8911%
2023-07-06 01:00: Horizon 12, MAE: 2.38, RMSE: 3.83, MAPE: 10.5540, SMAPE: 4.5277, WMAPE: 8.0168%
2023-07-06 01:00: Average Horizon, MAE: 2.17, RMSE: 3.47, MAPE: 9.4821, SMAPE: 4.1737, WMAPE: 7.3356%
/home/assassin/TRC/STG-NCDE-main
Namespace(dataset='shenzhen', mode='train', device=1, debug=False, model='GCDE', cuda=True, comment='', val_ratio=0.2, test_ratio=0.2, lag=12, horizon=12, num_nodes=627, tod=False, normalizer='std', column_wise=False, default_graph=True, model_type='type1', g_type='agc', input_dim=2, output_dim=1, embed_dim=10, hid_dim=64, hid_hid_dim=64, num_layers=2, cheb_k=2, solver='rk4', loss_func='mae', seed=0, batch_size=32, epochs=200, lr_init=0.001, weight_decay=0.001, lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', early_stop=True, early_stop_patience=15, grad_norm=False, max_grad_norm=5, teacher_forcing=False, real_value=True, missing_test=False, missing_rate=0.1, mae_thresh=None, mape_thresh=0.0, model_path='', log_dir='../runs', log_step=20, plot=False, tensorboard=True)
NeuralGCDE(
  (func_f): FinalTanh_f(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
    (linear_out): Linear(in_features=64, out_features=128, bias=True)
  )
  (func_g): VectorField_g(
    input_channels: 2, hidden_channels: 64, hidden_hidden_channels: 64, num_hidden_layers: 2
    (linear_in): Linear(in_features=64, out_features=64, bias=True)
    (linear_out): Linear(in_features=64, out_features=4096, bias=True)
  )
  (end_conv): Conv2d(1, 12, kernel_size=(1, 64), stride=(1, 1))
  (initial_h): Linear(in_features=2, out_features=64, bias=True)
  (initial_z): Linear(in_features=2, out_features=64, bias=True)
)
*****************Model Parameter*****************
node_embeddings torch.Size([627, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([627, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 383304
*****************Finish Parameter****************
Load shenzhen Dataset shaped:  (17280, 627, 1) 138.89999999973352 5.000000000113278 30.58014316470494 29.02020000004515
Normalize the dataset by Standard Normalization
Train:  (10345, 12, 627, 1) (10345, 12, 627, 1)
Val:  (3433, 12, 627, 1) (3433, 12, 627, 1)
Test:  (3433, 12, 627, 1) (3433, 12, 627, 1)
Creat Log File in:  ../runs/shenzhen/07-05-13h08m_shenzhen_GCDE_type1_embed{10}hid{64}hidhid{64}lyrs{2}lr{0.001}wd{0.001}/run.log
*****************Model Parameter*****************
node_embeddings torch.Size([627, 10]) True
func_f.linear_in.weight torch.Size([64, 64]) True
func_f.linear_in.bias torch.Size([64]) True
func_f.linears.0.weight torch.Size([64, 64]) True
func_f.linears.0.bias torch.Size([64]) True
func_f.linear_out.weight torch.Size([128, 64]) True
func_f.linear_out.bias torch.Size([128]) True
func_g.node_embeddings torch.Size([627, 10]) True
func_g.weights_pool torch.Size([10, 2, 64, 64]) True
func_g.bias_pool torch.Size([10, 64]) True
func_g.linear_in.weight torch.Size([64, 64]) True
func_g.linear_in.bias torch.Size([64]) True
func_g.linear_out.weight torch.Size([4096, 64]) True
func_g.linear_out.bias torch.Size([4096]) True
end_conv.weight torch.Size([12, 1, 1, 64]) True
end_conv.bias torch.Size([12]) True
initial_h.weight torch.Size([64, 2]) True
initial_h.bias torch.Size([64]) True
initial_z.weight torch.Size([64, 2]) True
initial_z.bias torch.Size([64]) True
Total params num: 383304
*****************Finish Parameter****************
an epoch time:  303.8304898738861
an epoch time:  501.3657510280609
an epoch time:  532.19855427742
an epoch time:  521.482851266861
an epoch time:  527.0224103927612
an epoch time:  683.5023584365845
an epoch time:  1324.047104358673
an epoch time:  1338.082726240158
an epoch time:  1299.9389634132385
an epoch time:  1314.6067190170288
an epoch time:  1329.0338320732117
an epoch time:  1305.6529536247253
an epoch time:  1319.3208701610565
an epoch time:  1336.089231967926
an epoch time:  1315.5165688991547
an epoch time:  1316.8899810314178
an epoch time:  1318.9105706214905
an epoch time:  1303.0592811107635
an epoch time:  1291.5620172023773
an epoch time:  1316.8209338188171
an epoch time:  1305.5644986629486
an epoch time:  1255.1427476406097
an epoch time:  1273.1832749843597
an epoch time:  1257.3338956832886
an epoch time:  1243.5727870464325
an epoch time:  1084.3845846652985
an epoch time:  971.4895734786987
an epoch time:  955.4072268009186
an epoch time:  966.6769526004791
an epoch time:  943.5043969154358
an epoch time:  970.5476832389832
an epoch time:  941.7145125865936
an epoch time:  964.9167366027832
an epoch time:  947.0743403434753
an epoch time:  643.7172522544861
an epoch time:  240.05789065361023
an epoch time:  239.72842621803284
an epoch time:  230.200177192688
an epoch time:  229.64759039878845
